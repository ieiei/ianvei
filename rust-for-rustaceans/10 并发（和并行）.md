
通过本章，我希望为您提供有效利用 Rust 程序中的并发性、在库中实现对并发使用的支持以及正确使用 Rust 的并发原语所需的所有信息和工具。 我不会直接教你如何实现并发数据结构或编写高性能并发应用程序。 相反，我的目标是让您充分了解基本机制，以便您能够自行运用它们来完成您可能需要的任何任务。

并发有三种形式：单线程并发（就像我们在第 8 章中讨论的 async/await 一样）、单核多线程并发和多核并发，后者产生真正的并行性。

每种风格都允许程序中的并发任务以不同的方式交错执行。 如果考虑到操作系统调度和抢占的细节，还会有更多的子风味，但我们不会对此进行深入探讨。

在类型层面，Rust 仅代表并发的一个方面：多线程。 一种类型要么可供多个线程安全使用，要么不安全。 即使你的程序有多个线程（因此是并发的）但只有一个核心（因此不是并行的），Rust 也必须假设如果有多个线程，则可能存在并行性。 无论两个线程实际上是否并行执行，我们将讨论的大多数类型和技术都同样适用，因此为了保持语言简单，我将使用“并发”一词的非正式含义，即“运行更多的东西” 或更少同时”贯穿本章。 当区别很重要时，我会指出这一点。

Rust 基于类型的安全多线程方法的特别之处在于，它不是编译器的功能，而是开发人员可以扩展以开发复杂的并发合约的库功能。 由于线程安全性是通过 Send 和 Sync 实现和边界在类型系统中表达的，这些实现和边界一直传播到应用程序代码，因此整个程序的线程安全性仅通过类型检查来检查。

*Rust 编程语言* 已经涵盖了并发方面的大部分基础知识，包括发送和同步特征、Arc 和互斥体以及通道。 因此，我不会在这里重复太多内容，除非值得在其他主题的背景下专门重复一些内容。 相反，我们将看看是什么让并发变得困难，以及一些旨在解决这些困难的常见并发模式。 在深入研究如何使用原子操作来实现较低级别的并发操作之前，我们还将探讨并发和异步如何相互作用（以及它们如何不相互作用）。 最后，我将提供一些有关如何在使用并发代码时保持理智的建议来结束本章。
## 并发的麻烦

在我们深入研究并发编程的良好模式和 Rust 并发机制的细节之前，值得花一些时间来理解为什么并发首先具有挑战性。 也就是说，为什么我们需要并发代码的特殊模式和机制？

### *正确性*

并发的主要困难是协调对多个线程之间共享的资源的访问（尤其是写访问）。 如果许多线程想要共享资源只是为了读取它，那么这通常很简单：将其粘贴在 Arc 中或将其放置在可以获得 &'static 的东西中，然后就完成了。 但是，一旦任何线程想要写入，就会出现各种问题，通常以“数据争用”的形式出现。 简而言之，当一个线程更新共享状态，而第二个线程也在访问该状态（读取或更新它）时，就会发生数据争用。 如果没有额外的保护措施，第二个线程可能会读取部分覆盖的状态，破坏第一个线程写入的部分内容，或者根本看不到第一个线程的写入！ 一般来说，所有数据竞争都被视为未定义的行为。

数据竞争是更广泛的一类问题的一部分，这些问题主要（但并非完全）发生在并发环境中：*竞争条件*。 当一系列指令可能产生多种结果时，就会出现竞争条件，具体取决于系统中其他事件的相对时间。 这些事件可以是执行特定代码段的线程、计时器关闭、网络数据包进入或任何其他随时间变化的事件。 与数据竞争不同，竞争条件本质上并不是坏事，并且不被视为未定义的行为。 然而，当特别特殊的种族发生时，它们就会成为虫子的滋生地，正如你将在本章中看到的那样。

### *性能*

通常，开发人员将并发引入到他们的程序中，以期提高性能。 或者，更准确地说，他们希望并发性将使他们能够通过利用更多的硬件资源来每秒执行更多的操作。 这可以在单核上通过让一个线程在另一个线程等待时运行来完成，或者通过让线程同时工作（每个核上一个）来跨多个核完成，否则这将在一个核上串行发生。 大多数开发人员在谈论并发性时指的是后一种性能增益，这通常是在可扩展性方面进行的。 在这种情况下，可扩展性意味着“该程序的性能随着核心数量的增加而扩展”，这意味着如果您为程序提供更多核心，其性能就会提高。

虽然实现这样的加速是可能的，但它比看起来更难。 可扩展性的最终目标是线性可扩展性，其中核心数量加倍会使程序每单位时间完成的工作量加倍。 线性可扩展性通常也称为完美可扩展性。 然而，实际上很少有并发程序能够实现这样的加速。 次线性扩展更为常见，当您从一个核心增加到两个核心时，吞吐量几乎呈线性增加，但添加更多核心会产生收益递减。 有些程序甚至会经历负扩展，即让程序访问更多内核*降低*吞吐量，通常是因为许多线程都在争夺某些共享资源。

想象一下一群人试图戳破一张泡沫包装纸上的所有气泡可能会有所帮助——增加更多的人最初会有所帮助，但在某些时候，你会得到收益递减，因为拥挤会让任何人的工作变得更加困难。 如果所涉及的人员效率特别低，您的团队可能最终会站在一起讨论谁应该下一个弹出，并且根本不弹出任何气泡！ 这种应该并行执行的任务之间的干扰称为“竞争”，是良好扩展的大敌。 争用可以通过多种方式产生，但主要的罪魁祸首是互斥、共享资源耗尽和错误共享。

#### 互斥

当任何时候只允许一个并发任务执行一段特定的代码时，我们说该段代码的执行是互斥的——如果一个线程执行它，则没有其他线程可以同时执行它。 典型的例子是互斥锁，或“互斥锁”，它明确强制在任何时候只有一个线程可以进入程序代码的特定关键部分。 然而，相互排斥也可能隐含地发生。 例如，如果您启动一个线程来管理共享资源并通过 mpsc 通道向其发送作业，则该线程可以有效地实现互斥，因为一次只能执行一个此类作业。

当调用内部强制对关键部分进行单线程访问的操作系统或库调用时，也可能会发生互斥。 例如，多年来，标准内存分配器需要对某些分配进行互斥，这使得内存分配成为在其他高度并行程序中引发严重争用的操作。 类似地，许多操作系统操作看似应该是独立的，例如在同一目录中创建两个具有不同名称的文件，但最终可能必须在内核中按顺序发生。

**NOTE** *可扩展并发分配是* *jemalloc* *内存分配器存在的理由！*

互斥是并行加速最明显的障碍，因为根据定义，它强制串行执行程序的某些部分。 即使您使程序的其余部分与内核数量完美匹配，您可以获得的总加速也会受到互斥串行部分的长度的限制。 请注意相互排斥的部分，并设法将它们限制在严格必要的地方。

**NOTE** *对于理论上的人来说，由于代码互斥部分而导致的可实现加速的限制可以使用阿姆达尔定律来计算。*
#### 共享资源耗尽

不幸的是，即使您在任务中实现了完美的并发性，这些任务需要交互的环境本身也可能无法完全扩展。 内核每秒只能在给定的 TCP 套接字上处理一定数量的发送，内存总线一次只能执行一定数量的读取，并且 GPU 的并发能力有限。 这个问题无法治愈。 在实践中，这种环境通常是完美的可扩展性崩溃的地方，而对这种情况的修复往往需要大量的重新设计（甚至是新的硬件！），所以我们在本章中不会过多讨论这个主题。 请记住，可扩展性很少是您可以“实现”的东西，而更多的是您需要努力争取的东西。

#### 虚假分享

当两个不应该相互竞争的操作无论如何都发生竞争时，就会发生错误共享，从而阻止高效的同时执行。 通常会发生这种情况，因为两个操作碰巧在某些共享资源上交叉，即使它们使用该资源的不相关部分。

最简单的例子是锁过度共享，其中锁保护某些复合状态，并且两个原本独立的操作都需要获取锁来更新其状态的特定部分。 这又意味着操作必须串行执行，而不是并行执行。 在某些情况下，可以将单个锁分成两个，一个用于每个不相交的部分，这使得操作能够并行进行。 然而，像这样分割锁并不总是那么简单——状态可能共享一个锁，因为某些第三个操作需要锁定状态的所有部分。 通常你仍然可以分割锁，但是你必须小心不同线程获取分割锁的顺序，以避免当两个操作尝试以不同的顺序获取它们时可能发生的死锁（查找“哲学家就餐问题， ”如果你好奇的话）。 或者，对于某些问题，您可以通过使用底层算法的无锁版本来完全避免临界区，尽管这些也很难正确解决。 最终，虚假共享是一个很难解决的问题，并且没有一个包罗万象的解决方案，但识别问题是一个好的开始。

错误共享的一个更微妙的例子发生在 CPU 级别，正如我们在第 2 章中简要讨论的那样。CPU 在内部根据缓存行（内存中较长的连续字节序列）而不是单个字节对内存进行操作，以分摊成本 内存访问。 例如，在大多数 Intel 处理器上，高速缓存行大小为 64 字节。 这意味着每个内存操作实际上最终都会读取或写入 64 字节的某个倍数。 当两个核心想要更新恰好落在同一缓存行上的两个不同字节的值时，错误共享就会发挥作用； 这些更新必须按顺序执行，即使这些更新在逻辑上是不相交的。

这可能看起来太低级，无关紧要，但实际上，这种错误共享可能会大大降低应用程序的并行加速。 想象一下，您分配一个整数值数组来指示每个线程已完成多少个操作，但所有整数都落在同一缓存行中 - 现在，所有其他并行线程都将在该缓存行上争夺它们执行的每个操作。 如果操作相对较快，那么您的“大部分”执行时间可能最终会花在与这些计数器竞争上！

避免错误缓存行共享的技巧是填充您的值，使其等于缓存行的大小。 这样，两个相邻的值总是落在不同的缓存行上。 当然，这也会增加数据结构的大小，因此仅当基准测试表明存在问题时才使用此方法。

> **可扩展性的代价** 
> 
> 您应该注意的并发性的一个有点正交的方面是首先引入并发性的成本。 编译器确实很擅长优化单线程代码——毕竟，他们已经这样做很长时间了——而且单线程代码往往比单线程代码需要更少昂贵的保护措施（比如锁、通道或原子指令）。 并发代码可以。 总的来说，在给定任意数量的内核的情况下，并发的各种成本都会使并行程序比单线程程序慢！ 这就是为什么在优化和并行化之前和之后进行测量很重要：结果可能会让您感到惊讶。
>
> 如果您对这个主题感到好奇，我强烈建议您阅读 Frank McSherry 2015 年的论文“可扩展性！ 但代价是什么？ （https://www.frankmcsherry.org/assets/COST.pdf），其中揭示了一些特别令人震惊的“昂贵的扩展”示例。 

## 并发模型

Rust 提供了三种您经常会遇到的向程序添加并发性的模式：共享内存并发性、工作池和参与者。 详细介绍可以添加并发性的各种方法需要一本书，所以在这里我将只关注这三种模式。

### *共享内存*

从概念上讲，共享内存并发性非常简单：线程通过在它们之间共享的内存区域上进行操作来进行协作。 这可能采用由互斥锁保护的状态或存储在哈希映射中的形式，并支持来自多个线程的并发访问。 许多线程可能在不相交的数据上执行相同的任务，例如，如果许多线程在 Vec 的不相交子范围上执行某些功能，或者它们可能正在执行需要某些共享状态的不同任务，例如在数据库中，其中一个 线程处理用户对表的查询，而另一个线程优化用于在后台存储该表的数据结构。

当您使用共享内存并发时，您对数据结构的选择很重要，特别是当涉及的线程需要非常紧密地合作时。 常规互斥锁可能会阻止扩展超出极少数核心，读取器/写入器锁可能允许更多并发读取，但代价是写入速度较慢，分片读取器/写入器锁可能允许完全可扩展的读取，但代价是写入速度变慢 极具破坏性。 类似地，一些并发哈希映射旨在获得良好的全面性能，而另一些则专门针对写入很少的并发读取。 一般来说，在共享内存并发中，您希望使用专为尽可能接近目标用例而设计的数据结构，以便您可以利用优化来权衡应用程序不关心的性能方面 它所做的那些。

共享内存并发非常适合线程需要以不交换的方式联合更新某些共享状态的用例。 也就是说，如果一个线程必须使用某个函数 f 更新状态 s，而另一个线程必须使用某个函数 g 更新状态，并且 f(g(s)) != g(f(s))，则共享内存 并发可能是必要的。 如果情况并非如此，那么其他两种模式可能更适合，因为它们往往会带来更简单、性能更高的设计。

**NOTE** *一些问题已知算法可以在不使用锁的情况下提供并发共享内存操作。 随着核心数量的增加，这些无锁算法可能比基于锁的算法具有更好的扩展性，但由于其复杂性，它们的每核心性能通常也较慢。 一如既往，对于性能问题，首先进行基准测试，然后寻找替代解决方案。*

### *工作池*

在工作池模型中，许多相同的线程从共享作业队列接收作业，然后它们完全独立地执行。 例如，Web 服务器通常有一个处理传入连接的工作池，异步代码的多线程运行时倾向于使用工作池来共同执行应用程序的所有未来（或更准确地说，其顶级任务）。

共享内存并发和工作池之间的界限通常很模糊，因为工作池倾向于使用共享内存并发来协调它们如何从队列中获取作业以及如何将不完整的作业返回到队列。 例如，假设您正在使用数据并行库 rayon 对向量的每个元素并行执行某些功能。 在幕后，rayon 启动一个工作池，将向量分割成子范围，然后将子范围分发给池中的线程。 当池中的线程完成一个范围时，rayon 会安排它开始处理下一个未处理的子范围。 该向量在所有工作线程之间共享，并且线程通过支持工作窃取的共享内存队列式数据结构进行协调。

工作窃取是大多数工作池的一个关键特征。 基本前提是，如果一个线程提前完成其工作，并且没有更多未分配的工作可用，则该线程可以窃取已分配给其他工作线程但尚未启动的作业。 并非所有工作都需要相同的时间来完成，因此即使为每个工人提供相同“数量”的工作，某些工人最终可能会比其他工人更快地完成工作。 那些提前完成的线程应该帮助掉队的线程，以便更快地完成整个操作，而不是坐等那些需要长时间运行的作业的线程完成。

实现一种支持这种工作窃取的数据结构是一项相当艰巨的任务，而且不会因线程不断尝试相互窃取工作而产生大量开销，但这一功能对于高性能工作池至关重要。 如果您发现自己需要一个工作池，那么最好的选择通常是使用已经进行了大量工作的工作池，或者至少重用现有工作池的数据结构，而不是自己从头开始编写一个工作池 。

当每个线程执行的工作相同，但执行的数据不同时，工作池非常适合。 在人造丝并行映射操作中，每个线程执行相同的映射计算； 他们只是对基础数据的不同子集执行它。 在多线程异步运行时，每个线程只需调用 Future::poll; 他们只是对不同的未来有不同的称呼。 如果您开始必须区分线程池中的线程，则不同的设计可能更合适。

> **连接池** 
> 
> 连接池是一种共享内存结构，它保留一组已建立的连接并将它们分发给需要连接的线程。 这是图书馆中管理与外部服务的连接的常见设计模式。 如果一个线程需要连接但连接不可用，则要么建立一个新连接，要么强制该线程阻塞。 当线程完成连接后，它会将该连接返回到池中，从而使其可供其他可能正在等待的线程使用。
>
> 通常，连接池最困难的任务是管理连接生命周期。 连接可以以最后一个使用它的线程置于的任何状态返回到池中。 因此，连接池必须确保与连接相关的任何状态（无论是在客户端还是在服务器上）都已重置，以便当该连接随后被另一个线程使用时，该线程可以表现得好像它被赋予了一个新的状态一样。 ，专用连接。

### *Actors*

Actor 并发模型在很多方面与工作池模型相反。 工作池有许多共享作业队列的相同线程，而参与者模型有许多单独的作业队列，每个作业“主题”都有一个队列。 每个作业队列都会输入一个特定的参与者，该参与者处理与应用程序状态的子集相关的所有作业。 该状态可能是数据库连接、文件、指标收集数据结构或您可以想象的许多线程可能需要能够访问的任何其他结构。 不管是什么，单个参与者拥有该状态，如果某个任务想要与该状态交互，它需要向拥有该状态的参与者发送一条消息，总结它希望执行的操作。 当拥有者收到该消息时，它会执行指示的操作，并使用操作结果（如果相关）响应查询任务。

由于参与者对其内部资源具有独占访问权，因此除了消息传递所需之外，不需要任何锁或其他同步机制。

Actor 模型的一个关键点是 actor 都互相交谈。 例如，如果负责日志记录的参与者需要写入文件和数据库表，它可能会向负责每个文件和数据库表的参与者发送消息，要求他们执行相应的操作，然后继续执行下一个操作 记录事件。 通过这种方式，Actor 模型更像是一个网络，而不是轮子上的辐条——用户对 Web 服务器的请求可能会作为对负责该连接的 Actor 的单个请求开始，但可能会过渡性地产生数十个、数百个甚至更多的请求。 在用户的请求得到满足之前，系统会向更深层的参与者发送数千条消息。

Actor 模型中没有任何内容要求每个 Actor 都是自己的线程。 相反，大多数参与者系统建议应该有大量参与者，因此每个参与者应该映射到一个任务而不是一个线程。 毕竟，参与者仅在执行时才需要对其包装的资源进行独占访问，并且不关心它们是否位于自己的线程上。 事实上，actor 模型经常与工作池模型结合使用，例如，使用多线程异步运行时 Tokio 的应用程序可以为每个 actor 生成一个异步任务，然后 Tokio 将执行 每个演员在其工作池中都有一份工作。 因此，当一个给定的 Actor 产生并恢复时，它的执行可能会在工作池中从一个线程移动到另一个线程，但每次 Actor 执行时，它都会保持对其包装资源的独占访问。

Actor 并发模型非常适合当您拥有许多可以相对独立运行的资源，并且每个资源内很少或根本没有并发机会的情况。 例如，操作系统可能有一个负责每个硬件设备的参与者，而 Web 服务器可能有一个负责每个后端数据库连接的参与者。 如果您只需要几个参与者，如果参与者之间的工作明显不平衡，或者如果某些参与者变得很大，那么参与者模型就不能很好地工作 - 在所有这些情况下，您的应用程序最终可能会遇到执行瓶颈 系统中单个参与者的速度。 由于每个参与者都希望能够独占访问他们的一小部分世界，因此您无法轻松地并行执行该瓶颈参与者。

## 异步和并行

正如我们在第 8 章中讨论的，Rust 中的异步实现了无需并行的并发——我们可以使用 select 和 join 等结构让单个线程轮询多个 future，并在其中一个、部分或全部完成时继续。 由于不涉及并行性，因此与 future 的并发从根本上不需要发送这些 future。 即使生成一个 future 作为额外的顶级任务运行，从根本上来说也不需要 Send，因为单个执行器线程可以同时管理多个 future 的轮询。

然而，在“大多数”情况下，应用程序既需要并发性又需要并行性。 例如，如果 Web 应用程序为每个传入连接构建一个 future，因此同时具有许多活动连接，则它可能希望异步执行器能够利用主机上的多个核心。 这不会自然发生：您的代码必须明确告诉执行器哪些 future 可以并行运行，哪些不能。

特别是，必须向执行器提供两条信息，以使其知道它可以将 future 中的工作分散到线程工作池中。 首先，所涉及的 future 是 Send 的——如果不是，则执行器不允许将 future 发送到其他线程进行处理，并且不可能实现并行性； 只有构建此类 future 的线程才能轮询它们。

第二个信息是如何将future拆分成可以独立运行的任务。 这与第 8 章中关于任务与 future 的讨论有关：如果一个巨大的 Future 包含许多 Future 实例，而这些实例本身对应于可以并行运行的任务，则执行器仍然必须对顶级 Future 调用 poll，并且它 必须从单个线程执行此操作，因为 poll 需要 &mut self。 因此，为了实现 future 的并行性，您必须显式地生成您希望能够并行运行的 future。 另外，由于第一个要求，您用来执行此操作的执行器函数将要求传入的 Future 是 Send。

> **异步同步原语** 
> 
> Most of the synchronization primitives that exist for blocking code (think std::sync) also have asynchronous counterparts . There are asynchronous variants of channels, mutexes, reader/writer locks, barriers, and all sorts of other similar constructs . We need these because, as discussed in Chapter 8, blocking inside a future will hold up other work the executor may need to do, and so is inadvisable . 
> 
> However, the asynchronous versions of these primitives are often slower than their synchronous counterparts because of the additional machinery needed to perform the necessary wake-ups . For that reason, you may want to use synchronous synchronization primitives even in asynchronous contexts when- ever the use does not risk blocking the executor . For example, while it’s generally true that acquiring a Mutex might block for a long time, that might not be true for a particular Mutex that, perhaps, is acquired only rarely, and only ever for short periods of time . In that case, blocking for the short time until the Mutex becomes available again might not actually cause any problems . You will want to make sure that you never yield or perform other long-running operations while holding the MutexGuard, but barring that you shouldn’t run into problems . 
> 
> As always with such optimizations, though, make sure you measure first, and choose only the synchronous primitive if it nets you significant performance improvements . If it does not, the additional footguns introduced by using a syn- chronous primitive in an asynchronous context are probably not worth it . 

## Lower-Level Concurrency

The standard library provides the std::sync::atomic module, which pro- vides access to the underlying CPU primitives, higher-level constructs like channels and mutexes are built with. These primitives come in the form of atomic types with names starting with Atomic—AtomicUsize, AtomicI32, AtomicBool, AtomicPtr, and so on—the Ordering type, and two functions called fence and compiler_fence. We’ll look at each of these over the next few sections. 

These types are the blocks used to build any code that has to communicate between threads. Mutexes, channels, barriers, concurrent hash tables, lock-free stacks, and all other synchronization constructs ultimately rely on these few primitives to do their jobs. They also come in handy on their own for lightweight cooperation between threads where heavyweight synchronization like a mutex is excessive—for example, to increment a shared counter or set a shared Boolean to true. 

The atomic types are special in that they have defined semantics for what happens when multiple threads try to access them concurrently. These types all support (mostly) the same API: load, store, fetch_*, and compare_ exchange. In the rest of this section, we’ll look at what those do, how to use them correctly, and what they’re useful for. But first, we have to talk about low-level memory operations and memory ordering. 

### *Memory Operations*

Informally, we often refer to accessing variables as “reading from” or “writing to” memory. In reality, a lot of machinery between code uses a variable and the actual CPU instructions that access your memory hardware. It’s important to understand that machinery, at least at a high level, in order to understand how concurrent memory accesses behave. 

The compiler decides what instructions to emit when your program reads the value of a variable or assigns a new value to it. It is permitted to perform all sorts of transformations and optimizations on your code and may end up reordering your program statements, eliminating operations it deems redundant, or using CPU registers rather than actual memory to store intermediate computations. The compiler is subject to a number of restrictions on these transformations, but ultimately only a subset of your variable accesses actually end up as memory access instructions. 

At the CPU level, memory instructions come in two main shapes: loads and stores. A load pulls bytes from a location in memory into a CPU register, and a store stores bytes from a CPU register into a location in memory. Loads and stores operate on small chunks of memory at a time: usually 8 bytes or less on modern CPUs. If a variable access spans more bytes than can be accessed with a single load or store, the compiler automatically turns it into multiple load or store instructions, as appropriate. The CPU also has some leeway in how it executes a program’s instructions to make better use of the hardware and improve program performance. For example, modern CPUs often execute instructions in parallel, or even out of order, when they don’t have dependencies on each other. There are also several layers of caches between each CPU and your computer’s DRAM, which means that a load of a given memory location may not necessarily see the latest store to that memory location, going by wall-clock time. 

In most code, the compiler and CPU are permitted to transform the code only in ways that don’t affect the semantics of the resulting program, so these transformations are invisible to the programmer. However, in the context of parallel execution, these transformations can have a significant impact on application behavior. Therefore, CPUs typically provide multiple different variations of the load and store instructions, each with different guarantees about how the CPU may reorder them and how they may be interleaved with parallel operations on other CPUs. Similarly, compilers (or rather, the language the compiler compiles) provide different annotations you can use to force particular execution constraints for some subset of their memory accesses. In Rust, those annotations come in the form of the atomic types and their methods, which we’ll spend the rest of this section picking apart. 

### *Atomic Types*

Rust’s atomic types are so called because they can be accessed atomically— that is, the value of an atomic-type variable is written all at once and will never be written using multiple stores, guaranteeing that a load of that vari- able cannot observe that only some of the bytes composing the value have changed while others have not (yet). This is easiest understood by way of contrast with non-atomic types. For example, reassigning a new value to a tuple of type (i64, i64) typically requires two CPU store instructions, one for each 8-byte value. If one thread were to perform both of those stores, another thread could (if we ignore the borrow checker for a second) read the tuple’s value after the first store but before the second, and thus end up with an inconsistent view of the tuple’s value. It would end up reading the new value for the first element and the old value for the second element, a value that was never actually stored by any thread. 

The CPU can atomically access values only of certain sizes, so there are only a few atomic types, all of which live in the atomic module. Each atomic type is of one of the sizes the CPU supports atomic access to, with multiple variations for things like whether the value is signed and to differentiate between an atomic usize and a pointer (which is of the same size as usize). Furthermore, the atomic types have explicit methods for loading and storing the values they hold, and a handful of more complex methods we’ll get back to later, so that the mapping between the code the programmer writes and the resulting CPU instructions is clearer. For example, AtomicI32::load performs a single load of a signed 32-bit value, and AtomicPtr::store per- forms a single store of a pointer-sized (64 bits on a 64-bit platform) value. 

### *Memory Ordering*

Most of the methods on the atomic types take an argument of type Ordering, which dictates the memory ordering restrictions the atomic operation is subject to. Across different threads, loads and stores of an atomic value may be sequenced by the compiler and CPU only in interleavings that are compatible with the requested memory ordering of each of the atomic operations on that atomic value. Over the next few sections, we’ll see some examples of why control over the ordering is important and necessary to get the expected semantics out of the compiler and CPU. 

Memory ordering often comes across as counterintuitive, because we humans like to read programs from top to bottom and imagine that they execute line by line—but that’s not how the code actually executes when it hits the hardware. Memory accesses can be reordered, or even entirely elided, and writes on one thread may not immediately be visible to other threads, even if later writes in program order have already been observed. 

Think of it like this: each memory location sees a sequence of modifications coming from different threads, and the sequences of modifications for different memory locations are independent. If two threads T1 and T2 both write to memory location M, then even if T1 executed first as measured by a user with a stopwatch, T2’s write to M may still appear to have happened first for M absent any other constraints between the two threads’ execution. Essentially, *the computer does not take wall-clock time into account* when it deter- mines the value of a given memory location—all that matter are the execu- tion constraints the programmer puts on what constitutes a valid execution. For example, if T1 writes to M and then spawns thread T2, which then writes to M, the computer must recognize T1’s write as having happened first because T2’s existence depends on T1. 

If that’s hard to follow, don’t fret—memory ordering can be mind- bending, and language specifications tend to use very precise but not very intuitive wording to describe it. We can construct a mental model that’s easier to grasp, if a little simplified, by instead focusing on the underlying hardware architecture. Very basically, your computer memory is structured as a treelike hierarchy of storage where the leaves are CPU registers and the roots are the storage on your physical memory chips, often called main memory. Between the two are several layers of caches, and different layers of the hierarchy can reside on different pieces of hardware. When a thread performs a store to a memory location, what really happens is that the CPU starts a write request for the value in a given CPU register that then has to make its way up the memory hierarchy toward main memory. When a thread performs a load, the request flows up the hierarchy until it hits a layer that has the value available, and returns from there. Herein lies the problem: writes aren’t visible everywhere until all caches of the written memory location have been updated, but other CPUs can execute instructions against the same memory location at the same time, and weirdness ensues. Memory ordering, then, is a way to request precise semantics for what happens when multiple CPUs access a particular memory location for a particular operation. 

With this in mind, let’s take a look at the Ordering type, which is the primary mechanism by which we, as programmers, can dictate additional constraints on what concurrent executions are valid. 

Ordering is defined as an enum with the variants shown in Listing 10-1. Concurrency (and Parallelism) **179** 

```rust
enum Ordering {
    Relaxed,
    Release,
    Acquire,
    AcqRel,
    SeqCst
}
```

*Listing 10-1: The definition of Ordering*

Each of these places different restrictions on the mapping from source code to execution semantics, and we’ll explore each one in turn in the remainder of this section. 

#### Relaxed Ordering

Relaxed ordering essentially guarantees nothing about concurrent access to the value beyond the fact that the access is atomic. In particular, relaxed ordering gives no guarantees about the relative ordering of memory accesses across different threads. This is the weakest form of memory ordering. Listing 10-2 shows a simple program in which two threads access two atomic variables using Ordering::Relaxed. 

``` rust
static X: AtomicBool = AtomicBool::new(false);
static Y: AtomicBool = AtomicBool::new(false);

let t1 = spawn(|| {
 1 let r1 = Y.load(Ordering::Relaxed); 2 X.store(r1, Ordering::Relaxed); 
 });
let t2 = spawn(|| {
3 let r2 = X.load(Ordering::Relaxed); 

4 Y.store(true, Ordering::Relaxed) }); 

```

Listing 10-2: Two racing threads with *Ordering::Relaxed* 

Looking at the thread spawned as t2, you might expect that r2 can never be true, since all values are false until the same thread assigns true to Y on the line *after* reading X. However, with a relaxed memory ordering, that outcome is completely possible. The reason is that the CPU is allowed to reorder the loads and stores involved. Let’s walk through exactly what happens here to make r2 = true possible. First, the CPU notices that 4 doesn’t have to happen after 3, since 4 doesn’t use any output or side effect of 3. That is, 4 has no execution dependency on 3. So, the CPU decides to reorder them for *waves hands* reasons that’ll make your program go faster. The CPU thus goes ahead and executes 4 first, setting Y = true, even though 3 hasn’t run yet. Then, t2 is put to sleep by the operating system and thread t1 executes a few instructions, or t1 simply executes on another core. In t1, the compiler must indeed run 1 first and then 2, since 2 depends on the value read in 1. Therefore, t1 reads true from  Y (written by 4) into r1 and then writes that back to X. Finally, t2 executes 3, which reads X and gets true, as was written by 2. 

The relaxed memory ordering allows this execution because it imposes no additional constraints on concurrent execution. That is, under relaxed memory ordering, the compiler must ensure only that execution dependen- cies on any given thread are respected (just as if atomics weren’t involved); it need not make any promises about the interleaving of concurrent opera- tions. Reordering 3 and 4 is permitted for a single-threaded execution, so it is permitted under relaxed ordering as well. 

In some cases, this kind of reordering is fine. For example, if you have a counter that just keeps track of metrics, it doesn’t really matter when exactly it executes relative to other instructions, and Ordering::Relaxed is fine. In other cases, this could be disastrous: say, if your program uses r2 to figure out if security protections have already been set up, and thus ends up erroneously believing that they already have been. 

You don’t generally notice this reordering when writing code that doesn’t make fancy use of atomics—the CPU has to promise that there is no observable difference between the code as written and what each thread actually executes, so everything seems like it runs in order just as you wrote it. This is referred to as respecting program order or evaluation order; the terms are synonyms. 

#### Acquire/Release Ordering

At the next step up in the memory ordering hierarchy, we have Ordering::Acquire, Ordering::Release, and Ordering::AcqRel (acquire plus release). At a high level, these establish an execution dependency between a store in one thread and a load in another and then restrict how operations can be reordered with respect to that load and store. Crucially, these dependencies not only establish a relationship between a store and a load of a single value, but also put ordering constraints on *other* loads and stores in the threads involved. This is because every execution must respect the pro- gram order; if a load in thread B has a dependency on some store in thread A (the store in A must execute before the load in B), then any read or write in B after that load must also happen after that store in A. 

**N O T E** *The* *Acquire* *memory ordering can be applied only to loads,* *Release* *only to stores, and* *AcqRel* *only to operations that both load* and *store (like* *fetch_add**).* 

Concretely, these memory orderings place the following restrictions on execution: 

1. Loads and stores cannot be moved forward past a store with Ordering::Release. 
2. Loads and stores cannot be moved back before a load with Ordering::Acquire. 
3. An Ordering::Acquire load of a variable must see all stores that happened before an Ordering::Release store that stored what the load loaded. 


**N O T E**  To see how these memory orderings change things, Listing 10-3 shows Listing 10-2 again but with the memory ordering swapped out for Acquire and Release. 

``` rust
static X: AtomicBool = AtomicBool::new(false);
static Y: AtomicBool = AtomicBool::new(false);
let t1 = spawn(|| {
    let r1 = Y.load(Ordering::Acquire);
    X.store(r1, Ordering::Release);
});
let t2 = spawn(|| {
1 let r2 = X.load(Ordering::Acquire); 
2 Y.store(true, Ordering::Release) }); 
```

Listing 10-3: Listing 10-2 with *Acquire/Release* memory ordering 

These additional restrictions mean that it is no longer possible for t2 to see r2 = true. To see why, consider the primary cause of the weird outcome in Listing 10-2: the reordering of 1 and 2. The very first restriction, on stores with Ordering::Release, dictates that we cannot move 1 below 2, so we’re all good! 

But these rules are useful beyond this simple example. For example, imagine that you implement a mutual exclusion lock. You want to make sure that any loads and stores a thread runs while it holds the lock are executed only while it’s actually holding the lock, and visible to any thread that takes the lock later. This is exactly what Release and Acquire enable you to do. By performing a Release store to release the lock and an Acquire load to acquire the lock, you can guarantee that the loads and stores in the critical section are never moved to before the lock was actually acquired or to after the lock was released! 

*On some CPU architectures, like x86,* *Acquire/Release* *ordering is guaranteed by the hardware, and there is no additional cost to using* *Ordering::Release* *and* *Ordering::Acquire* *over* *Ordering::Relaxed**. On other architectures that is not the case, and your program may see speedups if you switch to* *Relaxed* *for atomic operations that can tolerate the weaker memory ordering guarantees.* 

#### Sequentially Consistent Ordering

Sequentially consistent ordering (Ordering::SeqCst) is the strongest memory ordering we have access to. Its exact guarantees are somewhat hard to nail down, but very broadly, it requires not only that each thread sees results consistent with Acquire/Release, but also that all threads see the *same* ordering as one another. This is best seen by way of contrast with the behavior of Acquire and Release. Specifically, Acquire/Release ordering does *not* guarantee that if two threads A and B atomically load values written by two other threads X and Y, A and B will see a consistent pattern of when X wrote relative to Y. That’s fairly abstract, so consider the example in Listing 10-4, which shows a case where Acquire/Release ordering can produce unexpected results. Afterwards, we’ll see how sequentially consistent ordering avoids that particular unexpected outcome. 

``` rust
static X: AtomicBool = AtomicBool::new(false);
static Y: AtomicBool = AtomicBool::new(false);
static Z: AtomicI32 = AtomicI32::new(0);
let t1 = spawn(|| {
    X.store(true, Ordering::Release);
});
let t2 = spawn(|| {
    Y.store(true, Ordering::Release);
});
let t3 = spawn(|| {
    while (!X.load(Ordering::Acquire)) {}
	1 if (Y.load(Ordering::Acquire)) { Z.fetch_add(1, Ordering::Relaxed); } 
});
let t4 = spawn(|| {
	while (!Y.load(Ordering::Acquire)) {} 2 if (X.load(Ordering::Acquire)) { 
        Z.fetch_add(1, Ordering::Relaxed); }
});
```


Listing 10-4: Weird results with *Acquire/Release* ordering 

The two threads t1 and t2 set X and Y to true, respectively. Thread t3 waits for X to be true; once X is true, it checks if Y is true and, if so, adds 1 to Z. Thread t4 instead waits for Y to become true, and then checks if X is true and, if so, adds 1 to Z. At this point the question is: what are the possible values for Z after all the threads terminate? Before I show you the answer, try to work your way through it given the definitions of Release and Acquire ordering in the previous section. 

First, let’s recap the conditions under which Z is incremented. Thread t3 increments Z if it sees that Y is true after it observes that X is true, which can happen only if t2 runs before t3 evaluates the load at 1. Conversely, thread t4 increments Z if it sees that X is true after it observes that Y is true, so only if t1 runs before t4 evaluates the load at 2. To simplify the explanation, let’s assume for now that each thread runs to completion once it runs. 

Logically, then, Z can be incremented twice if the threads run in the order 1, 2, 3, 4—both X and Y are set to true, and then t3 and t4 run to find that their conditions for incrementing Z are met. Similarly, Z can trivially be incremented just once if the threads run in the order 1, 3, 2, 4. This sat- isfies t4’s condition for incrementing Z, but not t3’s. Getting Z to be 0, how- ever, *seems* impossible: if we want to prevent t3 from incrementing Z, t2 has to run after t3. Since t3 runs only after t1, that implies that t2 runs after t1. However, t4 won’t run until after t2 has run, so t1 must have run and set X to true by the time t4 runs, and so t4 will increment Z. 

Our inability to get Z to be 0 stems mostly from our human inclination for linear explanations; this happened, then this happened, then this happened. Computers aren’t limited in the same way and have no need to box all events into a single global order. There’s nothing in the rules for Release and Acquire that says that t3 must observe the same execution order for t1 and t2 as t4 observes. As far as the computer is concerned, it’s fine to let t3 observe t1 as having executed first, while having t4 observe t2 as having executed first. With that in mind, an execution in which t3 observes that Y is false after it observes that X is true (implying that t2 runs after t1), while in the same execution t4 observes that X is false after it observes that Y is true (implying that t2 runs before t1), is completely reasonable, even if that seems outrageous to us mere humans. 

As we discussed earlier, Acquire/Release requires only that an Ordering::Acquire load of a variable must see all stores that happened before an Ordering::Release store that stored what the load loaded. In the order- ing just discussed, the computer *did* uphold that property: t3 sees X == true, and indeed sees all stores by t1 prior to it setting X = true—there are none. It also sees Y == false, which was stored by the main thread at program startup, so there aren’t any relevant stores to be concerned with. Similarly, t4 sees Y = true and also sees all stores by t2 prior to setting Y = true—again, there are none. It also sees X == false, which was stored by the main thread and has no preceding store. No rules are broken, yet it just seems wrong somehow. 

Our intuitive expectation was that we could put the threads in some global order to make sense of what every thread saw and did, but that was not the case for Acquire/Release ordering in this example. To achieve something closer to that intuitive expectation, we need sequential consistency. Sequential consistency requires all the threads taking part in an atomic operation to coordinate to ensure that what each thread observes corresponds to (or at least appears to correspond to) *some* single, common execution order. This makes it easier to reason about but also makes it costly. 

Atomic loads and stores marked with Ordering::SeqCst instruct the com- piler to take any extra precautions (such as using special CPU instructions) needed to guarantee sequential consistency for those loads and stores. The exact formalism around this is fairly convoluted, but sequential consistency essentially ensures that if you looked at all the related SeqCst operations from across all your threads, you could put the thread executions in *some* order so that the values that were loaded and stored would all match up. 

If we replaced all the memory ordering arguments in Listing 10-4 with SeqCst, Z could not possibly be 0 after all the threads have exited, just as we originally expected. Under sequential consistency, it must be possible to say either that t1 definitely ran before t2 or that t2 definitely ran before t1, so the execution where t3 and t4 see different orders is not allowed, and thus Z cannot be 0. 

### *Compare and Exchange*

In addition to load and store, all of Rust’s atomic types provide a method called compare_exchange. This method is used to atomically *and conditionally* replace a value. You provide compare_exchange with the last value you observed for an atomic variable and the new value you want to replace the original value with, and it will replace the value only if it is still the same as it was when you last observed it. To see why this is important, take a look at the (broken) implementation of a mutual exclusion lock in Listing 10-5. This implementation keeps track of whether the lock is held in the static atomic variable LOCK. We use the Boolean value true to represent that the lock is held. To acquire the lock, a thread waits for LOCK to be false, then sets it to true again; it then enters its critical section and sets LOCK to false to release the lock when its work (f) is done. 

``` rust
static LOCK: AtomicBool = AtomicBool::new(false);
fn mutex(f: impl FnOnce()) {
    // Wait for the lock to become free (false).
    while LOCK.load(Ordering::Acquire)
      { /* .. TODO: avoid spinning .. */ }
    // Store the fact that we hold the lock.
    LOCK.store(true, Ordering::Release);
    // Call f while holding the lock.
f(); 
    // Release the lock.
    LOCK.store(false, Ordering::Release);
}
```

Listing 10-5: An incorrect implementation of a mutual exclusion lock 

This mostly works, but it has a terrible flaw—two threads might both see LOCK == false at the same time and both leave the while loop. Then they both set LOCK to true and both enter the critical section, which is exactly what the mutex function was supposed to prevent! 

The issue in Listing 10-5 is that there is a gap between when we load the current value of the atomic variable and when we subsequently update it, during which another thread might get to run and read or touch its value. It is exactly this problem that compare_exchange solves—it swaps out the value behind the atomic variable *only* if its value still matches the previous read, and otherwise notifies you that the value has changed. Listing 10-6 shows the corrected implementation using compare_exchange. 

``` rust
static LOCK: AtomicBool = AtomicBool::new(false);
fn mutex(f: impl FnOnce()) {
    // Wait for the lock to become free (false).
    loop {
      let take = LOCK.compare_exchange(
          false,
          true,
          Ordering::AcqRel,
          Ordering::Relaxed
      );
      match take {
        Ok(false) => break,
        Ok(true) | Err(false) => unreachable!(),
        Err(true) => { /* .. TODO: avoid spinning .. */ }
      }
	} 
	// Call f while holding the lock.
	
	f(); 
    // Release the lock.
    LOCK.store(false, Ordering::Release);
}

```
Listing 10-6: A corrected implementation of a mutual exclusion lock 

This time around, we use compare_exchange in the loop, and it takes care of both checking that the lock is currently not held and storing true to take the lock as appropriate. This happens through the first and second arguments to compare_exchange, respectively: in this case, false and then true. You can read the invocation as “Store true only if the current value is false.” The compare_exchange method returns a Result that indicates either that the value was successfully updated (Ok) or that it could not be updated (Err). In either case, it also returns the current value. This isn’t too useful with an AtomicBool since we know what the value must be if the operation failed, but for something like an AtomicI32, the updated current value will let you quickly recompute what to store and then try again without having to do another load. 

**N O T E**  *Note that* *compare_exchange* *checks only whether the value is the same as the one that was passed in as the current value. If some other thread modifies the atomic variable’s value and then resets it to the original value again, a* *compare_exchange* *on that variable will still succeed. This is often referred to as the A-B-A problem.* 

Unlike simple loads and stores, compare_exchange takes *two* Ordering arguments. The first is the “success ordering,” and it dictates what memory ordering should be used for the load and store that the compare_exchange represents in the case that the value was successfully updated. The second is the “failure ordering,” and it dictates the memory ordering for the load if the loaded value does not match the expected current value. These two orderings are kept separate so that the developer can give the CPU leeway to improve execution performance by reordering loads and stores on failure when appropriate, but still get the correct ordering on success. In this case, it’s okay to reorder loads and stores across failed iterations of the lock acquisition loop, but it’s *not* okay to reorder loads and stores inside the critical section in such a way that they end up outside of it. 

Even though its interface is simple, compare_exchange is a very powerful synchronization primitive—so much so that it’s been theoretically proven that you can build all other distributed consensus primitives using only compare_exchange! For that reason, it is the workhorse of many, if not most, synchronization constructs when you really dig into the implementation details. 

Be aware, though, that a compare_exchange requires that a single CPU has exclusive access to the underlying value, and it is therefore a form of mutual exclusion at the hardware level. This in turn means that compare_exchange can quickly become a scalability bottleneck: only one CPU can make progress at a time, so there’s a portion of your code that will not scale with the number of cores. In fact, it’s probably worse than that—the CPUs have to coordinate to ensure that only one CPU succeeds at a compare_exchange for a variable at a time (take a look at the MESI protocol if you’re curious about how that works), and that coordination grows quadratically more costly the more CPUs are involved! 

> **COMPARE_EXCHANGE_WEAK** 
> 
> 细心的文档读者会注意到，compare_exchange 有一个可疑的表兄弟，compare_exchange_weak，并想知道其中的区别是什么。 即使原子变量的值仍然与用户传入的预期值匹配，compare_exchange 的弱变体也允许失败，而在这种情况下，强变体必须成功。
>
> 这可能看起来很奇怪——除非值发生了变化，否则原子值交换怎么会失败呢？ 答案在于没有本机compare_exchange 操作的系统体系结构。 例如，ARM 处理器改为具有锁定加载和条件存储操作，其中如果自加载以来相关锁定加载读取的值尚未写入，则条件存储将失败。 Rust 标准库通过在循环中调用这对指令并仅在条件存储成功时返回来在 ARM 上实现compare_exchange。 这使得清单 10-6 中的代码效率不必要地低下——我们最终得到了一个嵌套循环，它需要更多的指令并且更难优化。 由于在这种情况下我们已经有了一个循环，因此我们可以使用compare_exchange_weak，删除Err(false)上的unreachable!()，并在ARM上获得更好的机器代码，在x86上获得相同的编译代码！

### *获取方法*

fetch 方法（fetch_add、fetch_sub、fetch_and 等）旨在允许更有效地执行交换的原子操作，即无论执行顺序如何，都具有有意义的语义的操作。这样做的动机是compare_exchange 方法很强大，但成本也很高——如果两个线程都想更新单个原子变量，一个会成功，而另一个会失败并必须重试。 如果涉及许多线程，它们都必须调解对底层值的顺序访问，并且当线程在失败时重试时，将会有大量的旋转。

对于交换的简单操作，我们可以告诉 CPU 对原子变量执行什么操作，而不是仅仅因为另一个线程修改了值而失败并重试。 当 CPU 最终获得独占访问权时，无论当前值是什么，它都会执行该操作。 考虑一个 AtomicUsize，它计算线程池已完成的操作数。 如果两个线程同时完成一项作业，那么哪个线程先更新计数器并不重要，只要它们的增量都被计数即可。

fetch 方法实现这些类型的交换操作。 它们在一个步骤中执行读取和存储操作，并保证当原子变量准确保存方法返回的值时，对原子变量执行存储操作。 例如，AtomicUsize::fetch_add(1, Ordering::Relaxed) 永远不会失败——它总是在 AtomicUsize 的当前值上加 1，无论它是什么，并在该线程的 1 为 1 时精确地返回 AtomicUsize 的值。 添加。

fetch 方法往往比compare_exchange 更高效，因为当多个线程争用访问变量时，它们不需要线程失败并重试。 一些硬件架构甚至具有专门的获取方法实现，随着涉及的 CPU 数量的增长，这些实现可以更好地扩展。 然而，如果有足够多的线程尝试对同一个原子变量进行操作，这些操作将开始减慢并由于所需的协调而表现出次线性缩放。 一般来说，显着提高并发算法性能的最佳方法是将竞争变量拆分为更多竞争较少的原子变量，而不是从 Compare_exchange 切换到 fetch 方法。

**NOTE**  *fetch_update* *方法的命名有点具有欺骗性——在幕后，它实际上只是一个* *compare_exchange_weak* *循环，因此它的性能配置文件将更接近于* *compare_exchange* *的匹配 其他获取方法。*

## 健全的并发性

编写正确且高性能的并发代码比编写顺序代码更困难； 您不仅要考虑可能的执行交错，还要考虑代码如何与编译器、CPU 和内存子系统交互。 有了如此多的可用武器，您很容易想要举手投足并完全放弃并发。 在本节中，我们将探讨一些技术和工具，可以帮助确保您编写正确的并发代码而不必担心。

### *从简单开始*

生活中的事实是，简单、直接、易于理解的代码更有可能是正确的。 这一原则也适用于并发代码——始终从您能想到的最简单的并发设计开始，然后进行测量，只有当测量揭示出性能问题时，您才应该优化算法。

要在实践中遵循此技巧，请从不需要复杂使用原子或大量细粒度锁的并发模式开始。 从运行顺序代码并通过通道进行通信或通过锁进行协作的多个线程开始，然后根据您关心的工作负载对结果性能进行基准测试。 与实现奇特的无锁算法或将锁分成一千块以避免错误共享相比，这种方式犯错误的可能性要小得多。 对于许多用例来说，这些设计足够快； 事实证明，为了让通道和锁表现良好，我们投入了大量的时间和精力！ 如果简单的方法对于您的用例来说足够快，为什么还要引入更复杂且容易出错的代码呢？

如果您的基准测试表明存在性能问题，请准确找出系统的哪一部分可扩展性较差。 尽可能集中精力单独解决该瓶颈，并尝试在可能的情况下进行小幅调整。 也许将锁一分为二就足够了，而不是转移到并发哈希表，或者引入另一个线程和通道而不是实现无锁工作窃取队列。 如果是这样，就这样做。

即使您确实必须直接使用原子等，也要保持简单，直到证明需要优化为止 - 首先使用 Ordering::SeqCst 和compare_exchange，然后如果您发现具体证据表明这些正在成为必须的瓶颈，则进行迭代 受到照顾。

### *编写压力测试*

作为作者，您对代码中的错误可能隐藏的位置有很多了解，但不一定知道这些错误是什么（无论如何）。 编写压力测试是消除一些隐藏错误的好方法。 压力测试不一定执行复杂的步骤序列，而是有许多线程并行执行相对简单的操作。

例如，如果您正在编写并发哈希映射，则一个压力测试可能是让 *N* 个线程插入或更新键，并且 *M* 个线程以这样的方式读取键：这些 *M*+*N* 个线程可能会 经常选择相同的键。 这样的测试不会测试特定的结果或值，而是尝试触发许多可能的操作交错，希望有问题的交错可能会暴露出来。

压力测试在很多方面类似于模糊测试； 模糊测试会为给定函数生成许多随机输入，而压力测试则会生成许多随机线程和内存访问计划。 因此，就像模糊器一样，压力测试的好坏取决于代码中的断言； 他们无法告诉您未以某种易于发现的方式（例如断言失败或其他类型的恐慌）表现出来的错误。 因此，最好在低级并发代码中添加断言，或者如果您担心特别热的循环中的运行时成本，则使用 debug_assert_* 。

### *使用并发测试工具*

编写并发代码的主要挑战是处理不同线程执行交错的所有可能方式。 正如我们在清单 10-4 中的 Ordering::SeqCst 示例中看到的，重要的不仅仅是线程调度，还包括给定线程在任何给定时间点可以观察哪些内存值。 编写执行每个可能的合法执行的测试不仅乏味而且困难——您需要对哪些线程执行、何时执行以及它们的读取返回什么值进行非常低级的控制，而操作系统可能不提供这些控制。

#### 使用 Loom 进行模型检查

幸运的是，已经有一个工具可以以 loom crate 的形式为您简化这种执行探索。 考虑到本书和 Rust 板条箱的相对发布周期，我不会在这里给出任何如何使用 Loom 的示例，因为当你阅读本书时它们可能已经过时了，但我会给出 它的作用的概述。

Loom 希望您以闭包的形式编写专用测试用例，并将其传递到 Loom 模型中。 该模型跟踪所有跨线程交互，并尝试通过多次执行测试用例闭包来智能地探索这些交互的所有可能的迭代。 为了检测和控制线程交互，Loom 为标准库中允许线程相互协调的所有类型提供了替换类型； 其中包括 std::sync 和 std::thread 下的大多数类型以及 UnsafeCell 和其他一些类型。 Loom 希望您的应用程序在您运行 Loom 测试时使用这些替换类型。 替换类型与 Loom 执行器绑定并执行双重功能：它们充当重新调度点，以便 Loom 可以选择在每个可能的线程交互点之后接下来运行哪个操作，并且它们通知 Loom 要考虑的新的可能交错。 本质上，Loom 为可能存在多个执行交错的每个点构建一棵所有未来可能执行的树，然后尝试一个接一个地执行所有这些执行。

Loom 尝试全面探索您提供的测试用例的所有可能执行情况，这意味着它可以找到仅在极其罕见的执行中出现的错误，而压力测试在一百年内都找不到这些错误。 虽然这对于较小的测试用例来说非常有用，但将这种严格的测试应用于测试更多涉及的操作序列或需要同时运行多个线程的较大测试用例通常是不可行的。 Loom 需要很长时间才能获得适当的代码覆盖。 因此，在实践中，您可能希望告诉 Loom 仅考虑可能执行的子集，Loom 的文档对此有更多详细信息。

与压力测试一样，Loom 只能捕获表现为恐慌的错误，因此这是花一些时间在并发代码中放置策略断言的另一个原因！ 在许多情况下，甚至可能值得向并发代码添加额外的状态跟踪和簿记指令，以提供更好的断言。

#### 使用 ThreadSanitizer 进行运行时检查

对于较大的测试用例，最好的选择是在 Google 出色的 ThreadSanitizer（也称为 TSan）下通过几次迭代来运行测试。 TSan 通过在每次内存访问之前放置额外的簿记指令来自动增强您的代码。 然后，当您的代码运行时，这些簿记指令会更新并检查一个特殊的状态机，该状态机会标记任何指示有问题的竞争条件的并发内存操作。 例如，如果线程 B 写入某个原子值 X，但尚未与写入 X 的先前值的线程同步（此处需要大量挥手），则表明存在写/写竞争，这几乎总是一个错误。

由于 TSan 只观察您的代码运行，并且不会像 Loom 那样一遍又一遍地执行它，因此它通常只会给程序的运行时增加一个常数因子的开销。 虽然这个因素可能很重要（在撰写本文时为 5-15 倍），但它仍然足够小，您可以在合理的时间内执行最复杂的测试用例。

在撰写本文时，要使用 TSan，您需要使用 Rust 编译器的夜间版本并传入 -Zsanitizer=thread 命令行参数（或在 RUSTFLAGS 中设置），但希望及时这将成为受支持的标准 选项。 还可以使用其他清理程序来检查越界内存访问、释放后使用、内存泄漏和读取未初始化内存等内容，您可能也希望通过这些来运行并发测试套件！

> **Heisenbug** 
> 
> Heisenbug 是一种当你尝试研究它们时似乎就会消失的 bug。 当尝试调试高度并发的代码时，这种情况经常发生； 调试问题的附加工具会改变并发事件的相对时间，并可能导致触发错误的执行交错不再发生。
>
> 消失并发错误的一个特别常见的原因是使用打印语句，这是迄今为止最常见的调试技术之一。 print 语句对并发错误产生如此大的影响有两个原因。 第一个，也许是最明显的，相对而言，将某些内容打印到用户的终端（或任何标准输出点）需要相当长的时间，特别是当您的程序产生大量输出时。 写入终端至少需要往返操作系统内核才能执行写入，但写入可能还必须等待终端本身将进程的输出读取到自己的缓冲区中。 所有这些额外的时间可能会严重延迟先前与其他线程中的操作竞争的操作，以致竞争条件消失。
>
> print 语句扰乱并发执行模式的第二个原因是写入标准输出（通常）受到锁的保护。 如果您查看标准库中的 Stdout 类型，您会发现它包含一个互斥体，用于保护对输出流的访问。 这样做是为了，如果多个线程尝试同时写入，输出不会出现太严重的乱码——如果没有锁，给定的行可能会有多个线程写入中散布的字符，但有了锁，线程将轮流写入 反而 。 不幸的是，获取输出锁是另一个线程同步点，并且每个打印线程都涉及其中。 这意味着，如果您的代码之前由于两个线程之间缺少同步而被破坏，或者只是因为两个线程之间可能存在特定的竞争，那么添加 print 语句可能会修复该错误！
>
> 一般来说，当您发现看似 Heisenbug 的内容时，请尝试寻找其他方法来缩小问题范围。 这可能涉及使用 Loom 或 TSan、使用 gdb 或 lldb，或者使用仅在最后打印的每线程内存日志。 许多日志记录框架还努力避免发出日志事件的关键路径上的同步点，因此切换到其中之一可能会让您的生活更轻松。 作为额外的好处，修复特定错误后留下的良好日志记录可能会在以后派上用场。 就我个人而言，我是追踪箱的忠实粉丝，但那里有很多不错的选择。
## 总结

在本章中，我们首先介绍了并发 Rust 中常见的正确性和性能陷阱，以及成功的并发应用程序倾向于用来解决这些问题的一些高级并发模式。 我们还探讨了异步 Rust 如何在没有并行性的情况下实现并发，以及如何在异步 Rust 代码中显式引入并行性。 然后，我们深入研究 Rust 的许多不同的低级并发原语，包括它们如何工作、它们有何不同以及它们的用途。 最后，我们探索了编写更好的并发代码的技术，并研究了 Loom 和 TSan 等可以帮助您审查代码的工具。 在下一章中，我们将通过深入研究外部函数接口来继续我们的 Rust 较低级别的旅程，这些接口允许 Rust 代码直接链接到用其他语言编写的代码。