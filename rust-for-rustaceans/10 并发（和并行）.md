
通过本章，我希望为您提供有效利用 Rust 程序中的并发性、在库中实现对并发使用的支持以及正确使用 Rust 的并发原语所需的所有信息和工具。 我不会直接教你如何实现并发数据结构或编写高性能并发应用程序。 相反，我的目标是让您充分了解基本机制，以便您能够自行运用它们来完成您可能需要的任何任务。

并发有三种形式：单线程并发（就像我们在第 8 章中讨论的 async/await 一样）、单核多线程并发和多核并发，后者产生真正的并行性。

每种风格都允许程序中的并发任务以不同的方式交错执行。 如果考虑到操作系统调度和抢占的细节，还会有更多的子风味，但我们不会对此进行深入探讨。

在类型层面，Rust 仅代表并发的一个方面：多线程。 一种类型要么可供多个线程安全使用，要么不安全。 即使你的程序有多个线程（因此是并发的）但只有一个核心（因此不是并行的），Rust 也必须假设如果有多个线程，则可能存在并行性。 无论两个线程实际上是否并行执行，我们将讨论的大多数类型和技术都同样适用，因此为了保持语言简单，我将使用“并发”一词的非正式含义，即“运行更多的东西” 或更少同时”贯穿本章。 当区别很重要时，我会指出这一点。

Rust 基于类型的安全多线程方法的特别之处在于，它不是编译器的功能，而是开发人员可以扩展以开发复杂的并发合约的库功能。 由于线程安全性是通过 Send 和 Sync 实现和边界在类型系统中表达的，这些实现和边界一直传播到应用程序代码，因此整个程序的线程安全性仅通过类型检查来检查。

*Rust 编程语言* 已经涵盖了并发方面的大部分基础知识，包括发送和同步特征、Arc 和互斥体以及通道。 因此，我不会在这里重复太多内容，除非值得在其他主题的背景下专门重复一些内容。 相反，我们将看看是什么让并发变得困难，以及一些旨在解决这些困难的常见并发模式。 在深入研究如何使用原子操作来实现较低级别的并发操作之前，我们还将探讨并发和异步如何相互作用（以及它们如何不相互作用）。 最后，我将提供一些有关如何在使用并发代码时保持理智的建议来结束本章。
## 并发的麻烦

在我们深入研究并发编程的良好模式和 Rust 并发机制的细节之前，值得花一些时间来理解为什么并发首先具有挑战性。 也就是说，为什么我们需要并发代码的特殊模式和机制？

### *正确性*

并发的主要困难是协调对多个线程之间共享的资源的访问（尤其是写访问）。 如果许多线程想要共享资源只是为了读取它，那么这通常很简单：将其粘贴在 Arc 中或将其放置在可以获得 &'static 的东西中，然后就完成了。 但是，一旦任何线程想要写入，就会出现各种问题，通常以“数据争用”的形式出现。 简而言之，当一个线程更新共享状态，而第二个线程也在访问该状态（读取或更新它）时，就会发生数据争用。 如果没有额外的保护措施，第二个线程可能会读取部分覆盖的状态，破坏第一个线程写入的部分内容，或者根本看不到第一个线程的写入！ 一般来说，所有数据竞争都被视为未定义的行为。

数据竞争是更广泛的一类问题的一部分，这些问题主要（但并非完全）发生在并发环境中：*竞争条件*。 当一系列指令可能产生多种结果时，就会出现竞争条件，具体取决于系统中其他事件的相对时间。 这些事件可以是执行特定代码段的线程、计时器关闭、网络数据包进入或任何其他随时间变化的事件。 与数据竞争不同，竞争条件本质上并不是坏事，并且不被视为未定义的行为。 然而，当特别特殊的种族发生时，它们就会成为虫子的滋生地，正如你将在本章中看到的那样。

### *性能*

通常，开发人员将并发引入到他们的程序中，以期提高性能。 或者，更准确地说，他们希望并发性将使他们能够通过利用更多的硬件资源来每秒执行更多的操作。 这可以在单核上通过让一个线程在另一个线程等待时运行来完成，或者通过让线程同时工作（每个核上一个）来跨多个核完成，否则这将在一个核上串行发生。 大多数开发人员在谈论并发性时指的是后一种性能增益，这通常是在可扩展性方面进行的。 在这种情况下，可扩展性意味着“该程序的性能随着核心数量的增加而扩展”，这意味着如果您为程序提供更多核心，其性能就会提高。

虽然实现这样的加速是可能的，但它比看起来更难。 可扩展性的最终目标是线性可扩展性，其中核心数量加倍会使程序每单位时间完成的工作量加倍。 线性可扩展性通常也称为完美可扩展性。 然而，实际上很少有并发程序能够实现这样的加速。 次线性扩展更为常见，当您从一个核心增加到两个核心时，吞吐量几乎呈线性增加，但添加更多核心会产生收益递减。 有些程序甚至会经历负扩展，即让程序访问更多内核*降低*吞吐量，通常是因为许多线程都在争夺某些共享资源。

想象一下一群人试图戳破一张泡沫包装纸上的所有气泡可能会有所帮助——增加更多的人最初会有所帮助，但在某些时候，你会得到收益递减，因为拥挤会让任何人的工作变得更加困难。 如果所涉及的人员效率特别低，您的团队可能最终会站在一起讨论谁应该下一个弹出，并且根本不弹出任何气泡！ 这种应该并行执行的任务之间的干扰称为“竞争”，是良好扩展的大敌。 争用可以通过多种方式产生，但主要的罪魁祸首是互斥、共享资源耗尽和错误共享。

#### 互斥

当任何时候只允许一个并发任务执行一段特定的代码时，我们说该段代码的执行是互斥的——如果一个线程执行它，则没有其他线程可以同时执行它。 典型的例子是互斥锁，或“互斥锁”，它明确强制在任何时候只有一个线程可以进入程序代码的特定关键部分。 然而，相互排斥也可能隐含地发生。 例如，如果您启动一个线程来管理共享资源并通过 mpsc 通道向其发送作业，则该线程可以有效地实现互斥，因为一次只能执行一个此类作业。

当调用内部强制对关键部分进行单线程访问的操作系统或库调用时，也可能会发生互斥。 例如，多年来，标准内存分配器需要对某些分配进行互斥，这使得内存分配成为在其他高度并行程序中引发严重争用的操作。 类似地，许多操作系统操作看似应该是独立的，例如在同一目录中创建两个具有不同名称的文件，但最终可能必须在内核中按顺序发生。

**NOTE** *可扩展并发分配是* *jemalloc* *内存分配器存在的理由！*

互斥是并行加速最明显的障碍，因为根据定义，它强制串行执行程序的某些部分。 即使您使程序的其余部分与内核数量完美匹配，您可以获得的总加速也会受到互斥串行部分的长度的限制。 请注意相互排斥的部分，并设法将它们限制在严格必要的地方。

**NOTE** *对于理论上的人来说，由于代码互斥部分而导致的可实现加速的限制可以使用阿姆达尔定律来计算。*
#### 共享资源耗尽

不幸的是，即使您在任务中实现了完美的并发性，这些任务需要交互的环境本身也可能无法完全扩展。 内核每秒只能在给定的 TCP 套接字上处理一定数量的发送，内存总线一次只能执行一定数量的读取，并且 GPU 的并发能力有限。 这个问题无法治愈。 在实践中，这种环境通常是完美的可扩展性崩溃的地方，而对这种情况的修复往往需要大量的重新设计（甚至是新的硬件！），所以我们在本章中不会过多讨论这个主题。 请记住，可扩展性很少是您可以“实现”的东西，而更多的是您需要努力争取的东西。

#### 虚假分享

当两个不应该相互竞争的操作无论如何都发生竞争时，就会发生错误共享，从而阻止高效的同时执行。 通常会发生这种情况，因为两个操作碰巧在某些共享资源上交叉，即使它们使用该资源的不相关部分。

最简单的例子是锁过度共享，其中锁保护某些复合状态，并且两个原本独立的操作都需要获取锁来更新其状态的特定部分。 这又意味着操作必须串行执行，而不是并行执行。 在某些情况下，可以将单个锁分成两个，一个用于每个不相交的部分，这使得操作能够并行进行。 然而，像这样分割锁并不总是那么简单——状态可能共享一个锁，因为某些第三个操作需要锁定状态的所有部分。 通常你仍然可以分割锁，但是你必须小心不同线程获取分割锁的顺序，以避免当两个操作尝试以不同的顺序获取它们时可能发生的死锁（查找“哲学家就餐问题， ”如果你好奇的话）。 或者，对于某些问题，您可以通过使用底层算法的无锁版本来完全避免临界区，尽管这些也很难正确解决。 最终，虚假共享是一个很难解决的问题，并且没有一个包罗万象的解决方案，但识别问题是一个好的开始。

错误共享的一个更微妙的例子发生在 CPU 级别，正如我们在第 2 章中简要讨论的那样。CPU 在内部根据缓存行（内存中较长的连续字节序列）而不是单个字节对内存进行操作，以分摊成本 内存访问。 例如，在大多数 Intel 处理器上，高速缓存行大小为 64 字节。 这意味着每个内存操作实际上最终都会读取或写入 64 字节的某个倍数。 当两个核心想要更新恰好落在同一缓存行上的两个不同字节的值时，错误共享就会发挥作用； 这些更新必须按顺序执行，即使这些更新在逻辑上是不相交的。

这可能看起来太低级，无关紧要，但实际上，这种错误共享可能会大大降低应用程序的并行加速。 想象一下，您分配一个整数值数组来指示每个线程已完成多少个操作，但所有整数都落在同一缓存行中 - 现在，所有其他并行线程都将在该缓存行上争夺它们执行的每个操作。 如果操作相对较快，那么您的“大部分”执行时间可能最终会花在与这些计数器竞争上！

避免错误缓存行共享的技巧是填充您的值，使其等于缓存行的大小。 这样，两个相邻的值总是落在不同的缓存行上。 当然，这也会增加数据结构的大小，因此仅当基准测试表明存在问题时才使用此方法。

> **可扩展性的代价** 
> 
> 您应该注意的并发性的一个有点正交的方面是首先引入并发性的成本。 编译器确实很擅长优化单线程代码——毕竟，他们已经这样做很长时间了——而且单线程代码往往比单线程代码需要更少昂贵的保护措施（比如锁、通道或原子指令）。 并发代码可以。 总的来说，在给定任意数量的内核的情况下，并发的各种成本都会使并行程序比单线程程序慢！ 这就是为什么在优化和并行化之前和之后进行测量很重要：结果可能会让您感到惊讶。
>
> 如果您对这个主题感到好奇，我强烈建议您阅读 Frank McSherry 2015 年的论文“可扩展性！ 但代价是什么？ （https://www.frankmcsherry.org/assets/COST.pdf），其中揭示了一些特别令人震惊的“昂贵的扩展”示例。 

## 并发模型

Rust 提供了三种您经常会遇到的向程序添加并发性的模式：共享内存并发性、工作池和参与者。 详细介绍可以添加并发性的各种方法需要一本书，所以在这里我将只关注这三种模式。

### *共享内存*

从概念上讲，共享内存并发性非常简单：线程通过在它们之间共享的内存区域上进行操作来进行协作。 这可能采用由互斥锁保护的状态或存储在哈希映射中的形式，并支持来自多个线程的并发访问。 许多线程可能在不相交的数据上执行相同的任务，例如，如果许多线程在 Vec 的不相交子范围上执行某些功能，或者它们可能正在执行需要某些共享状态的不同任务，例如在数据库中，其中一个 线程处理用户对表的查询，而另一个线程优化用于在后台存储该表的数据结构。

当您使用共享内存并发时，您对数据结构的选择很重要，特别是当涉及的线程需要非常紧密地合作时。 常规互斥锁可能会阻止扩展超出极少数核心，读取器/写入器锁可能允许更多并发读取，但代价是写入速度较慢，分片读取器/写入器锁可能允许完全可扩展的读取，但代价是写入速度变慢 极具破坏性。 类似地，一些并发哈希映射旨在获得良好的全面性能，而另一些则专门针对写入很少的并发读取。 一般来说，在共享内存并发中，您希望使用专为尽可能接近目标用例而设计的数据结构，以便您可以利用优化来权衡应用程序不关心的性能方面 它所做的那些。

共享内存并发非常适合线程需要以不交换的方式联合更新某些共享状态的用例。 也就是说，如果一个线程必须使用某个函数 f 更新状态 s，而另一个线程必须使用某个函数 g 更新状态，并且 f(g(s)) != g(f(s))，则共享内存 并发可能是必要的。 如果情况并非如此，那么其他两种模式可能更适合，因为它们往往会带来更简单、性能更高的设计。

**NOTE** *一些问题已知算法可以在不使用锁的情况下提供并发共享内存操作。 随着核心数量的增加，这些无锁算法可能比基于锁的算法具有更好的扩展性，但由于其复杂性，它们的每核心性能通常也较慢。 一如既往，对于性能问题，首先进行基准测试，然后寻找替代解决方案。*

### *工作池*

在工作池模型中，许多相同的线程从共享作业队列接收作业，然后它们完全独立地执行。 例如，Web 服务器通常有一个处理传入连接的工作池，异步代码的多线程运行时倾向于使用工作池来共同执行应用程序的所有未来（或更准确地说，其顶级任务）。

共享内存并发和工作池之间的界限通常很模糊，因为工作池倾向于使用共享内存并发来协调它们如何从队列中获取作业以及如何将不完整的作业返回到队列。 例如，假设您正在使用数据并行库 rayon 对向量的每个元素并行执行某些功能。 在幕后，rayon 启动一个工作池，将向量分割成子范围，然后将子范围分发给池中的线程。 当池中的线程完成一个范围时，rayon 会安排它开始处理下一个未处理的子范围。 该向量在所有工作线程之间共享，并且线程通过支持工作窃取的共享内存队列式数据结构进行协调。

工作窃取是大多数工作池的一个关键特征。 基本前提是，如果一个线程提前完成其工作，并且没有更多未分配的工作可用，则该线程可以窃取已分配给其他工作线程但尚未启动的作业。 并非所有工作都需要相同的时间来完成，因此即使为每个工人提供相同“数量”的工作，某些工人最终可能会比其他工人更快地完成工作。 那些提前完成的线程应该帮助掉队的线程，以便更快地完成整个操作，而不是坐等那些需要长时间运行的作业的线程完成。

实现一种支持这种工作窃取的数据结构是一项相当艰巨的任务，而且不会因线程不断尝试相互窃取工作而产生大量开销，但这一功能对于高性能工作池至关重要。 如果您发现自己需要一个工作池，那么最好的选择通常是使用已经进行了大量工作的工作池，或者至少重用现有工作池的数据结构，而不是自己从头开始编写一个工作池 。

当每个线程执行的工作相同，但执行的数据不同时，工作池非常适合。 在人造丝并行映射操作中，每个线程执行相同的映射计算； 他们只是对基础数据的不同子集执行它。 在多线程异步运行时，每个线程只需调用 Future::poll; 他们只是对不同的未来有不同的称呼。 如果您开始必须区分线程池中的线程，则不同的设计可能更合适。

> **连接池** 
> 
> 连接池是一种共享内存结构，它保留一组已建立的连接并将它们分发给需要连接的线程。 这是图书馆中管理与外部服务的连接的常见设计模式。 如果一个线程需要连接但连接不可用，则要么建立一个新连接，要么强制该线程阻塞。 当线程完成连接后，它会将该连接返回到池中，从而使其可供其他可能正在等待的线程使用。
>
> 通常，连接池最困难的任务是管理连接生命周期。 连接可以以最后一个使用它的线程置于的任何状态返回到池中。 因此，连接池必须确保与连接相关的任何状态（无论是在客户端还是在服务器上）都已重置，以便当该连接随后被另一个线程使用时，该线程可以表现得好像它被赋予了一个新的状态一样。 ，专用连接。

### *Actors*

Actor 并发模型在很多方面与工作池模型相反。 工作池有许多共享作业队列的相同线程，而参与者模型有许多单独的作业队列，每个作业“主题”都有一个队列。 每个作业队列都会输入一个特定的参与者，该参与者处理与应用程序状态的子集相关的所有作业。 该状态可能是数据库连接、文件、指标收集数据结构或您可以想象的许多线程可能需要能够访问的任何其他结构。 不管是什么，单个参与者拥有该状态，如果某个任务想要与该状态交互，它需要向拥有该状态的参与者发送一条消息，总结它希望执行的操作。 当拥有者收到该消息时，它会执行指示的操作，并使用操作结果（如果相关）响应查询任务。

由于参与者对其内部资源具有独占访问权，因此除了消息传递所需之外，不需要任何锁或其他同步机制。

Actor 模型的一个关键点是 actor 都互相交谈。 例如，如果负责日志记录的参与者需要写入文件和数据库表，它可能会向负责每个文件和数据库表的参与者发送消息，要求他们执行相应的操作，然后继续执行下一个操作 记录事件。 通过这种方式，Actor 模型更像是一个网络，而不是轮子上的辐条——用户对 Web 服务器的请求可能会作为对负责该连接的 Actor 的单个请求开始，但可能会过渡性地产生数十个、数百个甚至更多的请求。 在用户的请求得到满足之前，系统会向更深层的参与者发送数千条消息。

Actor 模型中没有任何内容要求每个 Actor 都是自己的线程。 相反，大多数参与者系统建议应该有大量参与者，因此每个参与者应该映射到一个任务而不是一个线程。 毕竟，参与者仅在执行时才需要对其包装的资源进行独占访问，并且不关心它们是否位于自己的线程上。 事实上，actor 模型经常与工作池模型结合使用，例如，使用多线程异步运行时 Tokio 的应用程序可以为每个 actor 生成一个异步任务，然后 Tokio 将执行 每个演员在其工作池中都有一份工作。 因此，当一个给定的 Actor 产生并恢复时，它的执行可能会在工作池中从一个线程移动到另一个线程，但每次 Actor 执行时，它都会保持对其包装资源的独占访问。

Actor 并发模型非常适合当您拥有许多可以相对独立运行的资源，并且每个资源内很少或根本没有并发机会的情况。 例如，操作系统可能有一个负责每个硬件设备的参与者，而 Web 服务器可能有一个负责每个后端数据库连接的参与者。 如果您只需要几个参与者，如果参与者之间的工作明显不平衡，或者如果某些参与者变得很大，那么参与者模型就不能很好地工作 - 在所有这些情况下，您的应用程序最终可能会遇到执行瓶颈 系统中单个参与者的速度。 由于每个参与者都希望能够独占访问他们的一小部分世界，因此您无法轻松地并行执行该瓶颈参与者。

## 异步和并行

正如我们在第 8 章中讨论的，Rust 中的异步实现了无需并行的并发——我们可以使用 select 和 join 等结构让单个线程轮询多个 future，并在其中一个、部分或全部完成时继续。 由于不涉及并行性，因此与 future 的并发从根本上不需要发送这些 future。 即使生成一个 future 作为额外的顶级任务运行，从根本上来说也不需要 Send，因为单个执行器线程可以同时管理多个 future 的轮询。

然而，在“大多数”情况下，应用程序既需要并发性又需要并行性。 例如，如果 Web 应用程序为每个传入连接构建一个 future，因此同时具有许多活动连接，则它可能希望异步执行器能够利用主机上的多个核心。 这不会自然发生：您的代码必须明确告诉执行器哪些 future 可以并行运行，哪些不能。

特别是，必须向执行器提供两条信息，以使其知道它可以将 future 中的工作分散到线程工作池中。 首先，所涉及的 future 是 Send 的——如果不是，则执行器不允许将 future 发送到其他线程进行处理，并且不可能实现并行性； 只有构建此类 future 的线程才能轮询它们。

第二个信息是如何将future拆分成可以独立运行的任务。 这与第 8 章中关于任务与 future 的讨论有关：如果一个巨大的 Future 包含许多 Future 实例，而这些实例本身对应于可以并行运行的任务，则执行器仍然必须对顶级 Future 调用 poll，并且它 必须从单个线程执行此操作，因为 poll 需要 &mut self。 因此，为了实现 future 的并行性，您必须显式地生成您希望能够并行运行的 future。 另外，由于第一个要求，您用来执行此操作的执行器函数将要求传入的 Future 是 Send。

> **异步同步原语** 
> 
> 大多数用于阻塞代码的同步原语（例如 std::sync）也有异步对应物。 通道、互斥锁、读/写锁、屏障和各种其他类似结构都有异步变体。 我们需要这些是因为，正如第 8 章所讨论的，在 future 中阻塞会阻碍执行器可能需要做的其他工作，因此是不可取的。
>
> 然而，这些原语的异步版本通常比同步版本慢，因为需要额外的机制来执行必要的唤醒。 因此，即使在异步上下文中，只要使用不会有阻塞执行器的风险，您可能也希望使用同步原语。 例如，虽然获取互斥体通常可能会阻塞很长时间，但对于特定的互斥体来说可能并非如此，因为该互斥体可能很少被获取，而且只在很短的时间内被获取。 在这种情况下，在互斥体再次可用之前短暂阻塞实际上可能不会导致任何问题。 您需要确保在持有 MutexGuard 时永远不会放弃或执行其他长时间运行的操作，但除非您不应该遇到问题。
>
> 不过，与此类优化一样，请确保首先进行测量，并且仅选择同步原语（如果它可以显着提高性能）。 如果不这样做，那么通过在异步上下文中使用同步原语引入的额外的枪可能不值得。

## 更低层的并发

标准库提供了 std::sync::atomic 模块，该模块提供对底层 CPU 原语的访问，以及构建通道和互斥体等更高级别的结构。 这些原语以原子类型的形式出现，其名称以 Atomic 开头 — AtomicUsize、AtomicI32、AtomicBool、AtomicPtr 等 — Ordering 类型，以及名为 fence 和 compiler_fence 的两个函数。 我们将在接下来的几节中逐一讨论。

这些类型是用于构建必须在线程之间通信的任何代码的块。 互斥体、通道、屏障、并发哈希表、无锁堆栈和所有其他同步结构最终都依赖于这几个原语来完成它们的工作。 它们本身还可以在线程之间的轻量级合作中派上用场，其中重量级同步（如互斥锁）过多，例如，增加共享计数器或将共享布尔值设置为 true。

原子类型的特殊之处在于，它们定义了当多个线程尝试同时访问它们时发生的情况的语义。 这些类型都支持（大部分）相同的 API：load、store、fetch_* 和 Compare_exchange。 在本节的其余部分中，我们将了解它们的用途、如何正确使用它们以及它们的用途。 但首先，我们必须讨论低级内存操作和内存排序。

### *内存操作*

非正式地，我们经常将访问变量称为“读取”或“写入”内存。 实际上，代码之间的许多机制都使用变量和访问内存硬件的实际 CPU 指令。 为了理解并发内存访问的行为方式，至少在高层次上了解该机制非常重要。

当程序读取变量的值或为其分配新值时，编译器决定发出哪些指令。 允许对代码执行各种转换和优化，最终可能会重新排序程序语句、消除它认为冗余的操作，或者使用 CPU 寄存器而不是实际内存来存储中间计算。 编译器在这些转换上受到许多限制，但最终只有变量访问的一个子集实际上最终作为内存访问指令。

在 CPU 级别，内存指令有两种主要形式：加载和存储。 加载将字节从内存中的某个位置拉入 CPU 寄存器，而存储将字节从 CPU 寄存器存储到内存中的某个位置。 加载和存储一次对小块内存进行操作：在现代 CPU 上通常为 8 字节或更少。 如果变量访问跨越的字节数多于单个加载或存储所能访问的字节数，则编译器会根据需要自动将其转换为多个加载或存储指令。 CPU 在如何执行程序指令方面也有一定的余地，以更好地利用硬件并提高程序性能。 例如，当现代 CPU 彼此不依赖时，它们通常会并行执行指令，甚至乱序执行指令。 每个 CPU 和计算机的 DRAM 之间还有多层缓存，这意味着给定内存位置的负载可能不一定会按照挂钟时间看到该内存位置的最新存储。

在大多数代码中，编译器和 CPU 只能以不影响结果程序语义的方式转换代码，因此这些转换对程序员来说是不可见的。 然而，在并行执行的上下文中，这些转换可能会对应用程序行为产生重大影响。 因此，CPU 通常提供加载和存储指令的多种不同变体，每种变体对于 CPU 如何重新排序以及它们如何与其他 CPU 上的并行操作交错具有不同的保证。 类似地，编译器（或者更确切地说，编译器编译的语言）提供不同的注释，您可以使用它们来强制对其内存访问的某些子集施加特定的执行约束。 在 Rust 中，这些注释以原子类型及其方法的形式出现，我们将在本节的其余部分中对它们进行分解。
### *原子类型*

Rust 的原子类型之所以如此命名，是因为它们可以被原子地访问——也就是说，原子类型变量的值是一次性写入的，并且永远不会使用多个存储来写入，从而保证该变量的加载无法观察到该变量的值。 只有组成该值的某些字节发生了更改，而其他字节尚未更改。 通过与非原子类型的对比，这是最容易理解的。 例如，将新值重新分配给类型 (i64, i64) 的元组通常需要两条 CPU 存储指令，每个 8 字节值一条。 如果一个线程要执行这两个存储，则另一个线程可能（如果我们暂时忽略借用检查器）在第一个存储之后但在第二个存储之前读取元组的值，从而最终得到元组值的不一致视图 。 它最终会读取第一个元素的新值和第二个元素的旧值，该值从未被任何线程实际存储过。

CPU 只能以原子方式访问特定大小的值，因此只有少数原子类型，所有这些都位于原子模块中。 每个原子类型都是 CPU 支持原子访问的大小之一，具有多种变化，例如值是否带符号以及区分原子 usize 和指针（与 usize 大小相同）。 此外，原子类型具有用于加载和存储它们所保存的值的显式方法，以及一些我们稍后将讨论的更复杂的方法，以便程序员编写的代码和生成的 CPU 指令之间的映射更加清晰。 例如，AtomicI32::load 执行有符号 32 位值的单次加载，AtomicPtr::store 执行指针大小（64 位平台上的 64 位）值的单次存储。

### *内存排序*

原子类型上的大多数方法都采用 Ordering 类型的参数，该参数指示原子操作所受到的内存排序限制。 在不同的线程中，原子值的加载和存储可以由编译器和CPU仅以与对该原子值的每个原子操作所请求的存储器排序兼容的交错来排序。 在接下来的几节中，我们将看到一些示例，说明为什么对顺序的控制对于从编译器和 CPU 中获得预期的语义非常重要且必要。

内存排序通常是违反直觉的，因为我们人类喜欢从上到下阅读程序并想象它们是逐行执行的，但这并不是代码在到达硬件时实际执行的方式。 内存访问可以重新排序，甚至完全消除，并且一个线程上的写入可能不会立即对其他线程可见，即使已经观察到了按程序顺序进行的后续写入。

可以这样想：每个内存位置都会看到来自不同线程的修改序列，并且不同内存位置的修改序列是独立的。 如果两个线程 T1 和 T2 都写入内存位置 M，那么即使用户用秒表测量 T1 首先执行，T2 对 M 的写入可能仍然看起来首先发生，因为 M 没有两个线程之间的任何其他约束 执行。 本质上，*计算机在确定给定内存位置的值时不会考虑挂钟时间*——重要的是程序员对构成有效执行的执行约束。 例如，如果 T1 写入 M，然后生成线程 T2，然后线程 T2 写入 M，则计算机必须将 T1 的写入识别为首先发生，因为 T2 的存在依赖于 T1。

如果这很难理解，请不要担心——内存排序可能会令人费解，并且语言规范往往使用非常精确但不是很直观的措辞来描述它。 我们可以通过关注底层硬件架构来构建一个更容易掌握（如果稍微简化的话）的心理模型。 基本上，您的计算机内存被构造为树状存储层次结构，其中叶是 CPU 寄存器，根是物理内存芯片上的存储（通常称为主内存）。 两者之间是多层缓存，并且层次结构的不同层可以驻留在不同的硬件上。 当线程对内存位置执行存储时，真正发生的情况是 CPU 启动对给定 CPU 寄存器中的值的写入请求，然后该请求必须沿着内存层次结构向上到达主内存。 当线程执行加载时，请求沿层次结构向上流动，直到到达具有可用值的层，然后从那里返回。 问题就在这里：在写入的内存位置的所有缓存都被更新之前，写操作并不到处可见，但其他 CPU 可以同时针对同一内存位置执行指令，这就出现了奇怪的情况。 那么，内存排序是一种请求精确语义的方法，用于了解当多个 CPU 访问特定内存位置以执行特定操作时会发生什么情况。

考虑到这一点，让我们看一下排序类型，这是我们作为程序员可以规定并发执行有效的附加约束的主要机制。

排序被定义为一个枚举，其变体如清单 10-1 所示。

```rust
enum Ordering {
    Relaxed,
    Release,
    Acquire,
    AcqRel,
    SeqCst
}
```

*Listing 10-1: The definition of Ordering*

Each of these places different restrictions on the mapping from source code to execution semantics, and we’ll explore each one in turn in the remainder of this section. 

#### 宽松的排序

除了访问是原子的这一事实之外，宽松的排序本质上不能保证对值的并发访问。 特别是，宽松的排序不能保证不同线程之间内存访问的相对顺序。 这是最弱的内存排序形式。 清单 10-2 显示了一个简单的程序，其中两个线程使用 Ordering::Relaxed 访问两个原子变量。

``` rust
static X: AtomicBool = AtomicBool::new(false);
static Y: AtomicBool = AtomicBool::new(false);

let t1 = spawn(|| {
 1 let r1 = Y.load(Ordering::Relaxed); 2 X.store(r1, Ordering::Relaxed); 
 });
let t2 = spawn(|| {
3 let r2 = X.load(Ordering::Relaxed); 

4 Y.store(true, Ordering::Relaxed) }); 

```

Listing 10-2: Two racing threads with *Ordering::Relaxed* 

查看作为 t2 生成的线程，您可能会认为 r2 永远不会为 true，因为所有值都是 false，直到同一线程*在*读取 X 后将 true 分配给行上的 Y。但是，通过宽松的内存排序，该结果 是完全有可能的。 原因是 CPU 可以重新排序所涉及的加载和存储。 让我们详细了解一下这里到底发生了什么，使得 r2 = true 成为可能。 首先，CPU 注意到 4 不必在 3 之后发生，因为 4 不使用 3 的任何输出或副作用。也就是说，4 对 3 没有执行依赖性。因此，CPU 决定对它们重新排序 *挥手*会让你的程序运行得更快的原因。 因此，CPU 会先执行 4，设置 Y = true，即使 3 尚未运行。 然后，操作系统将 t2 置于睡眠状态，线程 t1 执行一些指令，或者 t1 只是在另一个内核上执行。 在 t1 中，编译器确实必须先运行 1，然后运行 2，因为 2 取决于 1 中读取的值。因此，t1 将 Y（由 4 写入）中的 true 读入 r1，然后将其写回 X。最后，t2 执行 3，读取 X 并得到 true，如 2 所写。

宽松的内存排序允许这种执行，因为它不对并发执行施加额外的约束。 也就是说，在宽松的内存排序下，编译器必须仅确保尊重任何给定线程的执行依赖性（就像不涉及原子一样）； 它不需要对并发操作的交错做出任何承诺。 单线程执行允许重新排序 3 和 4，因此在宽松排序下也允许。

在某些情况下，这种重新排序是可以的。 例如，如果您有一个仅跟踪指标的计数器，那么它相对于其他指令的具体执行时间并不重要，并且 Ordering::Relaxed 就可以了。 在其他情况下，这可能是灾难性的：例如，如果您的程序使用 r2 来确定是否已经设置了安全保护，因此最终错误地认为它们已经设置了。

在编写不花哨地使用原子的代码时，您通常不会注意到这种重新排序 - CPU 必须保证编写的代码与每个线程实际执行的代码之间没有明显的差异，因此一切看起来都在运行 按照你写的顺序。 这称为尊重程序顺序或评估顺序； 这些术语是同义词。

#### #### 获取/释放顺序

在内存排序层次结构的下一步，我们有 Ordering::Acquire、Ordering::Release 和 Ordering::AcqRel（获取加释放）。 在较高级别上，它们在一个线程中的存储与另一个线程中的加载之间建立执行依赖性，然后限制如何相对于该加载和存储重新排序操作。 至关重要的是，这些依赖关系不仅建立了单个值的存储和加载之间的关系，而且还对所涉及的线程中的*其他*加载和存储施加了排序约束。 这是因为每次执行都必须遵守程序顺序； 如果线程 B 中的加载依赖于线程 A 中的某个存储（A 中的存储必须在 B 中的加载之前执行），则该加载之后 B 中的任何读取或写入也必须在 A 中的该存储之后发生。

**N O T E**  *获取内存排序仅适用于加载，释放仅适用于存储，以及 AcqRel 仅适用于同时加载和存储的操作（如fetch_add）。*

Concretely, these memory orderings place the following restrictions on execution: 

1. Loads and stores cannot be moved forward past a store with Ordering::Release. 
2. Loads and stores cannot be moved back before a load with Ordering::Acquire. 
3. An Ordering::Acquire load of a variable must see all stores that happened before an Ordering::Release store that stored what the load loaded. 


**N O T E**  To see how these memory orderings change things, Listing 10-3 shows Listing 10-2 again but with the memory ordering swapped out for Acquire and Release. 

``` rust
static X: AtomicBool = AtomicBool::new(false);
static Y: AtomicBool = AtomicBool::new(false);
let t1 = spawn(|| {
    let r1 = Y.load(Ordering::Acquire);
    X.store(r1, Ordering::Release);
});
let t2 = spawn(|| {
1 let r2 = X.load(Ordering::Acquire); 
2 Y.store(true, Ordering::Release) }); 
```

Listing 10-3: Listing 10-2 with *Acquire/Release* memory ordering 

这些额外的限制意味着 t2 不再可能看到 r2 = true。 要了解原因，请考虑清单 10-2 中奇怪结果的主要原因：1 和 2 的重新排序。第一个限制是对具有 Ordering::Release 的存储，规定我们不能将 1 移动到 2 以下，因此我们“ 一切都好！

但这些规则的用处超出了这个简单的例子。 例如，假设您实现了互斥锁。 您希望确保线程在持有锁时运行的任何加载和存储操作仅在其实际持有锁时执行，并且对稍后获取锁的任何线程可见。 这正是“发布”和“获取”使您能够执行的操作。 通过执行Release store来释放锁和Acquire load来获取锁，您可以保证临界区中的加载和存储永远不会移动到实际获取锁之前或释放锁之后！

*On some CPU architectures, like x86,* *Acquire/Release* *ordering is guaranteed by the hardware, and there is no additional cost to using* *Ordering::Release* *and* *Ordering::Acquire* *over* *Ordering::Relaxed**. On other architectures that is not the case, and your program may see speedups if you switch to* *Relaxed* *for atomic operations that can tolerate the weaker memory ordering guarantees.* 

#### Sequentially Consistent Ordering

顺序一致排序（Ordering::SeqCst）是我们可以访问的最强的内存排序。 它的确切保证有点难以确定，但从广义上讲，它不仅要求每个线程看到与获取/释放一致的结果，而且要求所有线程看到彼此“相同”的顺序。 通过与获取和释放的行为对比可以最好地看出这一点。 具体来说，获取/释放顺序*不*保证如果两个线程 A 和 B 以原子方式加载由另外两个线程 X 和 Y 写入的值，A 和 B 将看到 X 相对于 Y 写入时的一致模式。这是相当抽象的， 因此，请考虑清单 10-4 中的示例，该示例显示了获取/释放排序可能会产生意外结果的情况。 之后，我们将看到顺序一致的排序如何避免特定的意外结果。

``` rust
static X: AtomicBool = AtomicBool::new(false);
static Y: AtomicBool = AtomicBool::new(false);
static Z: AtomicI32 = AtomicI32::new(0);
let t1 = spawn(|| {
    X.store(true, Ordering::Release);
});
let t2 = spawn(|| {
    Y.store(true, Ordering::Release);
});
let t3 = spawn(|| {
    while (!X.load(Ordering::Acquire)) {}
	1 if (Y.load(Ordering::Acquire)) { Z.fetch_add(1, Ordering::Relaxed); } 
});
let t4 = spawn(|| {
	while (!Y.load(Ordering::Acquire)) {} 2 if (X.load(Ordering::Acquire)) { 
        Z.fetch_add(1, Ordering::Relaxed); }
});
```


Listing 10-4: Weird results with *Acquire/Release* ordering 

两个线程 t1 和 t2 分别将 X 和 Y 设置为 true。 线程 t3 等待 X 为 true； 一旦 X 为真，它会检查 Y 是否为真，如果是，则将 Z 加 1。线程 t4 而是等待 Y 变为真，然后检查 X 是否为真，如果是，则将 Z 加 1。 问题是：所有线程终止后 Z 的可能值是多少？ 在我向您展示答案之前，请根据上一节中发布和获取顺序的定义尝试解决该问题。

首先，让我们回顾一下 Z 递增的条件。 如果线程 t3 在观察到 X 为真后发现 Y 为真，则递增 Z，只有当 t2 在 t3 评估负载为 1 之前运行时才会发生这种情况。相反，如果线程 t4 在观察到 X 为真后发现 X 为真，则递增 Z Y 为 true，因此仅当 t1 在 t4 评估 2 处的负载之前运行时。为了简化说明，我们现在假设每个线程一旦运行就运行完成。

从逻辑上讲，如果线程按照 1、2、3、4 的顺序运行，则 Z 可以递增两次——X 和 Y 都设置为 true，然后 t3 和 t4 运行发现满足递增 Z 的条件。 类似地，如果线程按 1、3、2、4 的顺序运行，则 Z 可以简单地只递增一次。这满足 t4 递增 Z 的条件，但不满足 t3 的条件。 然而，让 Z 为 0“似乎”是不可能的：如果我们想阻止 t3 递增 Z，则 t2 必须在 t3 之后运行。 由于 t3 仅在 t1 之后运行，这意味着 t2 在 t1 之后运行。 但是，t4 在 t2 运行之后才会运行，因此 t1 必须已运行并在 t4 运行时将 X 设置为 true，因此 t4 将递增 Z。

我们无法让 Z 为 0 主要源于我们人类倾向于线性解释； 这发生了，然后这发生了，然后这发生了。 计算机不会受到同样的限制，也不需要将所有事件放入一个全局顺序中。 Release 和 Acquire 的规则中没有任何内容规定 t3 必须遵守与 t4 相同的 t1 和 t2 执行顺序。 就计算机而言，可以让 t3 观察到 t1 先执行，而让 t4 观察 t2 先执行。 考虑到这一点，t3 在观察到 X 为真之后观察到 Y 为假（意味着 t2 在 t1 之后运行），而在同一执行中 t4 在观察到 Y 为真之后观察到 X 为假（ 暗示 t2 在 t1 之前运行，是完全合理的，即使这对我们人类来说似乎令人难以容忍。

正如我们之前讨论的，获取/释放仅要求变量的 Ordering::Acquire 加载必须看到在存储加载加载内容的 Ordering::Release 存储之前发生的所有存储。 在刚刚讨论的排序中，计算机*确实*维护了该属性：t3 看到 X == true，并且确实在设置 X = true 之前看到 t1 的所有存储 — 没有。 它还看到 Y == false，它是在程序启动时由主线程存储的，因此没有任何相关的存储需要关注。 类似地，t4 看到 Y = true，并且还看到 t2 在设置 Y = true 之前的所有存储 — 同样，没有存储。 它还看到 X == false，它是由主线程存储的并且没有先前的存储。 没有违反任何规则，但不知何故似乎是错误的。

我们的直觉期望是，我们可以将线程按某种全局顺序排列，以了解每个线程所看到和所做的事情，但本示例中的获取/释放顺序并非如此。 为了实现更接近直观期望的东西，我们需要顺序一致性。 顺序一致性要求参与原子操作的所有线程进行协调，以确保每个线程观察到的内容对应于（或至少看起来对应于）*某些*单一的、常见的执行顺序。 这使得推理变得更容易，但也使其成本高昂。

用 Ordering::SeqCst 标记的原子加载和存储指示编译器采取任何额外的预防措施（例如使用特殊的 CPU 指令）来保证这些加载和存储的顺序一致性。 围绕此问题的确切形式相当复杂，但顺序一致性本质上确保了如果您查看所有线程中的所有相关 SeqCst 操作，您可以将线程执行按*某些*顺序排列，以便加载和存储的值 都会匹配的。

如果我们用 SeqCst 替换清单 10-4 中的所有内存排序参数，那么在所有线程退出后，Z 不可能为 0，正如我们最初预期的那样。 在顺序一致性下，必须可以说 t1 肯定在 t2 之前运行，或者 t2 肯定在 t1 之前运行，因此不允许 t3 和 t4 看到不同顺序的执行，因此 Z 不能为 0。

### *Compare And Exchange*

除了加载和存储之外，Rust 的所有原子类型都提供了一个名为compare_exchange 的方法。 此方法用于原子地*和有条件地*替换值。 您向compare_exchange提供您观察到的原子变量的最后一个值以及您想要用来替换原始值的新值，并且仅当该值仍然与您上次观察到它时相同时，它才会替换该值。 要了解为什么这很重要，请看一下清单 10-5 中互斥锁的（损坏的）实现。 此实现跟踪静态原子变量 LOCK 中是否持有锁。 我们使用布尔值 true 来表示锁定已被持有。 要获取锁，线程等待 LOCK 为 false，然后再次将其设置为 true； 然后，它进入其临界区并将 LOCK 设置为 false，以在其工作 (f) 完成时释放锁定。

``` rust
static LOCK: AtomicBool = AtomicBool::new(false);
fn mutex(f: impl FnOnce()) {
    // Wait for the lock to become free (false).
    while LOCK.load(Ordering::Acquire)
      { /* .. TODO: avoid spinning .. */ }
    // Store the fact that we hold the lock.
    LOCK.store(true, Ordering::Release);
    // Call f while holding the lock.
f(); 
    // Release the lock.
    LOCK.store(false, Ordering::Release);
}
```

Listing 10-5: An incorrect implementation of a mutual exclusion lock 

这在大多数情况下是有效的，但它有一个可怕的缺陷——两个线程可能同时看到 LOCK == false 并且都离开 while 循环。 然后他们都将 LOCK 设置为 true 并且都进入临界区，这正是互斥函数应该阻止的！

清单 10-5 中的问题是，我们加载原子变量的当前值和随后更新它之间存在一个间隙，在此期间另一个线程可能会运行并读取或触摸它的值。 Compare_exchange 正是解决了这个问题——只有当原子变量的值仍然与之前读取的值匹配时，它才会交换原子变量后面的值，否则通知您该值已更改。 清单 10-6 显示了使用compare_exchange 的更正实现。

``` rust
static LOCK: AtomicBool = AtomicBool::new(false);
fn mutex(f: impl FnOnce()) {
    // Wait for the lock to become free (false).
    loop {
      let take = LOCK.compare_exchange(
          false,
          true,
          Ordering::AcqRel,
          Ordering::Relaxed
      );
      match take {
        Ok(false) => break,
        Ok(true) | Err(false) => unreachable!(),
        Err(true) => { /* .. TODO: avoid spinning .. */ }
      }
	} 
	// Call f while holding the lock.
	
	f(); 
    // Release the lock.
    LOCK.store(false, Ordering::Release);
}

```
Listing 10-6: A corrected implementation of a mutual exclusion lock 

这一次，我们在循环中使用compare_exchange，它负责检查当前是否未持有锁并存储 true 以适当地获取锁。 这分别通过 Compare_exchange 的第一个和第二个参数发生：在本例中为 false，然后为 true。 您可以将调用理解为“仅当当前值为 false 时才存储 true”。 Compare_exchange 方法返回一个 Result，指示该值已成功更新 (Ok) 或无法更新 (Err)。 无论哪种情况，它都会返回当前值。 这对于 AtomicBool 来说不太有用，因为我们知道如果操作失败，该值必须是什么，但对于 AtomicI32 之类的东西，更新的当前值将让您快速重新计算要存储的内容，然后重试，而无需执行另一次操作 加载。

**N O T E** *注意  compare_exchange 仅检查该值是否与作为当前值传入的值相同。 如果其他线程修改了原子变量的值，然后再次将其重置为原始值，则该变量上的  compare_exchange  仍然会成功。 这通常称为 A-B-A 问题。*

与简单的加载和存储不同，compare_exchange 采用*两个* 排序参数。 第一个是“成功排序”，它规定了在值成功更新的情况下，compare_exchange 表示的加载和存储应使用什么内存排序。 第二个是“失败排序”，如果加载的值与预期的当前值不匹配，它指示加载的内存排序。 这两种排序是分开的，以便开发人员可以在适当的时候通过在失败时重新排序加载和存储来为 CPU 提供提高执行性能的余地，但在成功时仍然获得正确的排序。 在这种情况下，可以在锁获取循环的失败迭代中重新排序加载和存储，但在关键部分内重新排序加载和存储以使其最终位于关键部分之外是“不”可以的。

尽管它的接口很简单，compare_exchange 却是一个非常强大的同步原语，以至于从理论上证明，您可以仅使用compare_exchange 构建所有其他分布式共识原语！ 因此，当您真正深入研究实现细节时，它是许多（如果不是大多数）同步构造的主力。

但请注意，compare_exchange 要求单个 CPU 对底层值具有独占访问权限，因此它是硬件级别的一种互斥形式。 这反过来意味着compare_exchange很快就会成为可扩展性瓶颈：一次只有一个CPU可以取得进展，因此您的代码的一部分不会随着核心数量的增加而扩展。 事实上，情况可能比这更糟糕——CPU 必须进行协调，以确保一次只有一个 CPU 在对变量进行比较交换时成功（如果您对 MESI 协议的工作原理感到好奇，请查看 MESI 协议），并且 涉及的 CPU 越多，协调成本就会呈二次方增长！

> **COMPARE_EXCHANGE_WEAK** 
> 
> 细心的文档读者会注意到，compare_exchange 有一个可疑的表兄弟，compare_exchange_weak，并想知道其中的区别是什么。 即使原子变量的值仍然与用户传入的预期值匹配，compare_exchange 的弱变体也允许失败，而在这种情况下，强变体必须成功。
>
> 这可能看起来很奇怪——除非值发生了变化，否则原子值交换怎么会失败呢？ 答案在于没有本机compare_exchange 操作的系统体系结构。 例如，ARM 处理器改为具有锁定加载和条件存储操作，其中如果自加载以来相关锁定加载读取的值尚未写入，则条件存储将失败。 Rust 标准库通过在循环中调用这对指令并仅在条件存储成功时返回来在 ARM 上实现compare_exchange。 这使得清单 10-6 中的代码效率不必要地低下——我们最终得到了一个嵌套循环，它需要更多的指令并且更难优化。 由于在这种情况下我们已经有了一个循环，因此我们可以使用compare_exchange_weak，删除Err(false)上的unreachable!()，并在ARM上获得更好的机器代码，在x86上获得相同的编译代码！

### *获取方法*

fetch 方法（fetch_add、fetch_sub、fetch_and 等）旨在允许更有效地执行交换的原子操作，即无论执行顺序如何，都具有有意义的语义的操作。这样做的动机是compare_exchange 方法很强大，但成本也很高——如果两个线程都想更新单个原子变量，一个会成功，而另一个会失败并必须重试。 如果涉及许多线程，它们都必须调解对底层值的顺序访问，并且当线程在失败时重试时，将会有大量的旋转。

对于交换的简单操作，我们可以告诉 CPU 对原子变量执行什么操作，而不是仅仅因为另一个线程修改了值而失败并重试。 当 CPU 最终获得独占访问权时，无论当前值是什么，它都会执行该操作。 考虑一个 AtomicUsize，它计算线程池已完成的操作数。 如果两个线程同时完成一项作业，那么哪个线程先更新计数器并不重要，只要它们的增量都被计数即可。

fetch 方法实现这些类型的交换操作。 它们在一个步骤中执行读取和存储操作，并保证当原子变量准确保存方法返回的值时，对原子变量执行存储操作。 例如，AtomicUsize::fetch_add(1, Ordering::Relaxed) 永远不会失败——它总是在 AtomicUsize 的当前值上加 1，无论它是什么，并在该线程的 1 为 1 时精确地返回 AtomicUsize 的值。 添加。

fetch 方法往往比compare_exchange 更高效，因为当多个线程争用访问变量时，它们不需要线程失败并重试。 一些硬件架构甚至具有专门的获取方法实现，随着涉及的 CPU 数量的增长，这些实现可以更好地扩展。 然而，如果有足够多的线程尝试对同一个原子变量进行操作，这些操作将开始减慢并由于所需的协调而表现出次线性缩放。 一般来说，显着提高并发算法性能的最佳方法是将竞争变量拆分为更多竞争较少的原子变量，而不是从 Compare_exchange 切换到 fetch 方法。

**NOTE**  *fetch_update* *方法的命名有点具有欺骗性——在幕后，它实际上只是一个* *compare_exchange_weak* *循环，因此它的性能配置文件将更接近于* *compare_exchange* *的匹配 其他获取方法。*

## 健全的并发性

编写正确且高性能的并发代码比编写顺序代码更困难； 您不仅要考虑可能的执行交错，还要考虑代码如何与编译器、CPU 和内存子系统交互。 有了如此多的可用武器，您很容易想要举手投足并完全放弃并发。 在本节中，我们将探讨一些技术和工具，可以帮助确保您编写正确的并发代码而不必担心。

### *从简单开始*

生活中的事实是，简单、直接、易于理解的代码更有可能是正确的。 这一原则也适用于并发代码——始终从您能想到的最简单的并发设计开始，然后进行测量，只有当测量揭示出性能问题时，您才应该优化算法。

要在实践中遵循此技巧，请从不需要复杂使用原子或大量细粒度锁的并发模式开始。 从运行顺序代码并通过通道进行通信或通过锁进行协作的多个线程开始，然后根据您关心的工作负载对结果性能进行基准测试。 与实现奇特的无锁算法或将锁分成一千块以避免错误共享相比，这种方式犯错误的可能性要小得多。 对于许多用例来说，这些设计足够快； 事实证明，为了让通道和锁表现良好，我们投入了大量的时间和精力！ 如果简单的方法对于您的用例来说足够快，为什么还要引入更复杂且容易出错的代码呢？

如果您的基准测试表明存在性能问题，请准确找出系统的哪一部分可扩展性较差。 尽可能集中精力单独解决该瓶颈，并尝试在可能的情况下进行小幅调整。 也许将锁一分为二就足够了，而不是转移到并发哈希表，或者引入另一个线程和通道而不是实现无锁工作窃取队列。 如果是这样，就这样做。

即使您确实必须直接使用原子等，也要保持简单，直到证明需要优化为止 - 首先使用 Ordering::SeqCst 和compare_exchange，然后如果您发现具体证据表明这些正在成为必须的瓶颈，则进行迭代 受到照顾。

### *编写压力测试*

作为作者，您对代码中的错误可能隐藏的位置有很多了解，但不一定知道这些错误是什么（无论如何）。 编写压力测试是消除一些隐藏错误的好方法。 压力测试不一定执行复杂的步骤序列，而是有许多线程并行执行相对简单的操作。

例如，如果您正在编写并发哈希映射，则一个压力测试可能是让 *N* 个线程插入或更新键，并且 *M* 个线程以这样的方式读取键：这些 *M*+*N* 个线程可能会 经常选择相同的键。 这样的测试不会测试特定的结果或值，而是尝试触发许多可能的操作交错，希望有问题的交错可能会暴露出来。

压力测试在很多方面类似于模糊测试； 模糊测试会为给定函数生成许多随机输入，而压力测试则会生成许多随机线程和内存访问计划。 因此，就像模糊器一样，压力测试的好坏取决于代码中的断言； 他们无法告诉您未以某种易于发现的方式（例如断言失败或其他类型的恐慌）表现出来的错误。 因此，最好在低级并发代码中添加断言，或者如果您担心特别热的循环中的运行时成本，则使用 debug_assert_* 。

### *使用并发测试工具*

编写并发代码的主要挑战是处理不同线程执行交错的所有可能方式。 正如我们在清单 10-4 中的 Ordering::SeqCst 示例中看到的，重要的不仅仅是线程调度，还包括给定线程在任何给定时间点可以观察哪些内存值。 编写执行每个可能的合法执行的测试不仅乏味而且困难——您需要对哪些线程执行、何时执行以及它们的读取返回什么值进行非常低级的控制，而操作系统可能不提供这些控制。

#### 使用 Loom 进行模型检查

幸运的是，已经有一个工具可以以 loom crate 的形式为您简化这种执行探索。 考虑到本书和 Rust 板条箱的相对发布周期，我不会在这里给出任何如何使用 Loom 的示例，因为当你阅读本书时它们可能已经过时了，但我会给出 它的作用的概述。

Loom 希望您以闭包的形式编写专用测试用例，并将其传递到 Loom 模型中。 该模型跟踪所有跨线程交互，并尝试通过多次执行测试用例闭包来智能地探索这些交互的所有可能的迭代。 为了检测和控制线程交互，Loom 为标准库中允许线程相互协调的所有类型提供了替换类型； 其中包括 std::sync 和 std::thread 下的大多数类型以及 UnsafeCell 和其他一些类型。 Loom 希望您的应用程序在您运行 Loom 测试时使用这些替换类型。 替换类型与 Loom 执行器绑定并执行双重功能：它们充当重新调度点，以便 Loom 可以选择在每个可能的线程交互点之后接下来运行哪个操作，并且它们通知 Loom 要考虑的新的可能交错。 本质上，Loom 为可能存在多个执行交错的每个点构建一棵所有未来可能执行的树，然后尝试一个接一个地执行所有这些执行。

Loom 尝试全面探索您提供的测试用例的所有可能执行情况，这意味着它可以找到仅在极其罕见的执行中出现的错误，而压力测试在一百年内都找不到这些错误。 虽然这对于较小的测试用例来说非常有用，但将这种严格的测试应用于测试更多涉及的操作序列或需要同时运行多个线程的较大测试用例通常是不可行的。 Loom 需要很长时间才能获得适当的代码覆盖。 因此，在实践中，您可能希望告诉 Loom 仅考虑可能执行的子集，Loom 的文档对此有更多详细信息。

与压力测试一样，Loom 只能捕获表现为恐慌的错误，因此这是花一些时间在并发代码中放置策略断言的另一个原因！ 在许多情况下，甚至可能值得向并发代码添加额外的状态跟踪和簿记指令，以提供更好的断言。

#### 使用 ThreadSanitizer 进行运行时检查

对于较大的测试用例，最好的选择是在 Google 出色的 ThreadSanitizer（也称为 TSan）下通过几次迭代来运行测试。 TSan 通过在每次内存访问之前放置额外的簿记指令来自动增强您的代码。 然后，当您的代码运行时，这些簿记指令会更新并检查一个特殊的状态机，该状态机会标记任何指示有问题的竞争条件的并发内存操作。 例如，如果线程 B 写入某个原子值 X，但尚未与写入 X 的先前值的线程同步（此处需要大量挥手），则表明存在写/写竞争，这几乎总是一个错误。

由于 TSan 只观察您的代码运行，并且不会像 Loom 那样一遍又一遍地执行它，因此它通常只会给程序的运行时增加一个常数因子的开销。 虽然这个因素可能很重要（在撰写本文时为 5-15 倍），但它仍然足够小，您可以在合理的时间内执行最复杂的测试用例。

在撰写本文时，要使用 TSan，您需要使用 Rust 编译器的夜间版本并传入 -Zsanitizer=thread 命令行参数（或在 RUSTFLAGS 中设置），但希望及时这将成为受支持的标准 选项。 还可以使用其他清理程序来检查越界内存访问、释放后使用、内存泄漏和读取未初始化内存等内容，您可能也希望通过这些来运行并发测试套件！

> **Heisenbug** 
> 
> Heisenbug 是一种当你尝试研究它们时似乎就会消失的 bug。 当尝试调试高度并发的代码时，这种情况经常发生； 调试问题的附加工具会改变并发事件的相对时间，并可能导致触发错误的执行交错不再发生。
>
> 消失并发错误的一个特别常见的原因是使用打印语句，这是迄今为止最常见的调试技术之一。 print 语句对并发错误产生如此大的影响有两个原因。 第一个，也许是最明显的，相对而言，将某些内容打印到用户的终端（或任何标准输出点）需要相当长的时间，特别是当您的程序产生大量输出时。 写入终端至少需要往返操作系统内核才能执行写入，但写入可能还必须等待终端本身将进程的输出读取到自己的缓冲区中。 所有这些额外的时间可能会严重延迟先前与其他线程中的操作竞争的操作，以致竞争条件消失。
>
> print 语句扰乱并发执行模式的第二个原因是写入标准输出（通常）受到锁的保护。 如果您查看标准库中的 Stdout 类型，您会发现它包含一个互斥体，用于保护对输出流的访问。 这样做是为了，如果多个线程尝试同时写入，输出不会出现太严重的乱码——如果没有锁，给定的行可能会有多个线程写入中散布的字符，但有了锁，线程将轮流写入 反而 。 不幸的是，获取输出锁是另一个线程同步点，并且每个打印线程都涉及其中。 这意味着，如果您的代码之前由于两个线程之间缺少同步而被破坏，或者只是因为两个线程之间可能存在特定的竞争，那么添加 print 语句可能会修复该错误！
>
> 一般来说，当您发现看似 Heisenbug 的内容时，请尝试寻找其他方法来缩小问题范围。 这可能涉及使用 Loom 或 TSan、使用 gdb 或 lldb，或者使用仅在最后打印的每线程内存日志。 许多日志记录框架还努力避免发出日志事件的关键路径上的同步点，因此切换到其中之一可能会让您的生活更轻松。 作为额外的好处，修复特定错误后留下的良好日志记录可能会在以后派上用场。 就我个人而言，我是追踪箱的忠实粉丝，但那里有很多不错的选择。
## 总结

在本章中，我们首先介绍了并发 Rust 中常见的正确性和性能陷阱，以及成功的并发应用程序倾向于用来解决这些问题的一些高级并发模式。 我们还探讨了异步 Rust 如何在没有并行性的情况下实现并发，以及如何在异步 Rust 代码中显式引入并行性。 然后，我们深入研究 Rust 的许多不同的低级并发原语，包括它们如何工作、它们有何不同以及它们的用途。 最后，我们探索了编写更好的并发代码的技术，并研究了 Loom 和 TSan 等可以帮助您审查代码的工具。 在下一章中，我们将通过深入研究外部函数接口来继续我们的 Rust 较低级别的旅程，这些接口允许 Rust 代码直接链接到用其他语言编写的代码。