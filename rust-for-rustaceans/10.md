**10** 

**CONCURRENCY (AND PARALLELISM)** 

With this chapter I hope to provide you with all the information and tools you’ll 

need to take effective advantage of concur- rency in your Rust programs, to implement 

support for concurrent use in your libraries, and to use Rust’s concurrency primitives correctly. I won’t directly teach you how to implement a concurrent data structure or write a high-performance concur- rent application. Instead, my goal is to give you suf- ficient understanding of the underlying mechanisms that you’re equipped to wield them yourself for what- ever you may need them for. 

Concurrency comes in three flavors: single-thread concurrency (like with async/await, as we discussed in Chapter 8), single-core multithreaded concurrency, and multicore concurrency, which yields true parallelism. 

**168** 

Chapter 10 

Each flavor allows the execution of concurrent tasks in your program to be interleaved in different ways. There are even more subflavors if you take the details of operating system scheduling and preemption into account, but we won’t get too deep into that. 

At the type level, Rust represents only one aspect of concurrency: multi- threading. Either a type is safe for use by more than one thread, or it is not. Even if your program has multiple threads (and so is concurrent) but only one core (and so is not parallel), Rust must assume that if there are multiple threads, there may be parallelism. Most of the types and techniques we’ll be talking about apply equally whether two threads actually execute in parallel or not, so to keep the language simple, I’ll be using the word *concurrency
\* in the informal sense of “things running more or less at the same time” throughout this chapter. When the distinction is important, I’ll call that out. 

What’s particularly neat about Rust’s approach to type-based safe multithreading is that it is not a feature of the compiler, but rather a library feature that developers can extend to develop sophisticated concurrency contracts. Since thread safety is expressed in the type system through Send and Sync implementations and bounds, which propagate all the way out
 to application code, the thread safety of the entire program is checked through type checking alone. 

*The Rust Programming Language* already covers most of the basics when it comes to concurrency, including the Send and Sync traits, Arc and Mutex, and channels. I therefore won’t reiterate much of that here, except where it’s worth repeating something specifically in the context of some other topic. Instead, we’ll look at what makes concurrency difficult and some common concurrency patterns intended to deal with those difficulties. We’ll also explore how concurrency and asynchrony interact (and how they don’t) before diving into how to use atomic operations to implement lower-level concurrent operations. Finally, I’ll close out the chapter with some advice for how to retain your sanity when working with concurrent code. 

**The Trouble with Concurrency** 

Before we dive into good patterns for concurrent programming and the details of Rust’s concurrency mechanisms, it’s worth taking some time to understand why concurrency is challenging in the first place. That is, why do we need special patterns and mechanisms for concurrent code? 

**Correctness** 

The primary difficulty in concurrency is coordinating access—in particular, write access—to a resource that is shared among multiple threads. If lots of threads want to share a resource solely for the purposes of reading it, then that’s usually easy: you stick it in an Arc or place it in something you can 

get a &'static to, and you’re all done. But once any thread wants to write,
 all sorts of problems arise, usually in the form of *data races*. Briefly, a data race occurs when one thread updates shared state while a second thread is also accessing that state, either to read it or to update it. Without additional 

safeguards in place, the second thread may read partially overwritten state, clobber parts of what the first thread wrote, or fail to see the first thread’s write at all! In general, all data races are considered undefined behavior. 

Data races are a part of a broader class of problems that primarily, though not exclusively, occur in a concurrent setting: *race conditions*. A race condition occurs whenever multiple outcomes are possible from a sequence of instructions, depending on the relative timing of other events in the system. These events can be threads executing a particular piece of code,
 a timer going off, a network packet coming in, or any other time-variable occurrence. Race conditions, unlike data races, are not inherently bad,
 and are not considered undefined behavior. However, they are a breed-
 ing ground for bugs when particularly peculiar races occur, as you’ll see throughout this chapter. 

**Performance** 

Often, developers introduce concurrency into their programs in the
 hope of increasing performance. Or, to be more precise, they hope that concurrency will enable them to perform more operations per second in aggregate by taking advantage of more hardware resources. This can be done on a single core by having one thread run while another is waiting, or across multiple cores by having threads do work simultaneously, one on each core, that would otherwise happen serially on one core. Most devel- opers are referring to the latter kind of performance gain when they talk about concurrency, which is often framed in terms of scalability. Scalability in this context means “the performance of this program scales with the number of cores,” implying that if you give your program more cores, its performance improves. 

While achieving such a speedup is possible, it’s harder than it seems. The ultimate goal in scalability is linear scalability, where doubling the num- ber of cores doubles the amount of work your program completes per unit of time. Linear scalability is also often called perfect scalability. However, in reality, few concurrent programs achieve such speedups. Sublinear scaling
 is more common, where the throughput increases nearly linearly as you go from one core to two, but adding more cores yields diminishing returns. Some programs even experience negative scaling, where giving the program access to more cores *reduces* throughput, usually because the many threads are all contending for some shared resource. 

It might help to think of a group of people trying to pop all the bubbles on a piece of bubble wrap—adding more people helps initially, but at some point you get diminishing returns as the crowding makes any one person’s job harder. If the humans involved are particularly ineffective, your group may end up standing around discussing who should pop next and pop no bubbles at all! This kind of interference among tasks that are supposed to execute in parallel is called *contention* and is the archnemesis of scaling well. Contention can arise in a number of ways, but the primary offenders are mutual exclusion, shared resource exhaustion, and false sharing. 

Concurrency (and Parallelism) **169** 

**Mutual Exclusion** 

When only a single concurrent task is allowed to execute a particular piece of code at any one time, we say that execution of that segment of code is mutually exclusive—if one thread executes it, no other thread can do so
 at the same time. The archetypal example of this is a mutual exclusion lock, or *mutex*, which explicitly enforces that only one thread gets to enter a particular critical section of your program code at any one time. Mutual exclusion can also happen implicitly, however. For example, if you spin up a thread to manage a shared resource and send jobs to it over an mpsc chan- nel, that thread effectively implements mutual exclusion, since only one such job gets to execute at a time. 

Mutual exclusion can also occur when invoking operating system or library calls that internally enforce single-threaded access to a critical sec- tion. For example, for many years, the standard memory allocator required mutual exclusion for some allocations, which made memory allocation an operation that incurred significant contention in otherwise highly parallel programs. Similarly, many operating system operations that may seem like they should be independent, such as creating two files with different names in the same directory, may end up having to happen sequentially inside the kernel. 

**NOTE** *Scalable concurrent allocations is the raison d’être for the* *jemalloc* *memory allocator!* 

Mutual exclusion is the most obvious barrier to parallel speedup since, by definition, it forces serial execution of some portion of your program. Even if you make the remainder of your program scale with the number of cores perfectly, the total speedup you can achieve is limited by the length of the mutually exclusive, serial section. Be mindful of your mutually exclusive sections, and seek to restrict them to only where strictly necessary. 

**NOTE** *For the theoretically minded, the limits on the achievable speedup as a result of mutu- ally exclusive sections of code can be computed using Amdahl’s law.* 

**Shared Resource Exhaustion** 

Unfortunately, even if you achieve perfect concurrency within your tasks, the environment those tasks need to interact with may itself not be perfectly scalable. The kernel can handle only so many sends on a given TCP socket per second, the memory bus can do only so many reads at once, and your GPU has a limited capacity for concurrency. There’s no cure for this. The environment is usually where perfect scalability falls apart in practice, and fixes for such cases tend to require substantial re-engineering (or even new hardware!), so we won’t talk much more about this topic in this chapter. Just remember that scalability is rarely something you can “achieve,” and more something you just strive for. 

**170** Chapter 10 

**False Sharing** 

False sharing occurs when two operations that shouldn’t contend with one another contend anyway, preventing efficient simultaneous execution. This usually happens because the two operations happen to intersect on some shared resource even though they use unrelated parts of that resource. 

The simplest example of this is lock oversharing, where a lock guards some composite state, and two operations that are otherwise independent both need to take the lock to update their particular parts of the state.
 This in turn means the operations must execute serially instead of in paral- lel. In some cases it’s possible to split the single lock into two, one for each of the disjoint parts, which enables the operations to proceed in parallel. However, it’s not always straightforward to split a lock like this—the state may share a single lock because some third operation needs to lock over
 all the parts of the state. Usually you can still split the lock, but you have to be careful about the order in which different threads take the split locks to avoid deadlocks that can occur when two operations attempt to take them in different orders (look up the “dining philosophers problem,” if you’re curious). Alternatively, for some problems, you may be able to avoid the critical section entirely by using a lock-free version of the underlying algo- rithm, though those are also tricky to get right. Ultimately, false sharing is a hard problem to solve, and there isn’t a single catchall solution—but identi- fying the problem is a good start. 

A more subtle example of false sharing occurs on the CPU level, as we discussed briefly in Chapter 2. The CPU internally operates on memory in terms of cache lines—longer sequences of consecutive bytes in memory— rather than individual bytes, to amortize the cost of memory accesses. For example, on most Intel processors, the cache line size is 64 bytes. This means that every memory operation really ends up reading or writing some multiple of 64 bytes. The false sharing comes into play when two cores 

want to update the value of two different bytes that happen to fall on the same cache line; those updates must execute sequentially even though the updates are logically disjoint. 

This might seem too low-level to matter, but in practice this kind of false sharing can decimate the parallel speedup of an application. Imagine that you allocate an array of integer values to indicate how many operations each thread has completed, but the integers all fall within the same cache line—now, all your otherwise parallel threads will contend on that one cache line for every operation they perform. If the operations are relatively quick, *most* of your execution time may end up being spent contending on those counters! 

The trick to avoiding false cache line sharing is to pad your values so that they are the size of a cache line. That way, two adjacent values always fall on different cache lines. But of course, this also inflates the size of your data structures, so use this approach only when benchmarks indicate a problem. 

Concurrency (and Parallelism) **171** 

**172** 

Chapter 10 

**THE COST OF SCALABILITY** 

A somewhat orthogonal aspect of concurrency that you should be mindful of is the cost of introducing concurrency in the first place . Compilers are really good at optimizing single-threaded code—they’ve been doing it for a long time, after all—and single-threaded code tends to get away with fewer expensive safe- guards (like locks, channels, or atomic instructions) than concurrent code can . In aggregate, the various costs of concurrency can make a parallel program slower than its single-threaded counterpart, given any number of cores! This is why it’s important to measure both before and after you optimize and parallel- ize: the results may surprise you . 

If you’re curious about this topic, I highly recommend you read Frank McSherry’s 2015 paper “Scalability! But at what COST?” (https://www .frankmcsherry.org/assets/COST.pdf), which uncovers some particularly egregious examples of “costly scaling .” 

**Concurrency Models** 

Rust has three patterns for adding concurrency to your programs that you’ll come across fairly often: shared memory concurrency, worker pools, and actors. Going through every way you could add concurrency in detail would take a book of its own, so here I’ll focus on just these three patterns. 

**Shared Memory** 

Shared memory concurrency is, conceptually, very straightforward: the threads cooperate by operating on regions of memory shared between them. This might take the form of state guarded by a mutex or stored in
 a hash map with support for concurrent access from many threads. The many threads may be doing the same task on disjoint pieces of data, such as if many threads perform some function over disjoint subranges of a Vec, or they may be performing different tasks that require some shared state, such as in a database where one thread handles user queries to a table while another optimizes the data structures used to store that table in the background. 

When you use shared memory concurrency, your choice of data struc- tures is significant, especially if the threads involved need to cooperate very closely. A regular mutex might prevent scaling beyond a very small number of cores, a reader/writer lock might allow many more concurrent reads at the cost of slower writes, and a sharded reader/writer lock might allow perfectly scalable reads at the cost of making writes highly disruptive. Similarly, some concurrent hash maps aim for good all-round performance while others specifically target, say, concurrent reads where writes are rare. In general, in shared memory concurrency, you want to use data structures 

that are specifically designed for something as close to your target use case as possible, so that you can take advantage of optimizations that trade off performance aspects your application does not care about for those it does. 

Shared memory concurrency is a good fit for use cases where threads need to jointly update some shared state in a way that does not commute. That is, if one thread has to update the state s with some function f, and another has to update the state with some function g, and f(g(s)) != g(f(s)), then shared memory concurrency is likely necessary. If that is not the case, the other two patterns are likely better fits, as they tend to lead to simpler and more performant designs. 

**NOTE** *Some problems have known algorithms that can provide concurrent shared memory operations without the use of locks. As the number of cores grows, these* lock-free *algorithms may scale better than lock-based algorithms, though they also often have slower per-core performance due to their complexity. As always with performance mat- ters, benchmark first, then look for alternative solutions.* 

**Worker Pools** 

In the worker pool model, many identical threads receive jobs from a shared job queue, which they then execute entirely independently. Web servers, for example, often have a worker pool handling incoming connec- tions, and multithreaded runtimes for asynchronous code tend to use a worker pool to collectively execute all of an application’s futures (or, more accurately, its top-level tasks). 

The lines between shared memory concurrency and worker pools are often blurry, as worker pools tend to use shared memory concurrency to coordinate how they take jobs from the queue and how they return incom- plete jobs back to the queue. For example, say you’re using the data paral- lelism library rayon to perform some function over every element of a vector in parallel. Behind the scenes rayon spins up a worker pool, splits the vector into subranges, and then hands out subranges to the threads in the pool. When a thread in the pool finishes a range, rayon arranges for it to start working on the next unprocessed subrange. The vector is shared among all the worker threads, and the threads coordinate through a shared memory queue–like data structure that supports work stealing. 

Work stealing is a key feature of most worker pools. The basic premise is that if one thread finishes its work early, and there’s no more unassigned work available, that thread can steal jobs that have already been assigned to a different worker thread but haven’t been started yet. Not all jobs take the same amount of time to complete, so even if every worker is given the same *number* of jobs, some workers may end up finishing their jobs more quickly than others. Rather than sit around and wait for the threads that drew longer-running jobs to complete, those threads that finish early should help the stragglers so the overall operation is completed sooner. 

It’s quite a task to implement a data structure that supports this kind of work stealing without incurring significant overhead from threads con- stantly trying to steal work from one another, but this feature is vital to a 

Concurrency (and Parallelism) **173** 

high-performance worker pool. If you find yourself in need of a worker pool, your best bet is usually to use one that has already seen a lot of work go into it, or at least reuse data structures from an existing one, rather than to write one yourself from scratch. 

Worker pools are a good fit when the work that each thread performs is the same, but the data it performs it *on* varies. In a rayon parallel map opera- tion, every thread performs the same map computation; they just perform
 it on different subsets of the underlying data. In a multithreaded asynchro- nous runtime, each thread simply calls Future::poll; they just call it on dif- ferent futures. If you start having to distinguish between the threads in your thread pool, a different design is probably more appropriate. 

**CONNECTION POOLS** 

A connection pool is a shared memory construct that keeps a set of established connections and hands them out to threads that need a connection . It’s a com- mon design pattern in libraries that manage connections to external services .
 If a thread needs a connection but one isn’t available, either a new connection is established or the thread is forced to block . When a thread is done with a connection, it returns that connection to the pool, and thus makes it available to other threads that may be waiting . 

Usually, the hardest task for a connection pool is managing connection life cycles . A connection can be returned to the pool in whatever state it was put in by the last thread that used it . The connection pool therefore has to make sure any state associated with the connection, whether on the client or on the server, has been reset so that when the connection is subsequently used by another thread, that thread can act as though it was given a fresh, dedicated connection . 

**Actors** 

The actor concurrency model is, in many ways, the opposite of the worker pool model. Whereas the worker pool has many identical threads that share a job queue, the actor model has many separate job queues, one
 for each job “topic.” Each job queue feeds into a particular actor, which handles all jobs that pertain to a subset of the application’s state. That state might be a database connection, a file, a metrics collection data structure, or any other structure that you can imagine many threads may need to be able to access. Whatever it is, a single actor owns that state, and if some task wants to interact with that state, it needs to send a message to the owning actor summarizing the operation it wishes to perform. When the owning actor receives that message, it performs the indicated action and responds to the inquiring task with the result of the operation, if relevant. 

**174** Chapter 10 

Since the actor has exclusive access to its inner resource, no locks or other synchronization mechanisms are required beyond what’s needed for the messaging. 

A key point in the actor pattern is that actors all talk to one another.
 If, say, an actor that is responsible for logging needs to write to a file and a database table, it might send off messages to the actors responsible for each of those, asking them to perform the respective actions, and then proceed to the next log event. In this way, the actor model more closely resembles a web than spokes on a wheel—a user request to a web server might start as a single request to the actor responsible for that connection but might transi- tively spawn tens, hundreds, or even thousands of messages to actors deeper in the system before the user’s request is satisfied. 

Nothing in the actor model requires that each actor is its own thread. To the contrary, most actor systems suggest that there should be a large number of actors, and so each actor should map to a task rather than a thread. After all, actors require exclusive access to their wrapped resources only when they execute, and do not care whether they are on a thread of their own
 or not. In fact, very frequently, the actor model is used in conjunction with the worker pool model—for example, an application that uses the multi- threaded asynchronous runtime Tokio can spawn an asynchronous task for each actor, and Tokio will then make the execution of each actor a job in its worker pool. Thus, the execution of a given actor may move from thread to thread in the worker pool as the actor yields and resumes, but every time the actor executes it maintains exclusive access to its wrapped resource. 

The actor concurrency model is well suited for when you have many resources that can operate relatively independently, and where there is little or no opportunity for concurrency within each resource. For example, an operating system might have an actor responsible for each hardware device, and a web server might have an actor for each backend database connection. The actor model does not work so well if you need only a few actors, if work is skewed significantly among the actors, or if some actors grow large—in all of those cases, your application may end up being bottle- necked on the execution speed of a single actor in the system. And since actors each expect to have exclusive access to their little slice of the world, you can’t easily parallelize the execution of that one bottleneck actor. 

**Asynchrony and Parallelism** 

As we discussed in Chapter 8, asynchrony in Rust enables concurrency without parallelism—we can use constructs like selects and joins to have
 a single thread poll multiple futures and continue when one, some, or all
 of them complete. Because there is no parallelism involved, concurrency with futures does not fundamentally require those futures to be Send. Even spawning a future to run as an additional top-level task does not fundamen- tally require Send, since a single executor thread can manage the polling of many futures at once. 

Concurrency (and Parallelism) **175** 

However, in *most* cases, applications want both concurrency and parallel- ism. For example, if a web application constructs a future for each incoming connection and so has many active connections at once, it probably wants the asynchronous executor to be able to take advantage of more than one core on the host computer. That won’t happen naturally: your code has to explic- itly tell the executor which futures can run in parallel and which cannot. 

In particular, two pieces of information must be given to the executor to let it know that it can spread the work in the futures across a worker pool of threads. The first is that the futures in question are Send—if they aren’t, the executor is not allowed to send the futures to other threads for process- ing, and no parallelism is possible; only the thread that constructed such futures can poll them. 

The second piece of information is how to split the futures into tasks that can operate independently. This ties back to the discussion of tasks ver- sus futures from Chapter 8: if one giant Future contains a number of Future instances that themselves correspond to tasks that can run in parallel, the executor must still call poll on the top-level Future, and it must do so from
 a single thread, since poll requires &mut self. Thus, to achieve parallelism with futures, you have to explicitly spawn the futures you want to be able to run in parallel. Also, because of the first requirement, the executor func- tion you use to do so will require that the passed-in Future is Send. 

**ASYNCHRONOUS SYNCHRONIZATION PRIMITIVES** 

Most of the synchronization primitives that exist for blocking code (think std::sync) also have asynchronous counterparts . There are asynchronous vari- ants of channels, mutexes, reader/writer locks, barriers, and all sorts of other similar constructs . We need these because, as discussed in Chapter 8, blocking inside a future will hold up other work the executor may need to do, and so is inadvisable . 

However, the asynchronous versions of these primitives are often slower than their synchronous counterparts because of the additional machinery needed to perform the necessary wake-ups . For that reason, you may want to use synchronous synchronization primitives even in asynchronous contexts when- ever the use does not risk blocking the executor . For example, while it’s generally true that acquiring a Mutex might block for a long time, that might not be true for a particular Mutex that, perhaps, is acquired only rarely, and only ever for short periods of time . In that case, blocking for the short time until the Mutex becomes available again might not actually cause any problems . You will want to make sure that you never yield or perform other long-running operations while holding the MutexGuard, but barring that you shouldn’t run into problems . 

As always with such optimizations, though, make sure you measure first, and choose only the synchronous primitive if it nets you significant performance improvements . If it does not, the additional footguns introduced by using a syn- chronous primitive in an asynchronous context are probably not worth it . 

**176** Chapter 10 

**Lower-Level Concurrency** 

The standard library provides the std::sync::atomic module, which pro- vides access to the underlying CPU primitives, higher-level constructs like channels and mutexes are built with. These primitives come in the form
 of atomic types with names starting with Atomic—AtomicUsize, AtomicI32, AtomicBool, AtomicPtr, and so on—the Ordering type, and two functions called fence and compiler_fence. We’ll look at each of these over the next few sections. 

These types are the blocks used to build any code that has to communi- cate between threads. Mutexes, channels, barriers, concurrent hash tables, lock-free stacks, and all other synchronization constructs ultimately rely on these few primitives to do their jobs. They also come in handy on their own for lightweight cooperation between threads where heavyweight synchroni- zation like a mutex is excessive—for example, to increment a shared coun- ter or set a shared Boolean to true. 

The atomic types are special in that they have defined semantics for what happens when multiple threads try to access them concurrently. These types all support (mostly) the same API: load, store, fetch_*, and compare_ exchange. In the rest of this section, we’ll look at what those do, how to use them correctly, and what they’re useful for. But first, we have to talk about low-level memory operations and memory ordering. 

**Memory Operations** 

Informally, we often refer to accessing variables as “reading from” or “writ- ing to” memory. In reality, a lot of machinery between code uses a variable and the actual CPU instructions that access your memory hardware. It’s important to understand that machinery, at least at a high level, in order to understand how concurrent memory accesses behave. 

The compiler decides what instructions to emit when your program reads the value of a variable or assigns a new value to it. It is permitted to perform all sorts of transformations and optimizations on your code and may end up reordering your program statements, eliminating operations it deems redundant, or using CPU registers rather than actual memory to store intermediate computations. The compiler is subject to a number of restrictions on these transformations, but ultimately only a subset of your variable accesses actually end up as memory access instructions. 

At the CPU level, memory instructions come in two main shapes: loads and stores. A load pulls bytes from a location in memory into a CPU regis- ter, and a store stores bytes from a CPU register into a location in memory. Loads and stores operate on small chunks of memory at a time: usually
 8 bytes or less on modern CPUs. If a variable access spans more bytes than can be accessed with a single load or store, the compiler automatically turns it into multiple load or store instructions, as appropriate. The CPU also has some leeway in how it executes a program’s instructions to make better use of the hardware and improve program performance. For example, modern CPUs often execute instructions in parallel, or even out of order, when they don’t have dependencies on each other. There are also several layers of caches 

Concurrency (and Parallelism) **177** 

**178** Chapter 10 

between each CPU and your computer’s DRAM, which means that a load of a given memory location may not necessarily see the latest store to that memory location, going by wall-clock time. 

In most code, the compiler and CPU are permitted to transform the code only in ways that don’t affect the semantics of the resulting program, so these transformations are invisible to the programmer. However, in the context of parallel execution, these transformations can have a significant impact on application behavior. Therefore, CPUs typically provide multiple different variations of the load and store instructions, each with different guarantees about how the CPU may reorder them and how they may be interleaved with parallel operations on other CPUs. Similarly, compilers (or rather, the language the compiler compiles) provide different annotations you can use to force particular execution constraints for some subset of their memory accesses. In Rust, those annotations come in the form of the atomic types and their methods, which we’ll spend the rest of this section picking apart. 

**Atomic Types** 

Rust’s atomic types are so called because they can be accessed atomically— that is, the value of an atomic-type variable is written all at once and will never be written using multiple stores, guaranteeing that a load of that vari- able cannot observe that only some of the bytes composing the value have changed while others have not (yet). This is easiest understood by way of contrast with non-atomic types. For example, reassigning a new value to a tuple of type (i64, i64) typically requires two CPU store instructions, one for each 8-byte value. If one thread were to perform both of those stores, another thread could (if we ignore the borrow checker for a second) read the tuple’s value after the first store but before the second, and thus end up with an inconsistent view of the tuple’s value. It would end up reading the new value for the first element and the old value for the second element, a value that was never actually stored by any thread. 

The CPU can atomically access values only of certain sizes, so there are only a few atomic types, all of which live in the atomic module. Each atomic type is of one of the sizes the CPU supports atomic access to, with multiple variations for things like whether the value is signed and to differentiate between an atomic usize and a pointer (which is of the same size as usize). Furthermore, the atomic types have explicit methods for loading and stor- ing the values they hold, and a handful of more complex methods we’ll get back to later, so that the mapping between the code the programmer writes and the resulting CPU instructions is clearer. For example, AtomicI32::load performs a single load of a signed 32-bit value, and AtomicPtr::store per- forms a single store of a pointer-sized (64 bits on a 64-bit platform) value. 

**Memory Ordering** 

Most of the methods on the atomic types take an argument of type Ordering, which dictates the memory ordering restrictions the atomic operation is subject to. Across different threads, loads and stores of an atomic value 

may be sequenced by the compiler and CPU only in interleavings that are compatible with the requested memory ordering of each of the atomic operations on that atomic value. Over the next few sections, we’ll see some examples of why control over the ordering is important and necessary to get the expected semantics out of the compiler and CPU. 

Memory ordering often comes across as counterintuitive, because we humans like to read programs from top to bottom and imagine that they execute line by line—but that’s not how the code actually executes when it hits the hardware. Memory accesses can be reordered, or even entirely elided, and writes on one thread may not immediately be visible to other threads, even if later writes in program order have already been observed. 

Think of it like this: each memory location sees a sequence of modifica- tions coming from different threads, and the sequences of modifications for different memory locations are independent. If two threads T1 and T2 both write to memory location M, then even if T1 executed first as measured by
 a user with a stopwatch, T2’s write to M may still appear to have happened first for M absent any other constraints between the two threads’ execution. Essentially, *the computer does not take wall-clock time into account* when it deter- mines the value of a given memory location—all that matter are the execu- tion constraints the programmer puts on what constitutes a valid execution. For example, if T1 writes to M and then spawns thread T2, which then writes to M, the computer must recognize T1’s write as having happened first because T2’s existence depends on T1. 

If that’s hard to follow, don’t fret—memory ordering can be mind- bending, and language specifications tend to use very precise but not very intuitive wording to describe it. We can construct a mental model that’s easier to grasp, if a little simplified, by instead focusing on the underlying hardware architecture. Very basically, your computer memory is structured as a treelike hierarchy of storage where the leaves are CPU registers and the roots are the storage on your physical memory chips, often called main memory. Between the two are several layers of caches, and different lay-
 ers of the hierarchy can reside on different pieces of hardware. When a thread performs a store to a memory location, what really happens is that the CPU starts a write request for the value in a given CPU register that then has to make its way up the memory hierarchy toward main memory. When a thread performs a load, the request flows up the hierarchy until it hits a layer that has the value available, and returns from there. Herein lies the problem: writes aren’t visible everywhere until all caches of the written memory location have been updated, but other CPUs can execute instruc- tions against the same memory location at the same time, and weirdness ensues. Memory ordering, then, is a way to request precise semantics for what happens when multiple CPUs access a particular memory location for a particular operation. 

With this in mind, let’s take a look at the Ordering type, which is the primary mechanism by which we, as programmers, can dictate additional constraints on what concurrent executions are valid. 

Ordering is defined as an enum with the variants shown in Listing 10-1. Concurrency (and Parallelism) **179** 

```
enum Ordering {
    Relaxed,
    Release,
    Acquire,
    AcqRel,
    SeqCst
```

} 

Listing 10-1: The definition of *Ordering* 

Each of these places different restrictions on the mapping from source code to execution semantics, and we’ll explore each one in turn in the remainder of this section. 

**Relaxed Ordering** 

Relaxed ordering essentially guarantees nothing about concurrent access to the value beyond the fact that the access is atomic. In particular, relaxed ordering gives no guarantees about the relative ordering of mem- ory accesses across different threads. This is the weakest form of memory ordering. Listing 10-2 shows a simple program in which two threads access two atomic variables using Ordering::Relaxed. 

```
static X: AtomicBool = AtomicBool::new(false);
static Y: AtomicBool = AtomicBool::new(false);
```

let t1 = spawn(|| {
 1 let r1 = Y.load(Ordering::Relaxed); 2 X.store(r1, Ordering::Relaxed); 

```
});
let t2 = spawn(|| {
```

3 let r2 = X.load(Ordering::Relaxed); 

4 Y.store(true, Ordering::Relaxed) }); 

Listing 10-2: Two racing threads with *Ordering::Relaxed* 

Looking at the thread spawned as t2, you might expect that r2 can never be true, since all values are false until the same thread assigns true to Y on the line *after* reading X. However, with a relaxed memory ordering, that outcome is completely possible. The reason is that the CPU is allowed to reorder the loads and stores involved. Let’s walk through exactly what hap- pens here to make r2 = true possible.
 First, the CPU notices that 4 doesn’t have to happen after 3, since 4 doesn’t use any output or side effect of 3. That is, 4 has no execution depen- dency on 3. So, the CPU decides to reorder them for *waves hands* reasons that’ll make your program go faster. The CPU thus goes ahead and executes 4 first, setting Y = true, even though 3 hasn’t run yet. Then, t2 is put to sleep by the operating system and thread t1 executes a few instructions, or t1 sim- ply executes on another core. In t1, the compiler must indeed run 1 first and then 2, since 2 depends on the value read in 1. Therefore, t1 reads true from 

**180** Chapter 10 

**N O T E** 

Y (written by 4) into r1 and then writes that back to X. Finally, t2 executes 3, which reads X and gets true, as was written by 2. 

The relaxed memory ordering allows this execution because it imposes no additional constraints on concurrent execution. That is, under relaxed memory ordering, the compiler must ensure only that execution dependen- cies on any given thread are respected (just as if atomics weren’t involved); it need not make any promises about the interleaving of concurrent opera- tions. Reordering 3 and 4 is permitted for a single-threaded execution, so it is permitted under relaxed ordering as well. 

In some cases, this kind of reordering is fine. For example, if you have a counter that just keeps track of metrics, it doesn’t really matter when exactly it executes relative to other instructions, and Ordering::Relaxed is fine. In other cases, this could be disastrous: say, if your program uses r2 to figure out if security protections have already been set up, and thus ends up erro- neously believing that they already have been. 

You don’t generally notice this reordering when writing code that doesn’t make fancy use of atomics—the CPU has to promise that there is no observable difference between the code as written and what each thread actually executes, so everything seems like it runs in order just as you wrote it. This is referred to as respecting program order or evaluation order; the terms are synonyms. 

**Acquire/Release Ordering** 

At the next step up in the memory ordering hierarchy, we have Ordering::Acquire, Ordering::Release, and Ordering::AcqRel (acquire plus release). At a high level, these establish an execution dependency between
 a store in one thread and a load in another and then restrict how opera- tions can be reordered with respect to that load and store. Crucially, these dependencies not only establish a relationship between a store and a load of a single value, but also put ordering constraints on *other* loads and stores in the threads involved. This is because every execution must respect the pro- gram order; if a load in thread B has a dependency on some store in thread A (the store in A must execute before the load in B), then any read or write in B after that load must also happen after that store in A. 

*The* *Acquire* *memory ordering can be applied only to loads,* *Release* *only to stores, and* *AcqRel* *only to operations that both load* and *store (like* *fetch_add**).* 

Concretely, these memory orderings place the following restrictions on execution: 

1. Loads and stores cannot be moved forward past a store with Ordering::Release. 
2. Loads and stores cannot be moved back before a load with Ordering::Acquire. 
3. An Ordering::Acquire load of a variable must see all stores that happened before an Ordering::Release store that stored what the load loaded. 

Concurrency (and Parallelism) **181** 

**182** Chapter 10 

**N O T E** 

To see how these memory orderings change things, Listing 10-3 shows Listing 10-2 again but with the memory ordering swapped out for Acquire and Release. 

```
static X: AtomicBool = AtomicBool::new(false);
static Y: AtomicBool = AtomicBool::new(false);
let t1 = spawn(|| {
    let r1 = Y.load(Ordering::Acquire);
    X.store(r1, Ordering::Release);
});
let t2 = spawn(|| {
```

1 let r2 = X.load(Ordering::Acquire); 

2 Y.store(true, Ordering::Release) }); 

Listing 10-3: Listing 10-2 with *Acquire/Release* memory ordering 

These additional restrictions mean that it is no longer possible for t2 to see r2 = true. To see why, consider the primary cause of the weird outcome in Listing 10-2: the reordering of 1 and 2. The very first restriction, on stores with Ordering::Release, dictates that we cannot move 1 below 2, so we’re all good! 

But these rules are useful beyond this simple example. For example, imagine that you implement a mutual exclusion lock. You want to make sure that any loads and stores a thread runs while it holds the lock are exe- cuted only while it’s actually holding the lock, and visible to any thread that takes the lock later. This is exactly what Release and Acquire enable you to do. By performing a Release store to release the lock and an Acquire load to acquire the lock, you can guarantee that the loads and stores in the critical section are never moved to before the lock was actually acquired or to after the lock was released! 

*On some CPU architectures, like x86,* *Acquire/Release* *ordering is guaranteed
 by the hardware, and there is no additional cost to using* *Ordering::Release* *and* *Ordering::Acquire* *over* *Ordering::Relaxed**. On other architectures that is not the case, and your program may see speedups if you switch to* *Relaxed* *for atomic opera- tions that can tolerate the weaker memory ordering guarantees.* 

**Sequentially Consistent Ordering** 

Sequentially consistent ordering (Ordering::SeqCst) is the strongest memory ordering we have access to. Its exact guarantees are somewhat hard to nail down, but very broadly, it requires not only that each thread sees results consistent with Acquire/Release, but also that all threads see the *same* order- ing as one another. This is best seen by way of contrast with the behavior of Acquire and Release. Specifically, Acquire/Release ordering does *not* guaran- tee that if two threads A and B atomically load values written by two other threads X and Y, A and B will see a consistent pattern of when X wrote relative to Y. That’s fairly abstract, so consider the example in Listing 10-4, 

which shows a case where Acquire/Release ordering can produce unexpected results. Afterwards, we’ll see how sequentially consistent ordering avoids that particular unexpected outcome. 

```
static X: AtomicBool = AtomicBool::new(false);
static Y: AtomicBool = AtomicBool::new(false);
static Z: AtomicI32 = AtomicI32::new(0);
let t1 = spawn(|| {
    X.store(true, Ordering::Release);
});
let t2 = spawn(|| {
    Y.store(true, Ordering::Release);
});
let t3 = spawn(|| {
    while (!X.load(Ordering::Acquire)) {}
```

1 if (Y.load(Ordering::Acquire)) { Z.fetch_add(1, Ordering::Relaxed); } 

```
});
let t4 = spawn(|| {
```

while (!Y.load(Ordering::Acquire)) {} 2 if (X.load(Ordering::Acquire)) { 

```
        Z.fetch_add(1, Ordering::Relaxed); }
});
```

Listing 10-4: Weird results with *Acquire/Release* ordering 

The two threads t1 and t2 set X and Y to true, respectively. Thread t3 waits for X to be true; once X is true, it checks if Y is true and, if so, adds 1 to Z. Thread t4 instead waits for Y to become true, and then checks if X is true and, if so, adds 1 to Z. At this point the question is: what are the possible values for Z after all the threads terminate? Before I show you the answer, try to work your way through it given the definitions of Release and Acquire ordering in the previous section. 

First, let’s recap the conditions under which Z is incremented. Thread t3 increments Z if it sees that Y is true after it observes that X is true, which can happen only if t2 runs before t3 evaluates the load at 1. Conversely, thread t4 increments Z if it sees that X is true after it observes that Y is true, so only if t1 runs before t4 evaluates the load at 2. To simplify the explanation, let’s assume for now that each thread runs to completion once it runs. 

Logically, then, Z can be incremented twice if the threads run in the order 1, 2, 3, 4—both X and Y are set to true, and then t3 and t4 run to find that their conditions for incrementing Z are met. Similarly, Z can trivially be incremented just once if the threads run in the order 1, 3, 2, 4. This sat- isfies t4’s condition for incrementing Z, but not t3’s. Getting Z to be 0, how- ever, *seems* impossible: if we want to prevent t3 from incrementing Z, t2 has to run after t3. Since t3 runs only after t1, that implies that t2 runs after t1. However, t4 won’t run until after t2 has run, so t1 must have run and set X to true by the time t4 runs, and so t4 will increment Z. 

Our inability to get Z to be 0 stems mostly from our human inclina- tion for linear explanations; this happened, then this happened, then this 

Concurrency (and Parallelism) **183** 

**184** Chapter 10 

happened. Computers aren’t limited in the same way and have no need to box all events into a single global order. There’s nothing in the rules for Release and Acquire that says that t3 must observe the same execution order for t1 and t2 as t4 observes. As far as the computer is concerned, it’s fine
 to let t3 observe t1 as having executed first, while having t4 observe t2 as having executed first. With that in mind, an execution in which t3 observes that Y is false after it observes that X is true (implying that t2 runs after t1), while in the same execution t4 observes that X is false after it observes that Y is true (implying that t2 runs before t1), is completely reasonable, even if that seems outrageous to us mere humans. 

As we discussed earlier, Acquire/Release requires only that an Ordering::Acquire load of a variable must see all stores that happened before an Ordering::Release store that stored what the load loaded. In the order- ing just discussed, the computer *did* uphold that property: t3 sees X == true, and indeed sees all stores by t1 prior to it setting X = true—there are none. It also sees Y == false, which was stored by the main thread at program startup, so there aren’t any relevant stores to be concerned with. Similarly, t4 sees Y = true and also sees all stores by t2 prior to setting Y = true—again, there are none. It also sees X == false, which was stored by the main thread and has no preceding store. No rules are broken, yet it just seems wrong somehow. 

Our intuitive expectation was that we could put the threads in some global order to make sense of what every thread saw and did, but that was not the case for Acquire/Release ordering in this example. To achieve some- thing closer to that intuitive expectation, we need sequential consistency. Sequential consistency requires all the threads taking part in an atomic operation to coordinate to ensure that what each thread observes corre- sponds to (or at least appears to correspond to) *some* single, common execu- tion order. This makes it easier to reason about but also makes it costly. 

Atomic loads and stores marked with Ordering::SeqCst instruct the com- piler to take any extra precautions (such as using special CPU instructions) needed to guarantee sequential consistency for those loads and stores. The exact formalism around this is fairly convoluted, but sequential consistency essentially ensures that if you looked at all the related SeqCst operations from across all your threads, you could put the thread executions in *some* order so that the values that were loaded and stored would all match up. 

If we replaced all the memory ordering arguments in Listing 10-4 with SeqCst, Z could not possibly be 0 after all the threads have exited, just as we originally expected. Under sequential consistency, it must be possible to say either that t1 definitely ran before t2 or that t2 definitely ran before t1, so the execution where t3 and t4 see different orders is not allowed, and thus Z cannot be 0. 

**Compare and Exchange** 

In addition to load and store, all of Rust’s atomic types provide a method called compare_exchange. This method is used to atomically *and condition- ally* replace a value. You provide compare_exchange with the last value you 

observed for an atomic variable and the new value you want to replace the original value with, and it will replace the value only if it is still the same as it was when you last observed it. To see why this is important, take a look
 at the (broken) implementation of a mutual exclusion lock in Listing 10-5. This implementation keeps track of whether the lock is held in the static atomic variable LOCK. We use the Boolean value true to represent that the lock is held. To acquire the lock, a thread waits for LOCK to be false, then sets it to true again; it then enters its critical section and sets LOCK to false to release the lock when its work (f) is done. 

```
static LOCK: AtomicBool = AtomicBool::new(false);
fn mutex(f: impl FnOnce()) {
    // Wait for the lock to become free (false).
    while LOCK.load(Ordering::Acquire)
      { /* .. TODO: avoid spinning .. */ }
    // Store the fact that we hold the lock.
    LOCK.store(true, Ordering::Release);
    // Call f while holding the lock.
```

f(); 

```
    // Release the lock.
    LOCK.store(false, Ordering::Release);
}
```

Listing 10-5: An incorrect implementation of a mutual exclusion lock 

This mostly works, but it has a terrible flaw—two threads might both see LOCK == false at the same time and both leave the while loop. Then they both set LOCK to true and both enter the critical section, which is exactly what the mutex function was supposed to prevent! 

The issue in Listing 10-5 is that there is a gap between when we load the current value of the atomic variable and when we subsequently update it, during which another thread might get to run and read or touch its value. It is exactly this problem that compare_exchange solves—it swaps out the value behind the atomic variable *only* if its value still matches the previous read, and otherwise notifies you that the value has changed. Listing 10-6 shows the corrected implementation using compare_exchange. 

```
static LOCK: AtomicBool = AtomicBool::new(false);
fn mutex(f: impl FnOnce()) {
    // Wait for the lock to become free (false).
    loop {
      let take = LOCK.compare_exchange(
          false,
          true,
          Ordering::AcqRel,
          Ordering::Relaxed
      );
      match take {
        Ok(false) => break,
        Ok(true) | Err(false) => unreachable!(),
```

Concurrency (and Parallelism) **185** 

**186** Chapter 10 

**N O T E** 

```
        Err(true) => { /* .. TODO: avoid spinning .. */ }
      }
```

} 

```
    // Call f while holding the lock.
```

f(); 

```
    // Release the lock.
    LOCK.store(false, Ordering::Release);
}
```

Listing 10-6: A corrected implementation of a mutual exclusion lock 

This time around, we use compare_exchange in the loop, and it takes care of both checking that the lock is currently not held and storing true to take the lock as appropriate. This happens through the first and second argu- ments to compare_exchange, respectively: in this case, false and then true. You can read the invocation as “Store true only if the current value is false.” The compare_exchange method returns a Result that indicates either that the value was successfully updated (Ok) or that it could not be updated (Err).
 In either case, it also returns the current value. This isn’t too useful with an AtomicBool since we know what the value must be if the operation failed, but for something like an AtomicI32, the updated current value will let you quickly recompute what to store and then try again without having to do another load. 

*Note that* *compare_exchange* *checks only whether the value is the same as the one that was passed in as the current value. If some other thread modifies the atomic variable’s value and then resets it to the original value again, a* *compare_exchange* *on that vari- able will still succeed. This is often referred to as the A-B-A problem.* 

Unlike simple loads and stores, compare_exchange takes *two* Ordering argu- ments. The first is the “success ordering,” and it dictates what memory ordering should be used for the load and store that the compare_exchange represents in the case that the value was successfully updated. The second is the “failure ordering,” and it dictates the memory ordering for the load if the loaded value does not match the expected current value. These two orderings are kept separate so that the developer can give the CPU leeway to improve execution performance by reordering loads and stores on fail- ure when appropriate, but still get the correct ordering on success. In this case, it’s okay to reorder loads and stores across failed iterations of the lock acquisition loop, but it’s *not* okay to reorder loads and stores inside the criti- cal section in such a way that they end up outside of it. 

Even though its interface is simple, compare_exchange is a very powerful synchronization primitive—so much so that it’s been theoretically proven that you can build all other distributed consensus primitives using only compare_exchange! For that reason, it is the workhorse of many, if not most, synchronization constructs when you really dig into the implementation details. 

Be aware, though, that a compare_exchange requires that a single CPU has exclusive access to the underlying value, and it is therefore a form of mutual exclusion at the hardware level. This in turn means that compare_exchange 

can quickly become a scalability bottleneck: only one CPU can make prog- ress at a time, so there’s a portion of your code that will not scale with the number of cores. In fact, it’s probably worse than that—the CPUs have to coordinate to ensure that only one CPU succeeds at a compare_exchange for a variable at a time (take a look at the MESI protocol if you’re curious about how that works), and that coordination grows quadratically more costly the more CPUs are involved! 

**COMPARE_EXCHANGE_WEAK** 

The careful documentation reader will notice that compare_exchange has a suspi- ciously named cousin, compare_exchange_weak, and wonder what the difference is . The weak variant of compare_exchange is allowed to fail even if the atomic variable’s value does still match the expected value that the user passed in, whereas the strong variant must succeed in this case . 

This might seem odd—how could an atomic value swap fail except if the value has changed? The answer lies in system architectures that do not have a native compare_exchange operation . For example, ARM processors instead have locked load and conditional store operations, where a conditional store will fail if the value read by an associated locked load has not been written to since the load . The Rust standard library implements compare_exchange on ARM by calling this pair of instructions in a loop and returning only once the conditional store succeeds . This makes the code in Listing 10-6 needlessly inefficient—we end up with a nested loop, which requires more instructions and is harder to optimize . Since we already have a loop in this case, we could instead use com- pare_exchange_weak, remove the unreachable!() on Err(false), and get better machine code on ARM and the same compiled code on x86! 

**The Fetch Methods** 

Fetch methods (fetch_add, fetch_sub, fetch_and, and the like) are designed to allow more efficient execution of atomic operations that commute—that is, operations that have meaningful semantics regardless of the order they execute in. The motivation for this is that the compare_exchange method 

is powerful, but also costly—if two threads both want to update a single atomic variable, one will succeed, while the other will fail and have to retry. If many threads are involved, they all have to mediate sequential access to the underlying value, and there will be plenty of spinning while threads retry on failure. 

For simple operations that commute, rather than fail and retry just because another thread modified the value, we can tell the CPU what operation to perform on the atomic variable. It’ll then perform that opera- tion on whatever the current value happens to be when the CPU eventually gets exclusive access. Think of an AtomicUsize that counts the number of 

Concurrency (and Parallelism) **187** 

**188** 

Chapter 10 

**N O T E** 

operations a pool of threads has completed. If two threads both complete a job at the same time, it doesn’t matter which one updates the counter first as long as both their increments are counted. 

The fetch methods implement these kinds of commutative opera- tions. They perform a read *and* a store operation in a single step and guarantee that the store operation was performed on the atomic variable when it held exactly the value returned by the method. As an example, AtomicUsize::fetch_add(1, Ordering::Relaxed) never fails—it always adds 1 to the current value of the AtomicUsize, no matter what it is, and returns the value of the AtomicUsize precisely when this thread’s 1 was added. 

The fetch methods tend to be more efficient than compare_exchange because they don’t require threads to fail and retry when multiple threads contend for access to a variable. Some hardware architectures even have specialized fetch method implementations that scale much better as the number of involved CPUs grows. Nevertheless, if enough threads try to operate on the same atomic variable, those operations will begin to slow down and exhibit sublinear scaling due to the coordination required. In general, the best way to significantly improve the performance of a concur- rent algorithm is to split contended variables into more atomic variables that are each less contended, rather than switching from compare_exchange to a fetch method. 

*The* *fetch_update* *method is somewhat deceptively named—behind the scenes, it is really just a* *compare_exchange_weak* *loop, so its performance profile will more closely match that of* *compare_exchange* *than the other fetch methods.* 

**Sane Concurrency** 

Writing correct and performant concurrent code is harder than writing sequential code; you have to consider not only possible execution interleav- ings but also how your code interacts with the compiler, the CPU, and the memory subsystem. With such a wide array of footguns at your disposal, it’s easy to want to throw your hands in the air and just give up on concurrency altogether. In this section we’ll explore some techniques and tools that can help ensure that you write correct concurrent code without (as much) fear. 

**Start Simple** 

It is a fact of life that simple, straightforward, easy-to-follow code is more likely to be correct. This principle also applies to concurrent code—always start with the simplest concurrent design you can think of, then measure, and only if measurement reveals a performance problem should you opti- mize your algorithm. 

To follow this tip in practice, start out with concurrency patterns that do not require intricate use of atomics or lots of fine-grained locks. Begin with multiple threads that run sequential code and communicate over channels, or that cooperate through locks, and then benchmark the result- ing performance with the workload you care about. You’re much less likely 

to make mistakes this way than by implementing fancy lockless algorithms or by splitting your locks into a thousand pieces to avoid false sharing. For many use cases, these designs are plenty fast enough; it turns out a lot of time and effort has gone into making channels and locks perform well! And if the simple approach is fast enough for your use case, why introduce more complex and error-prone code? 

If your benchmarks indicate a performance problem, then figure out exactly which part of your system scales poorly. Focus on fixing that bottle- neck in isolation where you can, and try to do so with small adjustments where possible. Maybe it’s enough to split a lock in two rather than move to a concurrent hash table, or to introduce another thread and a channel rather than implement a lock-free work stealing queue. If so, do that. 

Even when you do have to work directly with atomics and the like, keep things simple until there’s a proven need to optimize—use Ordering::SeqCst and compare_exchange at first, and then iterate if you find concrete evidence that those are becoming bottlenecks that must be taken care of. 

**Write Stress Tests** 

As the author, you have a lot of insight into where bugs in your code
 may hide, without necessarily knowing what those bugs are (yet, anyway). Writing stress tests is a good way to shake out some of the hidden bugs. Stress tests don’t necessarily perform a complex sequence of steps but instead have lots of threads doing relatively simple operations in parallel. 

For example, if you were writing a concurrent hash map, one stress test might be to have *N* threads insert or update keys and *M* threads read keys in such a way that those *M*+*N* threads are likely to often choose the same keys. Such a test doesn’t test for a particular outcome or value but instead tries to trigger many possible interleavings of operations in the hopes that buggy interleavings might reveal themselves. 

Stress tests resemble fuzz tests in many ways; whereas fuzzing gener- ates many random inputs to a given function, the stress test instead gen- erates many random thread and memory access schedules. Just like fuzzers, stress tests are therefore only as good as the assertions in your code; they can’t tell you about a bug that doesn’t manifest in some easy-to-spot way like an assertion failure or some other kind of panic. For that reason, it’s a good idea to litter your low-level concurrency code with assertions, or debug_ assert_* if you’re worried about runtime cost in particularly hot loops. 

**Use Concurrency Testing Tools** 

The primary challenge in writing concurrent code is to handle all the pos- sible ways the execution of different threads can interleave. As we saw in the Ordering::SeqCst example in Listing 10-4, it’s not just the thread scheduling that matters, but also which memory values are possible for a given thread to observe at any given point in time. Writing tests that execute every pos- sible legal execution is not only tedious but also difficult—you need very low-level control over which threads execute when and what values their reads return, which the operating system likely doesn’t provide. 

Concurrency (and Parallelism) **189** 

**190** Chapter 10 

**Model Checking with Loom** 

Luckily, a tool already exists that can simplify this execution exploration
 for you in the form of the loom crate. Given the relative release cycles of this book and that of a Rust crate, I won’t give any examples of how to use Loom here, as they’d likely be out of date by the time you read this book, but I will give an overview of what it does. 

Loom expects you to write dedicated test cases in the form of closures that you pass into a Loom model. The model keeps track of all cross-thread interactions and tries to intelligently explore all possible iterations of those interactions by executing the test case closure multiple times. To detect and control thread interactions, Loom provides replacement types for all the types in the standard library that allow threads to coordinate with one another; that includes most types under std::sync and std::thread as well
 as UnsafeCell and a few others. Loom expects your application to use those replacement types whenever you run the Loom tests. The replacement types tie into the Loom executor and perform a dual function: they act as rescheduling points so that Loom can choose which operation to run next after each possible thread interaction point, and they inform Loom of new possible interleavings to consider. Essentially, Loom builds up a tree of all the possible future executions for each point at which multiple execution interleavings are possible and then tries to execute all of them, one after the other. 

Loom attempts to fully explore all possible executions of the test
 cases you provide it with, which means it can find bugs that occur only in extremely rare executions that stress testing would not find in a hundred years. While that’s great for smaller test cases, it’s generally not feasible
 to apply that kind of rigorous testing to larger test cases that test more involved sequences of operations or require many threads to run at once. Loom would simply take too long to get decent coverage of the code. In practice, you may therefore want to tell Loom to consider only a subset of the possible executions, which Loom’s documentation has more details on. 

Like with stress tests, Loom can catch only bugs that manifest as panics, so that’s yet another reason to spend some time placing strategic assertions in your concurrent code! In many cases, it may even be worthwhile to add additional state tracking and bookkeeping instructions to your concurrent code to give you better assertions. 

**Runtime Checking with ThreadSanitizer** 

For larger test cases, your best bet is to run the test through a couple of itera- tions under Google’s excellent ThreadSanitizer, also known as TSan. TSan automatically augments your code by placing extra bookkeeping instructions prior to every memory access. Then, as your code runs, those bookkeeping instructions update and check a special state machine that flags any con- current memory operations that indicate a problematic race condition. For example, if thread B writes to some atomic value X, but has not synchronized (lots of hand waving here) with the thread that wrote the previous value of X that indicates a write/write race, which is nearly always a bug. 

Since TSan only observes your code running and does not execute
 it over and over again like Loom, it generally only adds a constant-factor overhead to the runtime of your program. While that factor can be signifi- cant (5–15 times at the time of writing), it’s still small enough that you can execute even most complex test cases in a reasonable amount of time. 

At the time of writing, to use TSan you need to use a nightly version of the Rust compiler and pass in the -Zsanitizer=thread command-line argu- ment (or set it in RUSTFLAGS), though hopefully in time this will be a standard supported option. Other sanitizers are also available that check things like out-of-bounds memory accesses, use-after-free, memory leaks, and reads of uninitialized memory, and you may want to run your concurrent test suite through those too! 

**HEISENBUGS** 

Heisenbugs are bugs that seem to disappear when you try to study them . This happens quite frequently when trying to debug highly concurrent code; the additional instrumentation to debug the problem changes the relative timing of concurrent events and might cause the execution interleaving that triggered the bug to no longer happen . 

A particularly common cause of disappearing concurrency bugs is using print statements, which is by far one of the most common debugging techniques . There are two reasons why print statements have such an outsized effect on concurrency bugs . The first, and perhaps most obvious, is that relatively speak- ing, printing something to the user’s terminal (or wherever standard output points) takes quite a long time, especially if your program is producing a lot
 of output . Writing to the terminal requires, at the very least, a round-trip to the operating system kernel to perform the write, but the write may also have to wait for the terminal itself to read from the process’s output into its own buffers . All that extra time might so much delay the operation that previously raced with an operation in some other thread that the race condition disappears . 

The second reason why print statements disturb concurrent execution pat- terns is that writing to standard output is (generally) guarded by a lock . If you look inside the Stdout type in the standard library, you’ll see that it holds a Mutex that guards access to the output stream . It does this so that the output isn’t garbled too badly if multiple threads try to write at the same time—without a lock, a given
 line might have characters interspersed from multiple thread writes, but with the lock the threads will take turns writing instead . Unfortunately, acquiring the output lock, is another thread synchronization point, and one that every printing thread is involved in . This means that if your code was previously broken due to missing syn- chronization between two threads, or just because a particular race between two threads was possible, adding print statements might fix that bug as a side effect! 

In general, when you spot what seems like a Heisenbug, try to find other ways to narrow down the problem . That might involve using Loom or TSan, 

(continued) 

Concurrency (and Parallelism) **191** 

**192** 

Chapter 10 

using gdb or lldb, or using a per-thread in-memory log that you print only at the end . Many logging frameworks also work hard to avoid synchronization points on the critical path of issuing log events, so switching to one of those might make your life easier . As an added bonus, good logging that you leave behind after fixing a particular bug might come in handy later . Personally I’m a big fan of the tracing crate, but there are many good options out there . 

**Summary** 

In this chapter, we first covered common correctness and performance pit- falls in concurrent Rust, and some of the high-level concurrency patterns that successful concurrent applications tend to use to work around them. We also explored how asynchronous Rust enables concurrency without par- allelism, and how to explicitly introduce parallelism in asynchronous Rust code. We then dove deeper into Rust’s many different lower-level concur- rency primitives, including how they work, how they differ, and what they’re all for. Finally, we explored techniques for writing better concurrent code and looked at tools like Loom and TSan that can help you vet that code. In the next chapter we’ll continue our journey through the lower levels of Rust by digging into foreign function interfaces, which allow Rust code to link directly against code written in other languages. 