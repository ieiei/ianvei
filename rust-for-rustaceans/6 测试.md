
在本章中，我们将介绍扩展 Rust 测试功能的各种方法，以及您可能想要添加到测试组合中的其他类型的测试。 Rust 附带了许多内置测试工具，这些工具在《Rust 编程语言》中得到了很好的介绍，主要由 #[test] 属性和 *tests/* 目录表示。 这些将为您在广泛的应用程序和规模中提供良好的服务，并且通常是您开始项目时所需要的。 然而，随着代码库的发展和您的测试需求变得更加复杂，您可能需要不仅仅将 #[test] 标记到各个函数上。

本章分为两个主要部分。 第一部分涵盖 Rust 测试机制，例如标准测试工具和条件测试代码。 第二个着眼于评估 Rust 代码正确性的其他方法，例如基准测试、linting 和模糊测试。

## Rust测试机制

要了解 Rust 提供的各种测试机制，您必须首先了解 Rust 如何构建和运行测试。 当您运行 Cargo test --lib 时，Cargo 所做的唯一特殊的事情是将 --test 标志传递给 rustc。 该标志告诉 rustc 生成一个运行所有单元测试的测试二进制文件，而不仅仅是编译板条箱的库或二进制文件。 在幕后，--test 有两个主要作用。 首先，它启用 cfg(test)，以便您可以有条件地包含测试代码（稍后会详细介绍）。 其次，它使编译器生成一个*测试工具*：一个精心生成的主函数，在程序运行时调用程序中的每个#[test]函数。

### *The Test Harness*

编译器通过混合过程宏（我们将在第 7 章中更深入地讨论）和一些魔法来生成测试工具主函数。 本质上，该工具将 #[test] 注释的每个函数转换为测试*描述符*——这是程序宏部分。 然后，它将每个描述符的路径暴露给生成的主函数——这是神奇的部分。 描述符包括测试名称、它设置的任何附加选项（如#[should_panic]）等信息。 其核心是，测试工具迭代包中的测试、运行它们、捕获它们的结果并打印结果。 因此，它还包括解析命令行参数（例如 --test-threads=1 等）、捕获测试输出、并行运行列出的测试以及收集测试结果的逻辑。

截至撰写本文时，Rust 开发人员正在努力将测试工具生成的神奇部分变成公开可用的 API，以便开发人员可以构建自己的测试工具。 这项工作仍处于实验阶段，但该提案与当今存在的模型相当接近。 需要解决的部分问题是如何确保 #[test] 函数可用于生成的主函数，即使它们位于私有子模块内。

集成测试（*tests/* 中的测试）遵循与单元测试相同的过程，但有一个例外，它们各自编译为自己单独的 crate，这意味着它们只能访问主 crate 的公共接口，并针对主 crate 运行 crate 编译时没有 #[cfg(test)]。 为 *tests/* 中的每个文件生成一个测试工具。 不会为 *tests/* 下子目录中的文件生成测试工具，以允许您为测试共享子模块。

**NOTE** *If you explicitly want a test harness for a file in a subdirectory, you can opt in to that by calling the file* main.rs*.* 

Rust 不要求您使用默认的测试工具。 您可以选择退出它，并通过为 *Cargo.toml* 中给定的集成测试设置harness = false 来实现您自己的代表测试运行器的 main 方法，如清单 6-1 所示。 然后将调用您定义的 main 方法来运行测试。
``` toml
[[test]]
name = "custom"
path = "tests/custom.rs"
harness = false
```

*Listing 6-1: Opting out of the standard test harness*

如果没有测试工具，#[test] 周围的魔法就不会发生。 相反，您需要编写自己的主函数来运行您想要执行的测试代码。 本质上，您正在编写一个正常的 Rust 二进制文件，它恰好由货物测试运行。 该二进制文件负责处理默认线束通常执行的所有操作（如果您想支持它们），例如命令行标志。 为每个集成测试单独设置线束属性，因此您可以拥有一个使用标准线束的测试文件和一个不使用标准线束的测试文件。

> **ARGUMENTS TO THE DEFAULT TEST HARNESS** 
> 
> 默认测试工具支持许多命令行参数来配置测试的运行方式。 这些不会直接传递给cargo test，而是传递给当您运行cargo test 时Cargo 为您编译和运行的测试二进制文件。 要访问该组标志，请将 -- 传递给 Cargo test，后跟测试二进制文件的参数。 例如，要查看测试二进制文件的帮助文本，您可以运行 Cargo test -- --help 。
>
> 通过这些命令行参数可以使用许多方便的配置选项。 --nocapture 标志禁用运行 Rust 测试时通常发生的输出捕获。 如果您想实时观察测试的输出而不是在测试失败后立即观察所有输出，这非常有用。 您可以使用 --test-threads 选项来限制同时运行的测试数量，如果您有一个测试挂起或出现段错误，并且您想通过按顺序运行测试来确定是哪一个测试，这会很有帮助。 还有一个 --skip 选项用于跳过与特定模式匹配的测试， --ignored 运行通常会被忽略的测试（例如需要运行外部程序的测试），以及 --list 只列出所有 可用的测试。
>
   请记住，这些参数都是由默认测试工具实现的，因此，如果您禁用它（使用harness = false），则必须在主函数中自己实现您需要的参数！

Integration tests without a harness are primarily useful for benchmarks, as we’ll see later, but they also come in handy when you want to run tests that don’t fit the standard “one function, one test” model. For example, you’ll frequently see harnessless tests used with fuzzers, model checkers, and tests that require a custom global setup (like under WebAssembly or when working with custom targets). 

### *#[cfg(test)]*

When Rust builds code for testing, it sets the compiler configuration flag test, which you can then use with conditional compilation to have code that is compiled out unless it is specifically being tested. On the surface, this may seem odd: don’t you want to test exactly the same code that’s going into production? You do, but having code exclusively available when testing allows you to write better, more thorough tests, in a few ways. 

> **MOCKING** 
> 
> When writing tests, you often want tight control over the code you’re testing as well as any other types that your code may interact with . For example, if you are testing a network client, you probably do not want to run your unit tests over a real network but instead want to directly control what bytes are emitted by the “network” and when . Or, if you’re testing a data structure, you want your test to use types that allow you to control what each method returns on each invocation . You may also want to gather metrics such as how often a given method was called or whether a given byte sequence was emitted . 
> 
> These “fake” types and implementations are known as mocks, and they are a key feature of any extensive unit test suite . While you can often do the work needed to get this kind of control manually, it’s nicer to have a library take care of most of the nitty-gritty details for you . This is where automated mocking comes into play . A mocking library will have facilities for generating types (including functions) with particular properties or signatures, as well as mechanisms to control and introspect those generated items during a test execution . 
> 
> Mocking in Rust generally happens through generics—as long as your program, data structure, framework, or tool is generic over anything you might want to mock (or takes a trait object), you can use a mocking library to generate conforming types that will instantiate those generic parameters . You then write your unit tests by instantiating your generic constructs with the generated mock types, and you’re off to the races! 
> 
> In situations where generics are inconvenient or inappropriate, such as if you want to avoid making a particular aspect of your type generic to users, you can instead encapsulate the state and behavior you want to mock in a dedicated struct . You would then generate a mocked version of that struct and its methods and use conditional compilation to use either the real or mocked implementation depending on cfg(test) or a test-only feature like cfg(feature = "test_mock_foo") . 
> 
> At the moment, there isn’t a single mocking library, or even a single mocking approach, that has emerged as the One True Answer in the Rust community . The most extensive and thorough mocking library I know of is the mockall crate, but that is still under active development, and there are many other contenders . 

**Test-Only APIs** 

First, having test-only code allows you to expose additional methods, fields, and types to your (unit) tests so the tests can check not only that the public API behaves correctly but also that the internal state is correct. For example, consider the HashMap type from hashbrown, the crate that implements the standard library HashMap. The HashMap type is really just a wrapper around a RawTable type, which is what implements most of the hash table logic. Suppose that after doing a HashMap::insert on an empty map, you want to check that a single bucket in the map is nonempty, as shown in Listing 6-2. 

```rust
#[test]
fn insert_just_one() {
  let mut m = HashMap::new();
  m.insert(42, ());
  let full = m.table.buckets.iter().filter(Bucket::is_full).count();
  assert_eq!(full, 1);
}
```

*Listing 6-2: A test that accesses inaccessible internal state and thus does not compile*

此代码不会按编写方式编译，因为虽然测试代码可以访问 HashMap 的私有表字段，但它无法访问 RawTable 的私有存储桶字段，因为 RawTable 位于不同的模块中。 我们可以通过使存储桶字段可见性 pub(crate) 来解决此问题，但我们确实不希望 HashMap 一般能够接触存储桶，因为它可能会意外损坏 RawTable 的内部状态。 即使将存储桶设置为只读也可能会出现问题，因为 HashMap 中的新代码可能会根据 RawTable 的内部状态启动，从而使将来的修改变得更加困难。

解决方案是使用#[cfg(test)]。 我们可以向 RawTable 添加一个方法，仅允许在测试时访问存储桶，如清单 6-3 所示，从而避免为其余代码添加脚枪。 然后可以更新清单 6-2 中的代码以调用buckets()，而不是访问私有bucket 字段。

```rust
impl RawTable {
  #[cfg(test)]
  pub(crate) fn buckets(&self) -> &[Bucket] {
    &self.buckets
  }
}
```

*Listing 6-3: Using *#[cfg(test)]* to make internal state accessible in the testing context 

**Bookkeeping for Test Assertions** *

拥有仅在测试期间存在的代码的第二个好处是，您可以增强程序以执行额外的运行时簿记，然后可以通过测试进行检查。 例如，假设您正在从标准库编写自己版本的 BufWriter 类型。 测试时，您需要确保 BufWriter 不会发出不必要的系统调用。 最明显的方法是让 BufWriter 跟踪它在底层 Write 上调用 write 的次数。 然而，在生产中，这些信息并不重要，跟踪它会带来（边际）性能和内存开销。 使用#[cfg(test)]，您可以仅在测试时进行簿记，如清单6-4所示。

```rust
struct BufWriter<T> {
  #[cfg(test)]
  write_through: usize,
  // other fields...
}
impl<T: Write> Write for BufWriter<T> {
  fn write(&mut self, buf: &[u8]) -> Result<usize> {
  		// ... 
      if self.full() {
      #[cfg(test)]
      self.write_through += 1;
      let n = self.inner.write(&self.buffer[..])?;
 // ... 
}}  
```

*Listing 6-4: Using *#[cfg(test)]* to limit bookkeeping to the testing context*

Keep in mind that test is set only for the crate that is being compiled as a test. For unit tests, this is the crate being tested, as you would expect. For integration tests, however, it is the integration test binary being compiled as a test—the crate you are testing is just compiled as a library and so will not have test set. 

### *Doctests*

文档注释中的 Rust 代码片段会自动作为测试用例运行。 这些通常称为*doctests*。 由于文档测试出现在您的板条箱的公共文档中，并且用户可能会模仿它们包含的内容，因此它们作为集成测试运行。 这意味着文档测试无权访问私有字段和方法，并且测试未在主包的代码上设置。 每个 doctest 都被编译为自己的专用 crate，并独立运行，就像用户将 doctest 复制粘贴到自己的程序中一样。

在幕后，编译器对文档测试执行一些预处理，以使它们更加简洁。 最重要的是，它会自动在您的代码周围添加一个 fn main 。 这允许文档测试仅关注用户可能关心的重要部分，例如实际使用库中的类型和方法的部分，而不包含不必要的样板。

您可以通过在 doctest.txt 中定义您自己的 fn main 来选择退出此自动换行。 例如，如果您想使用 #[tokio::main] async fn main 之类的东西编写异步 main 函数，或者如果您想向 doctest 添加其他模块，您可能想要这样做。

要使用？ 操作符在你的 doctest 中，你通常不必使用自定义 main 函数，因为 rustdoc 包含一些启发式方法来将返回类型设置为 Result<(), impl Debug> 如果你的代码看起来使用 ? （例如，如果它以 Ok(()) 结尾）。 如果类型推断让您很难了解函数的错误类型，您可以通过更改要显式键入的 doctest 的最后一行来消除歧义，如下所示：Ok::<(), T>(())。

文档测试有许多附加功能，当您为更复杂的接口编写文档时，这些功能会派上用场。 第一个是隐藏单行的能力。 如果在 doctest 的一行中添加 # 前缀，则编译和运行 doctest 时会包含该行，但不会包含在文档中生成的代码片段中。 这使您可以轻松隐藏对当前示例不重要的细节，例如实现虚拟类型的特征或生成值。 如果您希望呈现一系列示例而不是每次都显示相同的前导代码，它也很有用。 清单 6-5 给出了带有隐藏行的 doctest 的示例。

``` rust
/// Completely frobnifies a number through I/O.
///
/// In this first example we hide the value generation.
/// ```
/// # let unfrobnified_number = 0;
/// # let already_frobnified = 1;
/// assert!(frobnify(unfrobnified_number).is_ok());
/// assert!(frobnify(already_frobnified).is_err());
/// ```
///
/// Here's an example that uses ? on multiple types
/// and thus needs to declare the concrete error type,
/// but we don't want to distract the user with that.
/// We also hide the use that brings the function into scope.
/// ```
/// # use mylib::frobnify;
/// frobnify("0".parse()?)?;
/// # Ok::<(), anyhow::Error>(())
/// ```
///
/// You could even replace an entire block of code completely,
/// though use this _very_ sparingly:
/// ```
/// # /*
/// let i = ...;
/// # */
/// # let i = 42;
/// frobnify(i)?;
/// ```
fn frobnify(i: usize) -> std::io::Result<()> {
```

*Listing 6-5: Hiding lines in a doctest with *#* *

**NOTE** *Use this feature with care; it can be frustrating to users if they copy-paste an example and then it doesn’t work because of required steps that you’ve hidden.* 

与 #[test] 函数非常相似，doctest 也支持修改 doctest 运行方式的属性。 这些属性紧跟在用于表示代码块的三个反引号之后，并且多个属性可以用逗号分隔。

与测试函数一样，您可以指定 should_panic 属性来指示特定 doctest 中的代码在运行时应该发生恐慌，或者仅当使用 --ignored 标志运行 Cargo 测试时才忽略检查代码段。 您还可以使用 no_run 属性来指示给定的 doctest 应该编译但不应运行。

属性compile_fail告诉rustdoc文档示例中的代码不应编译。 这向用户表明特定用途是不可能的，并且可以作为有用的测试来提醒您在库的相关方面发生变化时更新文档。 您还可以使用此属性来检查某些静态属性是否适用于您的类型。 清单6-6显示了如何使用compile_fail来检查给定类型是否未实现Send的示例，这对于维护不安全代码中的安全保证可能是必要的。

```rust
​compile_fail
# struct MyNonSendType(std::rc::Rc<()>);
fn is_send<T: Send>() {}
is_send::<MyNonSendType>();
​
```

Listing 6-6: Testing that code fails to compile with *compile_fail* 

compile_fail is a fairly crude tool in that it gives no indication of *why* the code does not compile. For example, if code doesn’t compile because of a missing semicolon, a compile_fail test will appear to have been successful. For that reason, you’ll usually want to add the attribute only after you have made sure that the test indeed fails to compile with the expected error. If you need more fine-grained tests for compilation errors, such as when developing macros, take a look at the trybuild crate. 

## Additional Testing Tools

There’s a lot more to testing than just running test functions and seeing that they produce the expected result. A thorough survey of testing techniques, methodologies, and tools is outside the scope of this book, but there are some key Rust-specific pieces that you should know about as you expand your testing repertoire. 

### *Linting*

You may not consider a linter’s checks to be tests, but in Rust they often can be. The Rust linter *clippy* categorizes a number of its lints as *correctness*  *lints*. These lints catch code patterns that compile but are almost certainly bugs. Some examples are a = b; b = a, which fails to swap a and b; std::mem::forget(t), where t is a reference; and for x in y.next(), which will iterate only over the first element in y. If you are not running clippy as part of your CI pipeline already, you probably should be. 

Clippy comes with a number of other lints that, while usually helpful, may be more opinionated than you’d prefer. For example, the type_complexity lint, which is on by default, issues a warning if you use a particularly involved type in your program, like Rc<Vec<Vec<Box<(u32, u32, u32, u32)>>>>. While that warning encourages you to write code that is easier to read, you may find it too pedantic to be broadly useful. If some part of your code erroneously triggers a particular lint, or you just want to allow a specific instance of it, you can opt out of the lint just for that piece of code with #[allow(clippy::name_of_lint)]. 

The Rust compiler also comes with its own set of lints in the form of warnings, though these are usually more directed toward writing idiomatic code than checking for correctness. Instead, correctness lints in the compiler are simply treated as errors (take a look at rustc -W help for a list). 

*Not all compiler warnings are enabled by default. Those disabled by default are usually still being refined, or are more about style than content. A good example of this is the “idiomatic Rust 2018 edition” lint, which you can enable with* *#![warn(rust_2018 _idioms)]**. When this lint is enabled, the compiler will tell you if you’re failing to take advantage of changes brought by the Rust 2018 edition. Some other lints that you may want to get into the habit of enabling when you start a new project are* *missing_docs* *and* *missing_debug_implementations**, which warn you if you’ve forgotten to document any public items in your crate or add* *Debug* *implementations for any public types, respectively.* 

### *Test Generation*

Writing a good test suite is a lot of work. And even when you do that work, the tests you write test only the particular set of behaviors you were considering at the time you wrote them. Luckily, you can take advantage of a number of test generation techniques to develop better and more thorough tests. These generate input for you to use to check your application’s correctness. Many such tools exist, each with their own strengths and weaknesses, so here I’ll cover only the main strategies used by these tools: fuzzing and property testing. 

**Fuzzing** 

Entire books have been written about fuzzing, but at a high level the idea is simple: generate random inputs to your program and see if it crashes. If the program crashes, that’s a bug. For example, if you’re writing a URL parsing library, you can fuzz-test your program by systematically generating random strings and throwing them at the parsing function until it panics. Done naively, this would take a while to yield results: if the fuzzer starts with a, then b, then c, and so on, it will take it a long time to generate a tricky URL like http://[:]. In practice, modern fuzzers use code coverage metrics to explore different paths in your code, which lets them reach higher degrees of coverage faster than if the inputs were truly chosen at random. 

Fuzzers are great at finding strange corner cases that your code doesn’t handle correctly. They require little setup on your part: you just point the fuzzer at a function that takes a “fuzzable” input, and off it goes. For example, Listing 6-7 shows an example of how you might fuzz-test a URL parser. 

```rust
libfuzzer_sys::fuzz_target!(|data: &[u8]| {
  if let Ok(s) = std::str::from_utf8(data) {
      let _ = url::Url::parse(s);
  }
}); 
```

Listing 6-7: Fuzzing a URL parser with *libfuzzer*

The fuzzer will generate semi-random inputs to the closure, and any that form valid UTF-8 strings will be passed to the parser. Notice that the code here doesn’t check whether the parsing succeeds or fails—instead, it’s looking for cases where the parser panics or otherwise crashes due to internal invariants that are violated. 

The fuzzer keeps running until you terminate it, so most fuzzing tools come with a built-in mechanism to stop after a certain number of test cases have been explored. If your input isn’t a trivially fuzzable type—something like a hash table—you can usually use a crate like arbitrary to turn the byte string that the fuzzer generates into a more complex Rust type. It feels like magic, but under the hood it’s actually implemented in a very straightforward fashion. The crate defines an Arbitrary trait with a single method, arbitrary, that constructs the implementing type from a source of random bytes. Primitive types like u32 or bool read the necessary number of bytes from that input to construct a valid instance of themselves, whereas more complex types like HashMap or BTreeSet produce one number from the input to dictate their length and then call Arbitrary that number of times on their inner types. There’s even an attribute, #[derive(Arbitrary)], that implements Arbitrary by just calling arbitrary on each contained type! To explore fuzzing further, I recommend starting with cargo-fuzz. 

**Property-Based Testing** 

Sometimes you want to check not only that your program doesn’t crash but also that it does what it’s expected to do. It’s great that your add function didn’t panic, but if it tells you that the result of add(1, 4) is 68, it’s probably still wrong. This is where *property-based testing* comes into play; you describe a number of properties your code should uphold, and then the property testing framework generates inputs and checks that those properties indeed hold. 

A common way to use property-based testing is to first write a simple but naive version of the code you want to test that you are confident is correct. Then, for a given input, you give that input to both the code you want to test and the simplified but naive version. If the result or output of the two implementations is the same, your code is good—that is the correctness property you’re looking for—but if it’s not, you’ve likely found a bug. You can also use property-based testing to check for properties not directly related to correctness, such as whether operations take strictly less time for one implementation than another. The common principle is that you want any difference in outcome between the real and test versions to be informative and actionable so that every failure allows you to make improvements. The naive implementation might be one from the standard library that you’re trying to replace or augment (like std::collections::VecDeque), or it might be a simpler version of an algorithm that you’re trying optimize (like naive versus optimized matrix multiplication). 

If this approach of generating inputs until some condition is met sounds a lot like fuzzing, that’s because it is—smarter people than I have argued that fuzzing is “just” property-based testing where the property you’re testing for is “it doesn’t crash.” 

One downside of property-based testing is that it relies more heavily on the provided descriptions of the inputs. Whereas fuzzing will keep trying all possible inputs, property testing tends to be guided by developer annotations like “a number between 0 and 64” or “a string that contains three commas.” This allows property testing to more quickly reach cases that fuzzers may take a long time to encounter randomly, but it does require manual work and may miss important but niche buggy inputs. As fuzzers and property testers grow closer, however, fuzzers are starting to gain this kind of constraint-based searching capability as well. 

If you’re curious about property-based test generation, I recommend starting with the proptest crate. 

>  **TESTING SEQUENCES OF OPERATIONS** 
>  
>  Since fuzzers and property testers allow you to generate arbitrary Rust types, you aren’t limited to testing a single function call in your crate . For example, say you want to test that some type Foo behaves correctly if you perform a particular sequence of operations on it . You could define an enum Operation that lists operations, and make your test function take a Vec\<Operation\> . Then you could instantiate a Foo and perform each operation on that Foo, one after the other . Most testers have support for minimizing inputs, so they will even search for the smallest sequence of operations that still violates a property if a propertyviolating input is found! 

### *Test Augmentation*

Let’s say you have a magnificent test suite all set up, and your code passes all the tests. It’s glorious. But then, one day, one of the normally reliable tests inexplicably fails or crashes with a segmentation fault. There are two common reasons for these kinds of nondeterministic test failures: race conditions, where your test might fail only if two operations occur on different threads in a particular order, and undefined behavior in unsafe code, such as if some unsafe code reads a particular value out of uninitialized memory. 

Catching these kinds of bugs with normal tests can be difficult—often you don’t have sufficient low-level control over thread scheduling, memory layout and content, or other random-ish system factors to write a reliable test. You could run each test many times in a loop, but even that may not catch the error if the bad case is sufficiently rare or unlikely. Luckily, there are tools that can help augment your tests to make catching these kinds of bugs much easier. 

The first of these is the amazing tool *Miri*, an interpreter for Rust’s *mid-level intermediate representation (MIR)*. MIR is an internal, simplified representation of Rust that helps the compiler find optimizations and check properties without having to consider all of the syntax sugar of Rust itself. Running your tests through Miri is as simple as running cargo miri test. Miri *interprets* your code rather than compiling and running it like a normal binary, which makes the tests run a decent amount slower. But in return, Miri can keep track of the entire program state as each line of your code executes. This allows Miri to detect and report if your program ever exhibits certain types of undefined behavior, such as uninitialized memory reads, uses of values after they’ve been dropped, or out-of-bounds pointer accesses. Rather than having these operations yield strange program behaviors that may only sometimes result in observable test failures (like crashes), Miri detects them when they happen and tells you immediately. 

For example, consider the very unsound code in Listing 6-8, which creates two exclusive references to a value. 

``` rust
let mut x = 42;
let x: *mut i32 = &mut x;
let (x1, x2) = unsafe { (&mut *x, &mut *x) };
println!("{} {}", x1, x2);
```

Listing 6-8: Wildly unsafe code that Miri detects is incorrect 

At the time of writing, if you run this code through Miri, you get an error that points out exactly what’s wrong: error: Undefined Behavior: trying to reborrow for Unique at alloc1383, but parent tag <2772> does not have an appropriate item in the borrow stack
``` rust
 --> src/main.rs:4:6
  |
4 | let (x1, x2) = unsafe { (&mut *x, &mut *x) };
  |      ^^ trying to reborrow for Unique at alloc1383, but parent tag <2772>
does not have an appropriate item in the borrow stack
```

**N O T E** *Miri is still under development, and its error messages aren’t always the easiest to understand. This is a problem that’s being actively worked on, so by the time you read this, the error output may have already gotten much better!* 

Another tool worth looking at is *Loom*, a clever library that tries to ensure your tests are run with every relevant interleaving of concurrent operations. At a high level, Loom keeps track of all cross-thread synchronization points and runs your tests over and over, adjusting the order in which threads proceed from those synchronization points each time. So, if thread A and thread B both take the same Mutex, Loom will ensure that the test runs once with A taking it first and once with B taking it first. Loom also keeps track of atomic accesses, memory orderings, and accesses to UnsafeCell (which we’ll discuss in Chapter 9) and checks that threads do not access them inappropriately. If a test fails, Loom can give you an exact rundown of which threads executed in what order so you can determine how the crash happened. 

### *Performance Testing*

Writing performance tests is difficult because it is often hard to accurately model a workload that reflects real-world usage of your crate. But having such tests is important; if your code suddenly runs 100 times slower, that really should be considered a bug, yet without a performance test you may not spot the regression. If your code runs 100 times *faster*, that might also indicate that something is off. Both of these are good reasons to have automated performance tests as part of your CI—if performance changes drastically in either direction, you should know about it. 

Unlike with functional testing, performance tests do not have a common, well-defined output. A functional test will either succeed or fail, whereas a performance test may give you a throughput number, a latency profile, a number of processed samples, or any other metric that might be relevant to the application in question. Also, a performance test may require running a function in a loop a few hundred thousand times, or it might take hours running across a distributed network of multicore boxes. For that reason, it is difficult to speak about how to write performance tests in a general sense. Instead, in this section, we’ll look at some of the issues you may encounter when writing performance tests in Rust and how to mitigate them. Three particularly common pitfalls that are often overlooked are performance variance, compiler optimizations, and I/O overhead. Let’s explore each of these in turn. 

**Performance Variance** 

Performance can vary for a huge variety of reasons, and many factors affect how fast a particular sequence of machine instructions run. Some are obvious, like the CPU and memory clock speed, or how loaded the machine otherwise is, but many are much more subtle. For example, your kernel version may change paging performance, the length of your username might change the layout of memory, and the temperature in the room might cause the CPU to clock down. Ultimately, it is highly unlikely that if you run a benchmark twice, you’ll get the same result. In fact, you may observe significant variance, even if you are using the same hardware. Or, viewed from another perspective, your code may have gotten slower or faster, but the effect may be invisible due to differences in the benchmarking environment. 

There are no perfect ways to eliminate all variance in your performance results, unless you happen to be able to run benchmarks repeatedly on a highly diverse fleet of machines. Even so, it’s important to try to handle this measurement variance as best we can to extract a signal from the noisy measurements benchmarks give us. In practice, our best friend in combating variance is to run each benchmark many times and then look at the *distribution* of measurements rather than just a single one. Rust has tools that can help with this. For example, rather than ask “How long did this function take to run on average?” crates like hdrhistogram enable us to look at statistics like “What range of runtime covers 95% of the samples we observed?” To be even more rigorous, we can use techniques like null hypothesis testing from statistics to build some confidence that a measured difference indeed corresponds to a true change and is not just noise. 

A lecture on statistical hypothesis testing is beyond the scope of this book, but luckily much of this work has already been done by others. The criterion crate, for instance, does all of this and more for you. All you have to do is give it a function that it can call to run one iteration of your benchmark, and it will run it the appropriate number of times to be fairly sure that the result is reliable. It then produces a benchmark report, which includes a summary of the results, analysis of outliers, and even graphical representations of trends over time. Of course, it can’t eliminate the effects of just testing on a particular configuration of hardware, but it at least categorizes the noise that is measurable across executions. 

**Compiler Optimizations** 

Compilers these days are really clever. They eliminate dead code, compute complex expressions at compile time, unroll loops, and perform other dark magic to squeeze every drop of performance out of our code. Normally this is great, but when we’re trying to measure how fast a particular piece of code is, the compiler’s smartness can give us invalid results. For example, take the code to benchmark Vec::push in Listing 6-9. 

``` rust
let mut vs = Vec::with_capacity(4);
let start = std::time::Instant::now();
for i in 0..4 {
  vs.push(i);
}
println!("took {:?}", start.elapsed());
```

Listing 6-9: A suspiciously fast performance benchmark 

If you were to look at the assembly output of this code compiled in release mode using something like the excellent *godbolt.org* or cargo-asm, you’d immediately notice that something was wrong: the calls to Vec::with _capacity and Vec::push, and indeed the whole for loop, are nowhere to be seen. They have been optimized out completely. The compiler realized that nothing in the code actually required the vector operations to be performed and eliminated them as dead code. Of course, the compiler is completely within its rights to do so, but for benchmarking purposes, this is not particularly helpful. 

To avoid these kinds of optimizations for benchmarking, the standard library provides std::hint::black_box. This function has been the topic of much debate and confusion and is still pending stabilization at the time of writing, but is so useful it’s worth discussing here nonetheless. At its core, it’s simply an identity function (one that takes x and returns x) that tells the compiler to assume that the argument to the function is used in arbitrary (legal) ways. It does not prevent the compiler from applying optimizations to the input argument, nor does it prevent the compiler from optimizing how the return value is used. Instead, it encourages the compiler to actually compute the argument to the function (under the assumption that it will be used) and to store that result somewhere accessible to the CPU such that black_box could be called with the computed value. The compiler is free to, say, compute the input argument at compile time, but it should still inject the result into the program. 

This function is all we need for many, though admittedly not all, of our benchmarking needs. For example, we can annotate Listing 6-9 so that the vector accesses are no longer optimized out, as shown in Listing 6-10. 

```rust
let mut vs = Vec::with_capacity(4);
let start = std::time::Instant::now();
for i in 0..4 {
  black_box(vs.as_ptr());
  vs.push(i);
  black_box(vs.as_ptr());
}
println!("took {:?}", start.elapsed());
```

Listing 6-10: A corrected version of Listing 6-9 

We’ve told the compiler to assume that vs is used in arbitrary ways
 on each iteration of the loop, both before and after the calls to push. This forces the compiler to perform each push in order, without merging or otherwise optimizing consecutive calls, since it has to assume that “arbitrary stuff that cannot be optimized out” (that’s the black_box part) may happen to vs between each call. 

Note that we used vs.as_ptr() and not, say, &vs. That’s because of the caveat that the compiler should assume black_box can perform any *legal* operation on its argument. It is not legal to mutate the Vec through a shared reference, so if we used black_box(&vs), the compiler might notice that vs will not change between iterations of the loop and implement optimizations based on that observation! 

**I/O Overhead Measurement** 

When writing benchmarks, it’s easy to accidentally measure the wrong thing. For example, we often want to get information in real time about how far along the benchmark is. To do that, we might write code like that in Listing 6-11, intended to measure how fast my_function runs: 

```rust
let start = std::time::Instant::now();
for i in 0..1_000_000 {
  println!("iteration {}", i);
  my_function();
}
println!("took {:?}", start.elapsed());
```

Listing 6-11: What are we really benchmarking here? 

This may look like it achieves the goal, but in reality, it does not actually measure how fast my_function is. Instead, this loop is most likely to tell us how long it takes to print a million numbers. The println! in the body of the loop does a lot of work behind the scenes: it turns a binary integer into decimal digits for printing, locks standard output, writes out a sequence of UTF-8 code points using at least one system call, and then releases the standard output lock. Not only that, but the system call might block if your terminal is slow to print out the input it receives. That’s a lot of cycles! And the time it takes to call my_function might pale in comparison. 

A similar thing happens when your benchmark uses random numbers. If you run my_function(rand::random()) in a loop, you may well be mostly measuring the time it takes to generate a million random numbers. The story is the same for getting the current time, reading a configuration file, or starting a new thread—these things all take a long time, relatively speaking, and may end up overshadowing the time you actually wanted to measure. 

Luckily, this particular issue is often easy to work around once you are aware of it. Make sure that the body of your benchmarking loop contains almost nothing but the particular code you want to measure. All other code should run either before the benchmark begins or outside of the measured part of the benchmark. If you’re using criterion, take a look at the different timing loops it provides—they’re all there to cater to benchmarking cases that require different measurement strategies! 

## Summary

In this chapter, we explored the built-in testing capabilities that Rust offers in great detail. We also looked at a number of testing facilities and techniques that are useful when testing Rust code. This is the last chapter that focuses on higher-level aspects of intermediate Rust use in this book. Starting with the next chapter on declarative and procedural macros, we will be focusing much more on Rust code. See you on the next page! 